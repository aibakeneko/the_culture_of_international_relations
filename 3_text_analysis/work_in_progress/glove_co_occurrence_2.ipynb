{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source from https://github.com/snji-khjuria/GloveModel\n",
    "    \n",
    "#essential imports\n",
    "import tensorflow as tf\n",
    "import  os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#configuration variables\n",
    "fileLocation        = \"./corpus\"\n",
    "vocab_size          = 100000\n",
    "min_occurence       = 1\n",
    "scaling_factor      = 3/4.0\n",
    "cooccurence_cap     = 100\n",
    "batch_size          = 2\n",
    "learning_rate       = 0.05\n",
    "embedding_size      = 10\n",
    "left_context_size   = 2\n",
    "right_context_size  = 2\n",
    "words               = None\n",
    "word_to_id          = None\n",
    "#cooccurence_matrix[w1, w2] = float\n",
    "cooccurence_matrix  = None\n",
    "embeddings          = None\n",
    "epoch_loss_print    = 10\n",
    "epoch_tsne_print    = 10\n",
    "log_dir             = \"./logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility to read the corpus\n",
    "def readCorpus(filename):\n",
    "#    docs = [\n",
    "#        \"With Windows 10 Home - get the best combination of Windows features you know and new improvements you'll love\",\n",
    "#        \"NVIDIA Geforce for High Graphics Performance\",\n",
    "#        \"15.6 inch HD LED Backlit Anti-glare TN Display\"\n",
    "#    ]\n",
    "    docs = [\n",
    "        \"roger roger roger anna anna roger roger\",\n",
    "        \"roger roger roger anna anna roger roger\"\n",
    "    ]\n",
    "    for line in docs: # open(filename):\n",
    "        line = line.strip()\n",
    "        yield line.lower().split()\n",
    "\n",
    "#get the left context\n",
    "def get_left_context(region, i, left_size):\n",
    "    start_index = i-left_size\n",
    "    if start_index<0:\n",
    "        start_index=0\n",
    "    left_context = region[start_index:i]\n",
    "    left_more = ['null_word']*(left_size-len(left_context))\n",
    "    left_more.extend(left_context)\n",
    "    return left_more\n",
    "\n",
    "#get the right context window\n",
    "def get_right_context(region, i, right_size):\n",
    "    end_index = i+right_size+1\n",
    "    total_region = len(region)\n",
    "    if end_index>total_region:\n",
    "        end_index=total_region\n",
    "    right_context = region[i+1:end_index]\n",
    "    right_context.extend(['null_word']*(right_size-len(right_context)))\n",
    "    return right_context\n",
    "\n",
    "#get the window\n",
    "def window(region, left_size=3, right_size=3):\n",
    "    total_region = len(region)\n",
    "    for i, word in enumerate(region):\n",
    "        left_context = get_left_context(region, i, left_size)\n",
    "        right_context = get_right_context(region, i, right_size)\n",
    "        yield (left_context, word, right_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "def fit_to_corpus(corpus, vocab_size, min_occurences, left_size, right_size):\n",
    "    words_count = Counter()\n",
    "    #provides value for non-existent key\n",
    "    cooccurence_counts = defaultdict(float)\n",
    "    for region in corpus:\n",
    "        words_count.update(region)\n",
    "        #add 1/distance from the position of centralized context word\n",
    "        for l_context, word, r_context in window(region, left_size, right_size):\n",
    "            for i, context_word in enumerate(l_context[::-1]):\n",
    "                cooccurence_counts[(word, context_word)] += 1/(i+1)\n",
    "            for i, context_word in enumerate(r_context):\n",
    "                cooccurence_counts[(word, context_word)] += 1/(i+1)\n",
    "    words = [word for word, count in words_count.most_common(vocab_size) if count>=min_occurences]\n",
    "    word_to_id = {word:i for i, word in enumerate(words)}\n",
    "    cooccurence_matrix = {\n",
    "                        (word_to_id[words[0]], word_to_id[words[1]]):count\n",
    "                        for words, count in cooccurence_counts.items()\n",
    "                         if words[0] in word_to_id and words[1] in word_to_id\n",
    "                        }\n",
    "    return words, word_to_id, cooccurence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the corpus\n",
    "corpus                                = readCorpus(fileLocation)\n",
    "#get words, word_to_id and cooccurence matrix by fitting it to corpus\n",
    "#we have words, wordstoid and possible cooccurence matrix for the words\n",
    "words, word_to_id, cooccurence_matrix = fit_to_corpus(corpus, vocab_size, min_occurence, left_context_size, right_context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 8.0, (1, 0): 8.0, (0, 0): 14.0, (1, 1): 4.0}\n"
     ]
    }
   ],
   "source": [
    "print(cooccurence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "focal_input         = tf.placeholder(tf.int32, shape=[batch_size], name=\"focal_words\")\n",
    "context_input       = tf.placeholder(tf.int32, shape=[batch_size], name=\"context_words\")\n",
    "cooccurence_count   = tf.placeholder(tf.float32, shape=[batch_size], name=\"cooccurence_count\")\n",
    "# epsilon          \n",
    "#full embedding size variables\n",
    "focal_embeddings    = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"focal_embeddings\")\n",
    "context_embeddings  = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"context_embeddings\")\n",
    "focal_biases        = tf.Variable(tf.random_uniform([vocab_size], -1.0, 1.0), name=\"focal_biases\")\n",
    "context_biases      = tf.Variable(tf.random_uniform([vocab_size], -1.0, 1.0), name=\"context_biases\")\n",
    "#embeddings lookup\n",
    "focal_embedding     = tf.nn.embedding_lookup([focal_embeddings], focal_input)\n",
    "context_embedding   = tf.nn.embedding_lookup([context_embeddings], context_input)\n",
    "focal_bias          = tf.nn.embedding_lookup([focal_biases], focal_input)\n",
    "context_bias        = tf.nn.embedding_lookup([context_biases], context_input)\n",
    "product             = tf.multiply(focal_embedding, context_embedding)\n",
    "embedding_product   = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n",
    "cooccurence_epsilon = cooccurence_count+1e-10\n",
    "log_cooccurences    = tf.log(cooccurence_epsilon)\n",
    "distance_expr       = tf.square(tf.add_n([\n",
    "                        embedding_product,\n",
    "                        focal_bias,\n",
    "                        context_bias,\n",
    "                        tf.negative(log_cooccurences)]))\n",
    "count_max           = tf.constant([cooccurence_cap], dtype=tf.float32, name=\"max_cooccurence_count\")\n",
    "scaling_factor_input      = tf.constant([scaling_factor], dtype=tf.float32, name=\"scaling_factor\")\n",
    "weighting_factor    = tf.minimum(1.0, tf.pow(tf.div(cooccurence_count, count_max), scaling_factor_input))\n",
    "single_losses       = tf.multiply(weighting_factor, distance_expr)\n",
    "total_loss          = tf.reduce_sum(single_losses)\n",
    "optimizer           = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "combined_embeddings = tf.add(focal_embeddings, context_embeddings, name=\"combined_embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(batch_size, *sequences):\n",
    "    for i in range(0, len(sequences[0]), batch_size):\n",
    "        yield tuple(sequence[i:i+batch_size] for sequence in sequences)\n",
    "\n",
    "def prepare_batches():\n",
    "    #get cooccurence matrix as list of elements and return each list as batch\n",
    "    cooccurrences = [(word_ids[0], word_ids[1], count) for word_ids, count in cooccurence_matrix.items()]\n",
    "    i_indices, j_indices, counts = zip(*cooccurrences)\n",
    "    return list(batchify(batch_size, i_indices, j_indices, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, path, size):\n",
    "    figure = plt.figure(figsize=size)  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        #for each label get its x and y position.\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        #text of annotation, xyposition, place label, coordinate system, \n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right',\n",
    "                     va='bottom')\n",
    "    if path is not None:\n",
    "        figure.savefig(path)\n",
    "        plt.close(figure)\n",
    "\n",
    "def generate_tsne(path, size=(10, 10), word_count=1000, embeddings=None):\n",
    "    #get tsne representation\n",
    "    tsne         = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    #get the tsne transformation for each embedding\n",
    "    low_dim_embs = tsne.fit_transform(embeddings[:word_count, :])\n",
    "    #get label to assign for each point in embedding space\n",
    "    labels       = words[:word_count]\n",
    "    return plot_with_labels(low_dim_embs, labels, path, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    #get the batches\n",
    "    total_steps=0\n",
    "    batches = prepare_batches()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_index, batch in enumerate(batches):\n",
    "                i_s, j_s, counts = batch\n",
    "                feed_dict = {focal_input:i_s, context_input:j_s, cooccurence_count:counts}\n",
    "                sess.run([optimizer], feed_dict=feed_dict)\n",
    "                total_steps+=1\n",
    "            if epoch%epoch_loss_print==0:\n",
    "                loss = sess.run([total_loss], feed_dict)\n",
    "                print(\"Loss is \" + str(loss))\n",
    "            if epoch%epoch_tsne_print==0:\n",
    "                embeddings      = combined_embeddings.eval()\n",
    "                outputLocation  = \"\"\n",
    "                output_path = os.path.join(log_dir, \"epoch{:03d}.jpeg\".format(epoch))\n",
    "                generate_tsne(output_path, embeddings=embeddings)\n",
    "        embeddings = combined_embeddings.eval()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is [3.5839138]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './logs/epoch000.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c0dac3a7099f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e7c74878d00c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0moutputLocation\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epoch{:03d}.jpeg\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mgenerate_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2a5f0967e57f>\u001b[0m in \u001b[0;36mgenerate_tsne\u001b[0;34m(path, size, word_count, embeddings)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#get label to assign for each point in embedding space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlabels\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_with_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_dim_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-2a5f0967e57f>\u001b[0m in \u001b[0;36mplot_with_labels\u001b[0;34m(low_dim_embs, labels, path, size)\u001b[0m\n\u001b[1;32m     12\u001b[0m                      va='bottom')\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, frameon, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2076\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_jpg\u001b[0;34m(self, filename_or_obj, dryrun, *args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dpi'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dpi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dpi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         \u001b[0mprint_jpeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_jpg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;31m# Open also for reading (\"+\"), because TIFF save_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m                 \u001b[0;31m# writer needs to go back and edit the written data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs/epoch000.jpeg'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = train(200)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generated\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(log_dir, \"final_embeddings.jpeg\")\n",
    "generate_tsne(output_path, embeddings=embeddings)\n",
    "print(\"Image generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#understand how the full code works with functions written in matrix\n",
    "#refactor for printing and the corpus fully built up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting the t-SNE section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
