{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import corenlp\n",
    "\n",
    "corenlp_tagger = corenlp.CoreNLPParser(url='http://localhost:9001', encoding='utf8', tagtype='ner')\n",
    "\n",
    "\n",
    "input_tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()\n",
    "expected_output = [\n",
    "    ('Rami', 'PERSON'),\n",
    "    ('Eid', 'PERSON'),\n",
    "    ('is', 'O'),\n",
    "    ('studying', 'O'),\n",
    "    ('at', 'O'),\n",
    "    ('Stony', 'ORGANIZATION'),\n",
    "    ('Brook', 'ORGANIZATION'),\n",
    "    ('University', 'ORGANIZATION'),\n",
    "    ('in', 'O'),\n",
    "    ('NY', 'O'),\n",
    "]\n",
    "tagged_output = corenlp_tagger.tag(input_tokens)\n",
    "tagged_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import common.utility as utility\n",
    "import common.config as config\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import pickle\n",
    "import topic_model\n",
    "import topic_model_utility\n",
    "import treaty_corpus\n",
    "\n",
    "DATA_FOLDER = '../data'\n",
    "LANGUAGE = 'en'\n",
    "\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "CORPUS_PATH = os.path.join(DATA_FOLDER, \"treaty_text_corpora_20181206_preprocessed.zip\")\n",
    "\n",
    "treaties = WTI_INDEX.get_treaties(language=LANGUAGE)\n",
    "document_stream = treaty_corpus.get_document_stream(CORPUS_PATH, LANGUAGE, treaties)\n",
    "\n",
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import codecs\n",
    "import time\n",
    "import collections\n",
    "import nltk.tag\n",
    "from nltk.parse import corenlp\n",
    "import nltk.tokenize.stanford as st\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "def extract_entity_phrases(data, classes=[ 'LOCATION', 'PERSON']):\n",
    "\n",
    "    # Extract entities of selected classes, add index to enable merge to phrases\n",
    "    entities = [ (i, word, wclass)\n",
    "        for (i, (word, wclass)) in enumerate(data) if classes is None or wclass in classes ]\n",
    "\n",
    "    # Merge adjacent entities having the same classifier\n",
    "    for i in range(len(entities) - 1, 0, -1):\n",
    "        if entities[i][0] == entities[i - 1][0] + 1 and entities[i][2] == entities[i - 1][2]:\n",
    "            entities[i - 1] = (entities[i - 1][0], entities[i - 1][1] + \" \" + entities[i][1], entities[i - 1][2])\n",
    "            del entities[i]\n",
    "\n",
    "    # Remove index in returned data\n",
    "    return [ (word, wclass) for (i, word, wclass) in entities  ]\n",
    "\n",
    "def create_ner_tagger(options):\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8', tagtype='ner')\n",
    "    return corenlp_tagger\n",
    "\n",
    "def create_tokenizer(options):\n",
    "    corenlp_tokenizer = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8')\n",
    "    return corenlp_tokenizer\n",
    "\n",
    "def create_statistics(entities):\n",
    "    wc = collections.Counter()\n",
    "    wc.update(entities)\n",
    "    return wc\n",
    "\n",
    "def serialize_content(stats, filename, token_count):\n",
    "    document_name, treaty_id, lang = extract_document_info(filename)\n",
    "    data = [ (document_name, treaty_id, lang, word, wclass, stats[(word, wclass)], token_count) for (word, wclass) in stats  ]\n",
    "    content = '\\n'.join(map(lambda x: ';'.join([str(y) for y in x]), data))\n",
    "    return content\n",
    "\n",
    "def write_content(outfile, content):\n",
    "    if content != '':\n",
    "        outfile.write(content)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "def recognize_entities(options):\n",
    "\n",
    "    corenlp_tokenizer = create_tokenizer(options)\n",
    "    corenlp_tagger = create_ner_tagger(options)\n",
    "    \n",
    "    outfile = os.path.join(options['output_folder'], \"output_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\")\n",
    "    tags = [ 'NUMBER', 'LOCATION', 'DATE', 'MISC', 'ORGANIZATION', 'DURATION', 'SET', 'ORDINAL', 'PERSON' ]\n",
    "    \n",
    "    document_stream = treaty_corpus.get_document_stream(options['source_path'], options['language'], treaties)\n",
    "    for treaty_id, language, filename, content in document_stream:\n",
    "        print('treaty_id')\n",
    "        \n",
    "options = {\n",
    "    \"language\": 'en',\n",
    "    \"source_path\": \"../data/treaty_text_corpora_20181206_preprocessed.zip\",\n",
    "    'server_url': 'http://localhost:9001',\n",
    "    'output_folder': DATA_FOLDER,\n",
    "}\n",
    "\n",
    "recognize_entities(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_current_corpus().textacy_corpus\n",
    "gpe = set([])\n",
    "for doc in corpus:\n",
    "    candidates = [ x for x in doc if len(x) > 1 and x.ent_type_ == 'GPE' and x.is_alpha ]\n",
    "    gpe = gpe.union(set([ x.lower_ for x in candidates]))\n",
    "    gpe = gpe.union(set([ x.lemma_ for x in candidates]))\n",
    "\n",
    "df = pd.DataFrame({ 'word': list(gpe)})\n",
    "df.sort_values('word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = '../data/'\n",
    "\n",
    "def include_predicate(filename, options):\n",
    "    \n",
    "options = {\n",
    "    \"language\": 'en',\n",
    "    \"source_path\": \"treaty_text_corpora_20181206_preprocessed.zip\",\n",
    "    'server_url': 'http://localhost:9001',\n",
    "    'output_folder': data_folder,\n",
    "}\n",
    "\n",
    "main(options)\n",
    "\n",
    "    for zip_source in options[\"zip_sources\"]:\n",
    "        with io.open(outfile, 'w', encoding='utf8') as o:\n",
    "            with zipfile.ZipFile(zip_source) as pope_zip:\n",
    "                for filename in pope_zip.namelist():\n",
    "                    with pope_zip.open(filename) as pope_file:\n",
    "                        try:\n",
    "                            text = pope_file.read().decode(\"utf-8\")\n",
    "                            tokens = corenlp_tokenizer.tokenize(text)\n",
    "                            data = corenlp_tagger.tag(tokens)\n",
    "                            entities = extract_entity_phrases(data, tags)  # [ 'LOCATION', 'PERSON', 'ORGANIZATION' ])\n",
    "                            statistics = create_statistics(entities)\n",
    "                            content = serialize_content(statistics, filename, len(tokens))\n",
    "                            write_content(o, content)\n",
    "                        except Exception as ex:\n",
    "                            raise\n",
    "                            print('Failed: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_current_corpus().textacy_corpus\n",
    "gpe = set([])\n",
    "for doc in corpus:\n",
    "    candidates = [ x for x in doc if len(x) > 1 and x.ent_type_ == 'GPE' ]\n",
    "    gpe = gpe.union(set([ x.lower_ for x in candidates]))\n",
    "    gpe = gpe.union(set([ x.lemma_ for x in candidates]))\n",
    "\n",
    "df = pd.DataFrame({ 'word': list(gpe)})\n",
    "df.sort_values('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [get_current_corpus().textacy_corpus[0]]\n",
    "ents = set([])\n",
    "for doc in corpus:\n",
    "    candidates = set([\n",
    "            x.lower_ + ' / ' + ' '.join([ t.ent_type_ for t in x ])\n",
    "        for x in doc.spacy_doc.ents if x.text not in ('', ' ', '\\n', '\\t')])\n",
    "    ents = ents.union(candidates)\n",
    "\n",
    "df_ent = pd.DataFrame({'ent': list(ents)})\n",
    "df_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"FB is hiring a new Vice President of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# the model didn't recognise \"FB\" as an entity :(\n",
    "\n",
    "ORG = doc.vocab.strings[u'ORG']  # get hash value of entity label\n",
    "fb_ent = Span(doc, 0, 1, label=ORG) # create a Span for the new entity\n",
    "doc.ents = list(doc.ents) + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('After', ents)\n",
    "# [(u'FB', 0, 2, 'ORG')] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'as_strings': True,\n",
    "    'named_entities': False,\n",
    "    'ngrams': [1, 2],\n",
    "    'normalize': 'lemma'\n",
    "}\n",
    "kwargs = {\n",
    "    'filter_punct': True,\n",
    "    'filter_stops': True,\n",
    "    'include_pos': ('NOUN', 'PROPN'),\n",
    "    'min_freq': 2\n",
    "}\n",
    "tokenizer_args = {\n",
    "    'args': args,\n",
    "    'kwargs': kwargs,\n",
    "    'extra_stop_words': {},\n",
    "    'mask_gpe': True,\n",
    "    'min_freq': 2,\n",
    "    'max_doc_freq': 0.80    \n",
    "}\n",
    "corpus = get_current_corpus().textacy_corpus\n",
    "fx_terms = lambda: ( textacy_utility.textacy_filter_terms(doc, tokenizer_args) for doc in corpus )\n",
    "terms = fx_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([ x for x in next(terms) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
