{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n",
    "- Open Url https://open-science.humlab.umu.se<br>\n",
    "- Username **phd_user_01, phd_user_02, ..., phd_user_11**<br>\n",
    "- Password **phd_course_2018**<br>\n",
    "- Open folder **phd_course** (just click on it) and notebook **intro_to_topic_modelling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Jupyter Notebook?\n",
    "[Jupyter](http://jupyter.org/) is an open-source software for **interactive and reproducible computing**.<br>\n",
    "<span style=\"float:left\"><img src=\"./images/narrative_new.svg\" style=\"width: 300px;padding: 0; margin: 0;\"></span>\n",
    "<br>\n",
    "> - The **open science movement** is a driving force for Jupyter's popularity.<br>\n",
    "> - Which in part is a response to the **reproducibility crisis in science** and the **statistical crisis in science**<br>\n",
    "> - Jupyter Notebooks contain **excutable code, equations, visualizations and narrative text**.<br>\n",
    "> - It is a **web application** with a simple and easy to use web interface.\n",
    "> - Supports a large number of programming languages (50+ e.g. Python, R, JavaScript)\n",
    "> - Sponsered by large companies such as Google and Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Instructions on How to Use Notebooks\n",
    "- **Menu Help** -> User Interface Tour\n",
    "- **<span style=\"color: red\">CODE CELLS**</span> contain Python script code and have **In [x]** in the left margin\n",
    "  - **In []** indicates that the code cell hasn't been executed yet\n",
    "  - **In [n]** indicates that the code has been executed(n is an integer)\n",
    "  - **In [\\*]** indicates that the code is executing, or waiting to be executed (i.e. other cells are executing)\n",
    "  - **Out[n]** indicates the output (or result) of a cell's execution and is directly below the executed cell\n",
    "- **The current code** is highlighted with a blue border - you make it current by clicking on it\n",
    "- **<span style=\"color: red\">SHIFT+ENTER</span>** or **PLAY BUTTON** executes the current cell. Code cells aren't executed automatically\n",
    "- **SHIFT+ENTER** automatically selects the next code cell\n",
    "- **SHIFT+ENTER** can hence be used repeatedly to executes the code cells in sequence\n",
    "- **Menu Cell -> Run All** executes the entire notebook in a single step (notice how \"In [\\*]\" indicators change to \"In [n]\")\n",
    "- **Double-Click** on a cell to edit its content.\n",
    "- **ESC key** Leaves edit mode (or just click on any other cell).\n",
    "- **Kernel -> Restart** restarts server side kernel (use if notebook seems stuck)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Topic Modelling?\n",
    "\n",
    "Topic modelling (TM) can be thought of as:\n",
    "- A method for finding **groups of words** that in some way **capture the information** in a collection of documents\n",
    "- A form of **text classification** - a way to cluster documents\n",
    "- A form of **text mining** - a way to **find recurring patterns of words** in a collection\n",
    "- TM assumes that documents have these kinds of **underlying patterns**\n",
    "- This means that certain **\"groups of words\" (i.e. topics) occur more frequently** in a specific document\n",
    "- These groups (or patterns) of words are the **topics**\n",
    "\n",
    "**<span style=\"color: red\">It is up to YOU to determine the \"meaning\" of the topics!</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is an LDA Topic Model?\n",
    "- LDA is a so called \"generative probabilistic\" model\n",
    "- The premise of LDA is the assumption that the documents at hand **have been generated via a statistical model/process**\n",
    "- Each topic is in this model a simple **a word frequency distribution**\n",
    "- A simplified view of how a document is generated is\n",
    "  - Select the document's mix of topics (e.g. *Topic X* 40%, *Topic Y* 25%, *Topic Z* 35%)\n",
    "  - Generate the document's words by repeating\n",
    "    - Draw a topic from the topic distribution\n",
    "    - Draw a word from that topic's word distribution\n",
    "\n",
    "**<span style=\"color: black\">Given this imaginary generative process, the corpus at hand is the correct answer!</span>**<br>\n",
    "Commonly used computational processes can be used to fit the corpus to the statistical model, which gives the topic distributions.<br>\n",
    "See *(Blei, 2003: Latent dirichlet allocation* [PDF](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<span style=\"float:left\">\n",
    "    <img src=\"./images/blei_lda.jpg\" style=\"width: 600px;padding: 0; margin: 0;\">\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<span style=\"float:left\">\n",
    "    <img src=\"./images/blei_2012b.png\" style=\"width: 600px;padding: 0; margin: 0;\">\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenges\n",
    "- **Whatâ€™s easy for humans can be extremely hard for computers**\n",
    "  - **Ambiguity** and fuzziness of terms and phrases\n",
    "  - Poor **data quality**, errors in data, wrong data, missing data, ambigious data\n",
    "  - Understand **domain contexts**, metadata, domain-specific data\n",
    "  - **Data size** (to much, to little)\n",
    "  - How to understand **internal representations** of data used by computational methods\n",
    "  - Internal representations are **simplified views** of the actual data (e.g. \"bag-of-words\" model)\n",
    "  - How to verify **performance** (correctness of result)<br>\n",
    "  - etc...\n",
    "\n",
    "**Human-in-the-loop or supervised learning can be very expensive**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Risks\n",
    "- Using tools and methods **without fully understanding** them\n",
    "- Using tools and methods **for non-intended purposes or in new contexts**\n",
    "- Risk of **data dredging**, p-hacking, \"the statistical crisis\".\n",
    "- The risk that **engineer makes micro-decisions** the researcher don't know about, or don't fully understand.\n",
    "- The risk of **reading to much into visualizations** (networks, layouts, clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Sample High-Level Text Analysis Workflow\n",
    "\n",
    "<img src=\"./images/text_analysis_workflow.svg\" alt=\"\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: blue'>IDENTIFY</span> 25 Academic Papers <span style='color: blue; float: right'></span>  \n",
    "The source data consists of 25 english academic articles downloded as PDF from Zotera.\n",
    "## <span style='color: green'>COLLECT</span> Extract Text From PDFs <span style='color: blue; float: right'>SKIP</span>  \n",
    "The first step is to extract the text from the PDF files. This can also be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed, PDFPage\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "\n",
    "def extract_pdf_text(filename):\n",
    "    text_lines = []\n",
    "    with open(filename, 'rb') as fp:\n",
    "        \n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "\n",
    "        resource_manager = PDFResourceManager()\n",
    "\n",
    "        result_buffer = StringIO()\n",
    "\n",
    "        device = TextConverter(resource_manager, result_buffer, codec='utf-8', laparams=LAParams())\n",
    "\n",
    "        interpreter = PDFPageInterpreter(resource_manager, device)\n",
    "\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "        lines = result_buffer.getvalue().splitlines()\n",
    "        for line in lines:\n",
    "            text_lines.append(line)\n",
    "\n",
    "    return text_lines\n",
    "\n",
    "\n",
    "def extract_pdf_texts(source_folder, target_zip_filename):\n",
    "    with zipfile.ZipFile(target_zip_filename, 'w', zipfile.ZIP_DEFLATED) as target_zip:\n",
    "\n",
    "        for filename in glob.glob(os.path.join(source_folder,'*.pdf')):\n",
    "\n",
    "            print('Processing: ' + filename)\n",
    "\n",
    "            text_lines = extract_pdf_text(filename)\n",
    "\n",
    "            target_filename = os.path.splitext(os.path.split(filename)[1])[0] + '.txt'\n",
    "            target_filename = target_filename.lower().replace(' ', '_').replace(',','')\n",
    "\n",
    "            target_zip.writestr(target_filename, '\\n'.join(text_lines))\n",
    "            \n",
    "#source_folder = './data/pdf'\n",
    "#target_zip_filename = 'data/paper_extracted_texts.zip'\n",
    "#extract_pdf_texts(source_folder, target_zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>INITIALIZE </span> Setup and Initialize the Notebook <span style='color: red; float: right'>MANDATORY RUN!</span>  \n",
    "The following CODE CELL must be run once to set up the run time environment. Please select the cell and hit **SHIFT-ENTER** or the **RUN** button in the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 13:58:25,581 : INFO : POS tag set: PUNCT SYM X ADJ VERB CONJ NUM DET ADV ADP  NOUN PROPN PART PRON SPACE INTJ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# folded code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, warnings, types, sys\n",
    "import numpy as np, pandas as pd\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re, string, zipfile\n",
    "import nltk, spacy, textacy, textacy.extract, textacy.preprocess\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "from gensim import corpora, models, matutils\n",
    "from IPython.display import display, HTML #, clear_output, IFrame\n",
    "from pivottablejs import pivot_ui\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "logger = utility.getLogger() #format=\"%(levelname)s;%(message)s\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "pd.set_option('precision', 10)\n",
    "\n",
    "def get_filenames(zip_filename, extension='.txt'):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return [ x for x in zf.namelist() if x.endswith(extension) ]\n",
    "    \n",
    "def get_text(zip_filename, filename):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return zf.read(filename).decode(encoding='utf-8')\n",
    "\n",
    "DEFAULT_TERM_PARAMS = dict(\n",
    "    args=dict(ngrams=1, named_entities=True, normalize='lemma', as_strings=True),\n",
    "    kwargs=dict(filter_stops=True, filter_punct=True, filter_nums=True, min_freq=1, drop_determiners=True, include_pos=('NOUN', 'PROPN', ))\n",
    ")\n",
    "\n",
    "FIXED_STOPWORDS = ['', '\\n', 'et', 'al', 'et.al.' ]\n",
    "def filter_terms(doc, term_args, chunk_size=None, min_length=2):\n",
    "    kwargs = utility.extend({}, DEFAULT_TERM_PARAMS['kwargs'], term_args['kwargs'])\n",
    "    args = utility.extend({}, DEFAULT_TERM_PARAMS['args'], term_args['args'])\n",
    "    terms = (x for x in doc.to_terms_list(\n",
    "        args['ngrams'],\n",
    "        args['named_entities'],\n",
    "        args['normalize'],\n",
    "        args['as_strings'],\n",
    "        **kwargs\n",
    "    ) if len(x) >= min_length and x not in FIXED_STOPWORDS)\n",
    "    return terms\n",
    "        \n",
    "def slim_title(x):\n",
    "    try:\n",
    "        m = re.match('.*\\((.*)\\)$', x).groups()\n",
    "        if m is not None and len(m) > 0:\n",
    "            return m[0]\n",
    "        return ' '.join(x.split(' ')[:3]) + '...'\n",
    "    except:\n",
    "        return x\n",
    "            \n",
    "LANGUAGE = 'en'\n",
    "SOURCE_FOLDER = '../data'\n",
    "\n",
    "EXTRACTED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'dummy.zip')\n",
    "EDITED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'dummy.zip')\n",
    "PREPROCESSED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'treaty_text_corpora_20181115_preprocessed.zip')\n",
    "\n",
    "SOURCE_FILES = {\n",
    "    'source_text_raw': { 'filename': EXTRACTED_TEXT_FILENAME, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "    'source_text_edited': { 'filename': EDITED_TEXT_FILENAME, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "    'source_text_preprocessed': { 'filename': PREPROCESSED_TEXT_FILENAME, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "}\n",
    "\n",
    "HYPHEN_REGEXP = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "AGGREGATES = { 'mean': np.mean, 'sum': np.sum, 'max': np.max, 'std': np.std }\n",
    "\n",
    "logger.info('POS tag set: ' + ' '.join(list(DF_TAGSET.POS.unique())))\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg') #'pdf', 'svg')\n",
    "    \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "class TopicModelNotComputed(Exception):\n",
    "    @staticmethod\n",
    "    def check():\n",
    "        if 'TM_GUI_MODEL' in globals():\n",
    "            gui =  globals()['TM_GUI_MODEL']\n",
    "            if None not in (gui, gui.model):\n",
    "                return True\n",
    "        msg = 'A topic model must be computed using step \"MODEL Compute an LDA Topic Model\"'\n",
    "        raise TopicModelNotComputed(msg)\n",
    "\n",
    "def get_current_model():\n",
    "    TopicModelNotComputed.check()\n",
    "    return globals()['TM_GUI_MODEL'].model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE: </span> Load and Prepare the Text Corpus <span style='color: red; float: right'>MANDATORY RUN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 13:59:04,334 : INFO : Loading model: english...\n",
      "2018-11-20 13:59:11,685 : INFO : Using pipeline: tagger parser ner\n",
      "2018-11-20 13:59:11,686 : INFO : Working: Computing new corpus ../data/corpus_en__disable().pkl...\n",
      "2018-11-20 13:59:11,714 : INFO : Processing 475782_fr_corr.txt\n",
      "2018-11-20 13:59:13,153 : INFO : Processing 475812_fr_corr.txt\n",
      "2018-11-20 13:59:13,976 : INFO : Processing 476047_fr_corr.txt\n",
      "2018-11-20 13:59:14,662 : INFO : Processing 507333_de_corr.txt\n",
      "2018-11-20 13:59:15,170 : INFO : Processing 507361_de_corr.txt\n",
      "2018-11-20 13:59:15,552 : INFO : Processing 507506_de_corr.txt\n",
      "2018-11-20 13:59:15,985 : INFO : Processing 507610_de_corr.txt\n",
      "2018-11-20 13:59:16,369 : INFO : Processing 570123_en_corr.txt\n",
      "2018-11-20 13:59:16,814 : INFO : Processing 724069_fr_corr.txt\n",
      "2018-11-20 13:59:17,091 : INFO : Processing 724129_fr_corr.txt\n",
      "2018-11-20 13:59:17,255 : INFO : Processing 724170_fr_corr.txt\n",
      "2018-11-20 13:59:18,386 : INFO : Processing XXX010_de.txt\n",
      "2018-11-20 13:59:18,539 : INFO : Processing XXX010_fr_corr.txt\n",
      "2018-11-20 13:59:19,850 : INFO : Processing XXX010_it_corr.txt\n",
      "2018-11-20 13:59:21,126 : INFO : Processing XXX011_fr_corr.txt\n",
      "2018-11-20 13:59:21,593 : INFO : Processing XXX020_en.txt\n",
      "2018-11-20 13:59:21,766 : INFO : Processing XXX020_fr_corr.txt\n",
      "2018-11-20 13:59:21,931 : INFO : Processing XXX022_fr_corr.txt\n",
      "2018-11-20 13:59:22,377 : INFO : Processing XXX023_fr_corr.txt\n",
      "2018-11-20 13:59:22,925 : INFO : Processing XXX024_en.txt\n",
      "2018-11-20 13:59:23,329 : INFO : Processing XXX024_fr_corr.txt\n",
      "2018-11-20 13:59:23,734 : INFO : Processing XXX026_fr_corr.txt\n",
      "2018-11-20 13:59:24,362 : INFO : Processing XXX027_fr_corr.txt\n",
      "2018-11-20 13:59:24,748 : INFO : Processing XXX028_fr_corr.txt\n",
      "2018-11-20 13:59:25,032 : INFO : Processing XXX043_de_corr.txt\n",
      "2018-11-20 13:59:25,749 : INFO : Processing XXX045_de_corr.txt\n",
      "2018-11-20 13:59:26,481 : INFO : Processing XXX046_it.txt\n",
      "2018-11-20 13:59:28,113 : INFO : Processing 10707_en_corr.txt\n",
      "2018-11-20 13:59:28,312 : INFO : Processing 10707_fr_corr.txt\n",
      "2018-11-20 13:59:28,526 : INFO : Processing 100075_en_corr.txt\n",
      "2018-11-20 13:59:29,059 : INFO : Processing 100075_fr_corr.txt\n",
      "2018-11-20 13:59:29,621 : INFO : Processing 100139_en_corr.txt\n",
      "2018-11-20 13:59:30,169 : INFO : Processing 100181_en_corr.txt\n",
      "2018-11-20 13:59:30,721 : INFO : Processing 100181_fr_corr.txt\n",
      "2018-11-20 13:59:31,275 : INFO : Processing 100182_en_corr.txt\n",
      "2018-11-20 13:59:32,203 : INFO : Processing 100182_fr_corr.txt\n",
      "2018-11-20 13:59:33,295 : INFO : Processing 100230_en_corr.txt\n",
      "2018-11-20 13:59:33,889 : INFO : Processing 100266_en_corr.txt\n",
      "2018-11-20 13:59:34,476 : INFO : Processing 100366_en_corr.txt\n",
      "2018-11-20 13:59:35,021 : INFO : Processing 100368_en_corr.txt\n",
      "2018-11-20 13:59:35,541 : INFO : Processing 100487_en_corr.txt\n",
      "2018-11-20 13:59:36,039 : INFO : Processing 100493_en_corr.txt\n",
      "2018-11-20 13:59:36,515 : INFO : Processing 100493_fr_corr.txt\n",
      "2018-11-20 13:59:37,084 : INFO : Processing 100514_en_corr.txt\n",
      "2018-11-20 13:59:37,878 : INFO : Processing 100515_en-corr.txt\n",
      "2018-11-20 13:59:38,448 : INFO : Processing 100516_en_corr.txt\n",
      "2018-11-20 13:59:39,221 : INFO : Processing 100526_en_corr.txt\n",
      "2018-11-20 13:59:39,847 : INFO : Processing 100526_fr_corr.txt\n",
      "2018-11-20 13:59:40,577 : INFO : Processing 100528_en_corr.txt\n",
      "2018-11-20 13:59:41,143 : INFO : Processing 100641_en_corr.txt\n",
      "2018-11-20 13:59:41,560 : INFO : Processing 100698_en_corr.txt\n",
      "2018-11-20 13:59:42,168 : INFO : Processing 100699_en_corr.txt\n",
      "2018-11-20 13:59:42,964 : INFO : Processing 100700_en_corr.txt\n",
      "2018-11-20 13:59:43,481 : INFO : Processing 100700_fr_corr.txt\n",
      "2018-11-20 13:59:44,115 : INFO : Processing 100701_en_corr.txt\n",
      "2018-11-20 13:59:44,887 : INFO : Processing 100701_fr_corr.txt\n",
      "2018-11-20 13:59:45,791 : INFO : Processing 100707_en_corr.txt\n",
      "2018-11-20 13:59:46,361 : INFO : Processing 100892_en_corr.txt\n",
      "2018-11-20 13:59:46,779 : INFO : Processing 100892_fr_corr.txt\n",
      "2018-11-20 13:59:47,476 : INFO : Processing 100982_en_corr.txt\n",
      "2018-11-20 13:59:47,955 : INFO : Processing 100990_en_corr.txt\n",
      "2018-11-20 13:59:48,725 : INFO : Processing 100990_fr_corr.txt\n",
      "2018-11-20 13:59:49,715 : INFO : Processing 101003_en_corr.txt\n",
      "2018-11-20 13:59:49,898 : INFO : Processing 101099_en_corr.txt\n",
      "2018-11-20 13:59:50,568 : INFO : Processing 101099_fr_corr.txt\n",
      "2018-11-20 13:59:51,347 : INFO : Processing 101568_en_corr.txt\n",
      "2018-11-20 13:59:51,803 : INFO : Processing 102135_en_corr.txt\n",
      "2018-11-20 13:59:52,448 : INFO : Processing 102237_en_corr.txt\n",
      "2018-11-20 13:59:53,056 : INFO : Processing 102237_fr_corr.txt\n",
      "2018-11-20 13:59:53,738 : INFO : Processing 102238_en_corr.txt\n",
      "2018-11-20 13:59:54,364 : INFO : Processing 102238_fr_corr.txt\n",
      "2018-11-20 13:59:55,134 : INFO : Processing 102333_en_corr.txt\n",
      "2018-11-20 13:59:55,689 : INFO : Processing 102334_en_corr.txt\n",
      "2018-11-20 13:59:55,961 : INFO : Processing 102343_en_corr.txt\n",
      "2018-11-20 13:59:56,402 : INFO : Processing 102425_en_corr.txt\n",
      "2018-11-20 13:59:56,782 : INFO : Processing 102431_en_corr.txt\n",
      "2018-11-20 13:59:57,090 : INFO : Processing 102431_fr_corr.txt\n",
      "2018-11-20 13:59:57,521 : INFO : Processing 102533_en_corr.txt\n",
      "2018-11-20 13:59:57,791 : INFO : Processing 102570_en_corr.txt\n",
      "2018-11-20 13:59:58,601 : INFO : Processing 102570_fr_corr.txt\n",
      "2018-11-20 13:59:59,567 : INFO : Processing 102583_en_corr.txt\n",
      "2018-11-20 13:59:59,746 : INFO : Processing 102708_en_corr.txt\n",
      "2018-11-20 14:00:00,097 : INFO : Processing 102750_en_corr.txt\n",
      "2018-11-20 14:00:00,384 : INFO : Processing 102750_fr_corr.txt\n",
      "2018-11-20 14:00:00,693 : INFO : Processing 102854_en_corr.txt\n",
      "2018-11-20 14:00:01,247 : INFO : Processing 102886_en_corr.txt\n",
      "2018-11-20 14:00:01,605 : INFO : Processing 102886_fr_corr.txt\n",
      "2018-11-20 14:00:02,049 : INFO : Processing 103125_en_corr.txt\n",
      "2018-11-20 14:00:02,615 : INFO : Processing 103125_fr_corr.txt\n",
      "2018-11-20 14:00:03,215 : INFO : Processing 103187_en_corr.txt\n",
      "2018-11-20 14:00:03,468 : INFO : Processing 103439_en_corr.txt\n",
      "2018-11-20 14:00:03,644 : INFO : Processing 103525_en_corr.txt\n",
      "2018-11-20 14:00:03,919 : INFO : Processing 103569_en_corr.txt\n",
      "2018-11-20 14:00:04,282 : INFO : Processing 103660_en_corr.txt\n",
      "2018-11-20 14:00:04,620 : INFO : Processing 103687_en_corr.txt\n",
      "2018-11-20 14:00:04,865 : INFO : Processing 103692_en_corr.txt\n",
      "2018-11-20 14:00:05,341 : INFO : Processing 103692_fr_corr.txt\n",
      "2018-11-20 14:00:05,885 : INFO : Processing 103693_en_corr.txt\n",
      "2018-11-20 14:00:06,174 : INFO : Processing 103694_en_corr.txt\n",
      "2018-11-20 14:00:06,665 : INFO : Processing 103695_en_corr.txt\n",
      "2018-11-20 14:00:06,956 : INFO : Processing 103696_en_corr.txt\n",
      "2018-11-20 14:00:07,660 : INFO : Processing 103697_en_corr.txt\n",
      "2018-11-20 14:00:08,011 : INFO : Processing 103698_en_corr.txt\n",
      "2018-11-20 14:00:08,364 : INFO : Processing 103699_en_corr.txt\n",
      "2018-11-20 14:00:08,705 : INFO : Processing 103700_en_corr.txt\n",
      "2018-11-20 14:00:09,313 : INFO : Processing 103707_en_corr.txt\n",
      "2018-11-20 14:00:09,722 : INFO : Processing 103713_en_corr.txt\n",
      "2018-11-20 14:00:09,966 : INFO : Processing 103766_en_corr.txt\n",
      "2018-11-20 14:00:10,356 : INFO : Processing 103766_fr_corr.txt\n",
      "2018-11-20 14:00:10,833 : INFO : Processing 103770_en_corr.txt\n",
      "2018-11-20 14:00:11,160 : INFO : Processing 103839_en_corr.txt\n",
      "2018-11-20 14:00:11,838 : INFO : Processing 103847_en_corr.txt\n",
      "2018-11-20 14:00:12,086 : INFO : Processing 103961_en_corr.txt\n",
      "2018-11-20 14:00:12,378 : INFO : Processing 103962_en_corr.txt\n",
      "2018-11-20 14:00:12,823 : INFO : Processing 103990_en_corr.txt\n",
      "2018-11-20 14:00:13,176 : INFO : Processing 104059_en_corr.txt\n",
      "2018-11-20 14:00:13,385 : INFO : Processing 104064_en_corr.txt\n",
      "2018-11-20 14:00:13,806 : INFO : Processing 104076_en.txt\n",
      "2018-11-20 14:00:14,073 : INFO : Processing 104079_en.txt\n",
      "2018-11-20 14:00:14,382 : INFO : Processing 104097_en.txt\n",
      "2018-11-20 14:00:14,662 : INFO : Processing 104145_en.txt\n",
      "2018-11-20 14:00:15,203 : INFO : Processing 104271_en.txt\n",
      "2018-11-20 14:00:15,511 : INFO : Processing 104317_en.txt\n",
      "2018-11-20 14:00:15,852 : INFO : Processing 104339_en.txt\n",
      "2018-11-20 14:00:16,160 : INFO : Processing 104374_en_corr.txt\n",
      "2018-11-20 14:00:16,441 : INFO : Processing 104394_en_corr.txt\n",
      "2018-11-20 14:00:16,938 : INFO : Processing 104394_fr_corr.txt\n",
      "2018-11-20 14:00:17,549 : INFO : Processing 104396_en.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 14:00:17,920 : INFO : Processing 104421_en.txt\n",
      "2018-11-20 14:00:18,103 : INFO : Processing 104422_en.txt\n",
      "2018-11-20 14:00:18,452 : INFO : Processing 104457_en.txt\n",
      "2018-11-20 14:00:18,674 : INFO : Processing 104530_en_corr.txt\n",
      "2018-11-20 14:00:19,536 : INFO : Processing 104530_fr_corr.txt\n",
      "2018-11-20 14:00:20,565 : INFO : Processing 104547_en.txt\n",
      "2018-11-20 14:00:20,820 : INFO : Processing 104567_en.txt\n",
      "2018-11-20 14:00:21,037 : INFO : Processing 104622_en_corr.txt\n",
      "2018-11-20 14:00:21,307 : INFO : Processing 104622_fr_corr.txt\n",
      "2018-11-20 14:00:21,613 : INFO : Processing 104625_en.txt\n",
      "2018-11-20 14:00:21,899 : INFO : Processing 104626_en_corr.txt\n",
      "2018-11-20 14:00:22,204 : INFO : Processing 104626_fr_corr.txt\n",
      "2018-11-20 14:00:22,655 : INFO : Processing 104633_en.txt\n",
      "2018-11-20 14:00:22,926 : INFO : Processing 104635_en_corr (1).txt\n",
      "2018-11-20 14:00:23,214 : INFO : Processing 104635_fr_corr.txt\n",
      "2018-11-20 14:00:23,552 : INFO : Processing 104654_en.txt\n",
      "2018-11-20 14:00:23,821 : INFO : Processing 104692_en.txt\n",
      "2018-11-20 14:00:24,063 : INFO : Processing 104697_en.txt\n",
      "2018-11-20 14:00:24,265 : INFO : Processing 104720_en.txt\n",
      "2018-11-20 14:00:24,612 : INFO : Processing 104843_en_corr.txt\n",
      "2018-11-20 14:00:24,946 : INFO : Processing 104903_en.txt\n",
      "2018-11-20 14:00:25,214 : INFO : Processing 104907_en.txt\n",
      "2018-11-20 14:00:25,589 : INFO : Processing 104908_en.txt\n",
      "2018-11-20 14:00:25,931 : INFO : Processing 104911_en_corr.txt\n",
      "2018-11-20 14:00:26,212 : INFO : Processing 104911_fr_corr.txt\n",
      "2018-11-20 14:00:26,530 : INFO : Processing 104913_en.txt\n",
      "2018-11-20 14:00:26,847 : INFO : Processing 104928_en_corr.txt\n",
      "2018-11-20 14:00:27,469 : INFO : Processing 104928_fr_corr.txt\n",
      "2018-11-20 14:00:28,168 : INFO : Processing 104959_en.txt\n",
      "2018-11-20 14:00:28,389 : INFO : Processing 104962_en.txt\n",
      "2018-11-20 14:00:28,556 : INFO : Processing 105025_en.txt\n",
      "2018-11-20 14:00:28,708 : INFO : Processing 105032_en_corr.txt\n",
      "2018-11-20 14:00:29,929 : INFO : Processing 105032_fr_corr.txt\n",
      "2018-11-20 14:00:31,324 : INFO : Processing 105095_en.txt\n",
      "2018-11-20 14:00:31,594 : INFO : Processing 105118_en.txt\n",
      "2018-11-20 14:00:31,983 : INFO : Processing 105134_en.txt\n",
      "2018-11-20 14:00:32,256 : INFO : Processing 105145_en_corr.txt\n",
      "2018-11-20 14:00:32,589 : INFO : Processing 105145_fr_corr.txt\n",
      "2018-11-20 14:00:32,884 : INFO : Processing 105172_en.txt\n",
      "2018-11-20 14:00:35,838 : INFO : Processing 105238_en.txt\n",
      "2018-11-20 14:00:36,027 : INFO : Processing 105339_en_corr.txt\n",
      "2018-11-20 14:00:36,439 : INFO : Processing 105339_fr_corr.txt\n",
      "2018-11-20 14:00:36,931 : INFO : Processing 105379_en.txt\n",
      "2018-11-20 14:00:37,099 : INFO : Processing 105404_en_corr.txt\n",
      "2018-11-20 14:00:38,006 : INFO : Processing 105404_fr_corr.txt\n",
      "2018-11-20 14:00:38,961 : INFO : Processing 105406_en.txt\n",
      "2018-11-20 14:00:39,159 : INFO : Processing 105444_en.txt\n",
      "2018-11-20 14:00:39,374 : INFO : Processing 105538_en.txt\n",
      "2018-11-20 14:00:39,634 : INFO : Processing 105554_en.txt\n",
      "2018-11-20 14:00:39,856 : INFO : Processing 105560_en_corr.txt\n",
      "2018-11-20 14:00:40,218 : INFO : Processing 105560_fr_corr.txt\n",
      "2018-11-20 14:00:40,643 : INFO : Processing 105589_en.txt\n",
      "2018-11-20 14:00:40,917 : INFO : Processing 105590_en.txt\n",
      "2018-11-20 14:00:41,135 : INFO : Processing 105642_en.txt\n",
      "2018-11-20 14:00:41,515 : INFO : Processing 105642_fr_corr.txt\n",
      "2018-11-20 14:00:41,941 : INFO : Processing 105644_en_corr.txt\n",
      "2018-11-20 14:00:42,245 : INFO : Processing 105699_en_corr.txt\n",
      "2018-11-20 14:00:42,576 : INFO : Processing 105717_en_corr.txt\n",
      "2018-11-20 14:00:43,173 : INFO : Processing 105717_fr_corr.txt\n",
      "2018-11-20 14:00:43,826 : INFO : Processing 105735_en_corr.txt\n",
      "2018-11-20 14:00:44,307 : INFO : Processing 105736_en_corr.txt\n",
      "2018-11-20 14:00:44,576 : INFO : Processing 105759_en_corr.txt\n",
      "2018-11-20 14:00:45,468 : INFO : Processing 105810_en_corr.txt\n",
      "2018-11-20 14:00:46,833 : INFO : Processing 105810_fr_corr.txt\n",
      "2018-11-20 14:00:48,403 : INFO : Processing 105818_en.txt\n",
      "2018-11-20 14:00:48,654 : INFO : Processing 105818_fr_corr.txt\n",
      "2018-11-20 14:00:48,960 : INFO : Processing 105874_en_corr.txt\n",
      "2018-11-20 14:00:49,368 : INFO : Processing 105874_fr_corr.txt\n",
      "2018-11-20 14:00:49,853 : INFO : Processing 105910_en_corr.txt\n",
      "2018-11-20 14:00:50,113 : INFO : Processing 105945_en_corr.txt\n",
      "2018-11-20 14:00:50,314 : INFO : Processing 105966_en_corr.txt\n",
      "2018-11-20 14:00:50,831 : INFO : Processing 105966_fr-corr.txt\n",
      "2018-11-20 14:00:51,419 : INFO : Processing 105971_en_corr.txt\n",
      "2018-11-20 14:00:52,006 : INFO : Processing 105971_fr_corr.txt\n",
      "2018-11-20 14:00:52,759 : INFO : Processing 106048_en_corr.txt\n",
      "2018-11-20 14:00:53,004 : INFO : Processing 106049_en_corr.txt\n",
      "2018-11-20 14:00:53,364 : INFO : Processing 106066_en_corr.txt\n",
      "2018-11-20 14:00:53,898 : INFO : Processing 106090_en_corr.txt\n",
      "2018-11-20 14:00:54,169 : INFO : Processing 106097_en_corr.txt\n",
      "2018-11-20 14:00:54,333 : INFO : Processing 106134_en_corr.txt\n",
      "2018-11-20 14:00:54,628 : INFO : Processing 106171_en_corr.txt\n",
      "2018-11-20 14:00:54,874 : INFO : Processing 106213_en_corr.txt\n",
      "2018-11-20 14:00:55,143 : INFO : Processing 106218_en_corr.txt\n",
      "2018-11-20 14:00:55,449 : INFO : Processing 106219_en_corr.txt\n",
      "2018-11-20 14:00:55,751 : INFO : Processing 106220_en_corr.txt\n",
      "2018-11-20 14:00:56,070 : INFO : Processing 106221_en_corr.txt\n",
      "2018-11-20 14:00:56,245 : INFO : Processing 106224_en_corr.txt\n",
      "2018-11-20 14:00:56,547 : INFO : Processing 106272_en_corr.txt\n",
      "2018-11-20 14:00:56,901 : INFO : Processing 106284_en_corr.txt\n",
      "2018-11-20 14:00:57,212 : INFO : Processing 106285_en_corr.txt\n",
      "2018-11-20 14:00:57,560 : INFO : Processing 106297_en_corr.txt\n",
      "2018-11-20 14:00:57,838 : INFO : Processing 106306_en_corr.txt\n",
      "2018-11-20 14:00:58,116 : INFO : Processing 106309_en_corr.txt\n",
      "2018-11-20 14:00:58,461 : INFO : Processing 106314_en_corr.txt\n",
      "2018-11-20 14:00:58,790 : INFO : Processing 106321_en_corr.txt\n",
      "2018-11-20 14:00:59,161 : INFO : Processing 106322_en_corr.txt\n",
      "2018-11-20 14:00:59,421 : INFO : Processing 106323_en_corr.txt\n",
      "2018-11-20 14:00:59,791 : INFO : Processing 106426_en_corr.txt\n",
      "2018-11-20 14:01:00,115 : INFO : Processing 106426_fr_corr.txt\n",
      "2018-11-20 14:01:00,473 : INFO : Processing 106431_en_corr.txt\n",
      "2018-11-20 14:01:00,741 : INFO : Processing 106442_en_corr.txt\n",
      "2018-11-20 14:01:00,959 : INFO : Processing 106447_en_corr.txt\n",
      "2018-11-20 14:01:01,172 : INFO : Processing 106460_en_corr.txt\n",
      "2018-11-20 14:01:01,652 : INFO : Processing 106547_en_corr.txt\n",
      "2018-11-20 14:01:01,963 : INFO : Processing 106573_en_corr.txt\n",
      "2018-11-20 14:01:02,175 : INFO : Processing 106574_en_corr.txt\n",
      "2018-11-20 14:01:02,508 : INFO : Processing 106588_en_corr.txt\n",
      "2018-11-20 14:01:02,881 : INFO : Processing 106589_en_corr.txt\n",
      "2018-11-20 14:01:03,365 : INFO : Processing 106601_en.txt\n",
      "2018-11-20 14:01:03,662 : INFO : Processing 106704_en_corr.txt\n",
      "2018-11-20 14:01:03,891 : INFO : Processing 106797_en_corr.txt\n",
      "2018-11-20 14:01:04,375 : INFO : Processing 106797_fr_corr.txt\n",
      "2018-11-20 14:01:04,922 : INFO : Processing 106812_en_corr.txt\n",
      "2018-11-20 14:01:05,238 : INFO : Processing 106860_en_corr.txt\n",
      "2018-11-20 14:01:05,489 : INFO : Processing 106874_en_corr.txt\n",
      "2018-11-20 14:01:06,084 : INFO : Processing 106882_en_corr.txt\n",
      "2018-11-20 14:01:06,321 : INFO : Processing 106887_en_corr.txt\n",
      "2018-11-20 14:01:07,599 : INFO : Processing 106887_fr_corr.txt\n",
      "2018-11-20 14:01:08,971 : INFO : Processing 106960_en_corr.txt\n",
      "2018-11-20 14:01:09,283 : INFO : Processing 106987_en_corr.txt\n",
      "2018-11-20 14:01:09,625 : INFO : Processing 107020_en_corr.txt\n",
      "2018-11-20 14:01:09,882 : INFO : Processing 107024_en_corr.txt\n",
      "2018-11-20 14:01:10,144 : INFO : Processing 107046_en_corr.txt\n",
      "2018-11-20 14:01:10,487 : INFO : Processing 107095_en_corr.txt\n",
      "2018-11-20 14:01:10,908 : INFO : Processing 107095_fr_corr.txt\n",
      "2018-11-20 14:01:11,391 : INFO : Processing 107126_en_corr.txt\n",
      "2018-11-20 14:01:11,732 : INFO : Processing 107142_en_corr.txt\n",
      "2018-11-20 14:01:12,780 : INFO : Processing 107324_en.txt\n",
      "2018-11-20 14:01:13,066 : INFO : Processing 107448_en_corr.txt\n",
      "2018-11-20 14:01:13,392 : INFO : Processing 107458_en_corr.txt\n",
      "2018-11-20 14:01:13,665 : INFO : Processing 107464_en_corr.txt\n",
      "2018-11-20 14:01:14,092 : INFO : Processing 107504_en_corr.txt\n",
      "2018-11-20 14:01:14,304 : INFO : Processing 107504_fr_corr.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 14:01:14,551 : INFO : Processing 107505_en_corr.txt\n",
      "2018-11-20 14:01:14,873 : INFO : Processing 107513_en_corr.txt\n",
      "2018-11-20 14:01:15,161 : INFO : Processing 107531_en.txt\n",
      "2018-11-20 14:01:15,391 : INFO : Processing 107534_en_corr.txt\n",
      "2018-11-20 14:01:15,859 : INFO : Processing 107551_en_corr.txt\n",
      "2018-11-20 14:01:16,268 : INFO : Processing 107638_en_corr.txt\n",
      "2018-11-20 14:01:16,628 : INFO : Processing 107639_en_corr.txt\n",
      "2018-11-20 14:01:16,957 : INFO : Processing 107641_en_corr.txt\n",
      "2018-11-20 14:01:17,388 : INFO : Processing 107642_en_corr.txt\n",
      "2018-11-20 14:01:17,600 : INFO : Processing 107681_en_corr.txt\n",
      "2018-11-20 14:01:17,891 : INFO : Processing 107701_en_corr.txt\n",
      "2018-11-20 14:01:18,205 : INFO : Processing 107785_en_corr.txt\n",
      "2018-11-20 14:01:18,675 : INFO : Processing 107789_en_corr.txt\n",
      "2018-11-20 14:01:19,432 : INFO : Processing 107790_en_corr.txt\n",
      "2018-11-20 14:01:20,050 : INFO : Processing 107792_en_corr.txt\n",
      "2018-11-20 14:01:20,507 : INFO : Processing 107817_en_corr.txt\n",
      "2018-11-20 14:01:20,850 : INFO : Processing 107834_en_corr.txt\n",
      "2018-11-20 14:01:21,268 : INFO : Processing 107834_fr_corr.txt\n",
      "2018-11-20 14:01:21,741 : INFO : Processing 107896_en_corr.txt\n",
      "2018-11-20 14:01:23,402 : INFO : Processing 107896_fr_corr.txt\n",
      "2018-11-20 14:01:25,267 : INFO : Processing 107908_en_corr.txt\n",
      "2018-11-20 14:01:25,601 : INFO : Processing 107925_en_corr.txt\n",
      "2018-11-20 14:01:26,404 : INFO : Processing 107927_de_corr.txt\n",
      "2018-11-20 14:01:27,247 : INFO : Processing 107927_en_corr.txt\n",
      "2018-11-20 14:01:28,102 : INFO : Processing 107927_fr_corr.txt\n",
      "2018-11-20 14:01:28,979 : INFO : Processing 107934_en_corr.txt\n",
      "2018-11-20 14:01:30,112 : INFO : Processing 107934_fr_corr.txt\n",
      "2018-11-20 14:01:31,391 : INFO : Processing 107949_en_corr.txt\n",
      "2018-11-20 14:01:31,772 : INFO : Processing 108006_en_corr.txt\n",
      "2018-11-20 14:01:32,101 : INFO : Processing 108022_en_corr.txt\n",
      "2018-11-20 14:01:32,409 : INFO : Processing 108050_en_corr.txt\n",
      "2018-11-20 14:01:32,684 : INFO : Processing 108051_en_corr.txt\n",
      "2018-11-20 14:01:33,325 : INFO : Processing 108051_fr_corr.txt\n",
      "2018-11-20 14:01:34,027 : INFO : Processing 108053_en_corr.txt\n",
      "2018-11-20 14:01:34,484 : INFO : Processing 108078_en_corr.txt\n",
      "2018-11-20 14:01:34,639 : INFO : Processing 108106_en_corr.txt\n",
      "2018-11-20 14:01:34,877 : INFO : Processing 108202_en_corr.txt\n",
      "2018-11-20 14:01:35,128 : INFO : Processing 108242_en_corr.txt\n",
      "2018-11-20 14:01:35,901 : INFO : Processing 108281_en_corr.txt\n",
      "2018-11-20 14:01:36,279 : INFO : Processing 108293_en_corr.txt\n",
      "2018-11-20 14:01:36,558 : INFO : Processing 108314_en_corr.txt\n",
      "2018-11-20 14:01:36,838 : INFO : Processing 108315_en_corr.txt\n",
      "2018-11-20 14:01:37,076 : INFO : Processing 108331_en_corr.txt\n",
      "2018-11-20 14:01:37,831 : INFO : Processing 108331_fr_corr.txt\n",
      "2018-11-20 14:01:38,648 : INFO : Processing 108337_en_corr.txt\n",
      "2018-11-20 14:01:39,017 : INFO : Processing 108337_fr_corr.txt\n",
      "2018-11-20 14:01:39,414 : INFO : Processing 108353_en_corr.txt\n",
      "2018-11-20 14:01:39,732 : INFO : Processing 108372_en_corr.txt\n",
      "2018-11-20 14:01:40,212 : INFO : Processing 108373_en_corr.txt\n",
      "2018-11-20 14:01:40,828 : INFO : Processing 108377_en_corr.txt\n",
      "2018-11-20 14:01:41,086 : INFO : Processing 108475_en_corr.txt\n",
      "2018-11-20 14:01:43,833 : INFO : Processing 108524_en_corr.txt\n",
      "2018-11-20 14:01:44,421 : INFO : Processing 108566_en_corr.txt\n",
      "2018-11-20 14:01:44,766 : INFO : Processing 108579_en_corr.txt\n",
      "2018-11-20 14:01:45,123 : INFO : Processing 108624_en.txt\n",
      "2018-11-20 14:01:45,367 : INFO : Processing 108630_en_corr.txt\n",
      "2018-11-20 14:01:45,593 : INFO : Processing 108633_en_corr.txt\n",
      "2018-11-20 14:01:45,922 : INFO : Processing 108695_en_corr.txt\n",
      "2018-11-20 14:01:46,248 : INFO : Processing 108716_en_corr.txt\n",
      "2018-11-20 14:01:46,558 : INFO : Processing 108743_en_corr.txt\n",
      "2018-11-20 14:01:46,895 : INFO : Processing 108754_en_corr.txt\n",
      "2018-11-20 14:01:47,658 : INFO : Processing 108754_fr_corr.txt\n",
      "2018-11-20 14:01:48,259 : INFO : Processing 108756_en_corr.txt\n",
      "2018-11-20 14:01:48,620 : INFO : Processing 108785_en_corr.txt\n",
      "2018-11-20 14:01:50,227 : INFO : Processing 108785_fr_corr.txt\n",
      "2018-11-20 14:01:52,210 : INFO : Processing 108921_en_corr.txt\n",
      "2018-11-20 14:01:52,790 : INFO : Processing 108974_en_corr.txt\n",
      "2018-11-20 14:01:53,245 : INFO : Processing 108995_en_corr.txt\n",
      "2018-11-20 14:01:53,570 : INFO : Processing 109033_en_corr.txt\n",
      "2018-11-20 14:01:53,939 : INFO : Processing 109033_fr_corr.txt\n",
      "2018-11-20 14:01:54,411 : INFO : Processing 109034_en_corr.txt\n",
      "2018-11-20 14:01:54,789 : INFO : Processing 109034_fr_corr.txt\n",
      "2018-11-20 14:01:55,248 : INFO : Processing 109039_en_corr.txt\n",
      "2018-11-20 14:01:55,640 : INFO : Processing 109069_en_corr.txt\n",
      "2018-11-20 14:01:55,901 : INFO : Processing 109073_en_corr.txt\n",
      "2018-11-20 14:01:56,198 : INFO : Processing 109080_en_corr.txt\n",
      "2018-11-20 14:01:56,895 : INFO : Processing 109080_fr_corr.txt\n",
      "2018-11-20 14:01:57,740 : INFO : Processing 109083_en_corr.txt\n",
      "2018-11-20 14:01:57,996 : INFO : Processing 109084_en_corr.txt\n",
      "2018-11-20 14:01:58,235 : INFO : Processing 109086_en_corr.txt\n",
      "2018-11-20 14:01:58,671 : INFO : Processing 109089_en_corr.txt\n",
      "2018-11-20 14:01:59,096 : INFO : Processing 109119_en_corr.txt\n",
      "2018-11-20 14:01:59,436 : INFO : Processing 109140_en_corr.txt\n",
      "2018-11-20 14:01:59,761 : INFO : Processing 109173_en_corr.txt\n",
      "2018-11-20 14:02:00,060 : INFO : Processing 109173_fr_corr.txt\n",
      "2018-11-20 14:02:00,411 : INFO : Processing 109174_en_corr.txt\n",
      "2018-11-20 14:02:00,965 : INFO : Processing 109183_en_corr.txt\n",
      "2018-11-20 14:02:01,195 : INFO : Processing 109189_en_corr.txt\n",
      "2018-11-20 14:02:01,587 : INFO : Processing 109192_en_corr.txt\n",
      "2018-11-20 14:02:01,896 : INFO : Processing 109197_en_corr.txt\n",
      "2018-11-20 14:02:02,208 : INFO : Processing 109275_en_corr.txt\n",
      "2018-11-20 14:02:02,626 : INFO : Processing 109283_en_corr.txt\n",
      "2018-11-20 14:02:02,857 : INFO : Processing 109283_fr_corr.txt\n",
      "2018-11-20 14:02:03,168 : INFO : Processing 109284_en-corr.txt\n",
      "2018-11-20 14:02:03,510 : INFO : Processing 109284_fr_corr.txt\n",
      "2018-11-20 14:02:03,812 : INFO : Processing 109291_en_corr.txt\n",
      "2018-11-20 14:02:04,285 : INFO : Processing 109291_fr_corr.txt\n",
      "2018-11-20 14:02:04,795 : INFO : Processing 109323_en_corr.txt\n",
      "2018-11-20 14:02:05,253 : INFO : Processing 109402_en_corr.txt\n",
      "2018-11-20 14:02:05,507 : INFO : Processing 109427_en_corr.txt\n",
      "2018-11-20 14:02:05,994 : INFO : Processing 109429_en_corr.txt\n",
      "2018-11-20 14:02:06,599 : INFO : Processing 109430_en_corr.txt\n",
      "2018-11-20 14:02:07,362 : INFO : Processing 109442_en_corr.txt\n",
      "2018-11-20 14:02:07,962 : INFO : Processing 109442_fr_corr.txt\n",
      "2018-11-20 14:02:08,639 : INFO : Processing 109444_en_corr.txt\n",
      "2018-11-20 14:02:09,072 : INFO : Processing 109444_fr_corr.txt\n",
      "2018-11-20 14:02:09,493 : INFO : Processing 109501_en_corr.txt\n",
      "2018-11-20 14:02:09,992 : INFO : Processing 109508_en_corr.txt\n",
      "2018-11-20 14:02:13,173 : INFO : Processing 109508_fr_corr.txt\n",
      "2018-11-20 14:02:16,429 : INFO : Processing 109524_en_corr.txt\n",
      "2018-11-20 14:02:16,773 : INFO : Processing 109530_en_corr.txt\n",
      "2018-11-20 14:02:17,470 : INFO : Processing 109530_fr_corr.txt\n",
      "2018-11-20 14:02:18,133 : INFO : Processing 109556_en_corr.txt\n",
      "2018-11-20 14:02:18,367 : INFO : Processing 109557_en_corr.txt\n",
      "2018-11-20 14:02:18,605 : INFO : Processing 109558_en_corr.txt\n",
      "2018-11-20 14:02:18,803 : INFO : Processing 109560_en_corr.txt\n",
      "2018-11-20 14:02:19,097 : INFO : Processing 109568_en.txt\n",
      "2018-11-20 14:02:19,395 : INFO : Processing 109573_en_corr.txt\n",
      "2018-11-20 14:02:19,789 : INFO : Processing 109847_en_corr.txt\n",
      "2018-11-20 14:02:20,092 : INFO : Processing 109849_en_corr.txt\n",
      "2018-11-20 14:02:20,636 : INFO : Processing 109873_en_corr.txt\n",
      "2018-11-20 14:02:20,859 : INFO : Processing 109880_en_corr.txt\n",
      "2018-11-20 14:02:22,147 : INFO : Processing 109880_fr_corr.txt\n",
      "2018-11-20 14:02:23,730 : INFO : Processing 109885_en_corr.txt\n",
      "2018-11-20 14:02:25,091 : INFO : Processing 109885_fr_corr.txt\n",
      "2018-11-20 14:02:26,549 : INFO : Processing 109954_en_corr.txt\n",
      "2018-11-20 14:02:26,877 : INFO : Processing 109955_en_corr.txt\n",
      "2018-11-20 14:02:27,141 : INFO : Processing 109955_fr_corr.txt\n",
      "2018-11-20 14:02:27,527 : INFO : Processing 110101_en_corr.txt\n",
      "2018-11-20 14:02:29,738 : INFO : Processing 110101_fr_corr.txt\n",
      "2018-11-20 14:02:32,166 : INFO : Processing 110173_en_corr.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 14:02:32,564 : INFO : Processing 110179_en_corr.txt\n",
      "2018-11-20 14:02:32,911 : INFO : Processing 110181_en_corr.txt\n",
      "2018-11-20 14:02:33,632 : INFO : Processing 110186_en_corr.txt\n",
      "2018-11-20 14:02:33,941 : INFO : Processing 110187_en_corr.txt\n",
      "2018-11-20 14:02:35,980 : INFO : Processing 110187_fr_corr.txt\n",
      "2018-11-20 14:02:38,313 : INFO : Processing 110209_en_corr.txt\n",
      "2018-11-20 14:02:38,649 : INFO : Processing 110211_en_corr.txt\n",
      "2018-11-20 14:02:39,244 : INFO : Processing 110228_en_corr.txt\n",
      "2018-11-20 14:02:39,680 : INFO : Processing 110228_fr_corr.txt\n",
      "2018-11-20 14:02:40,110 : INFO : Processing 110278_en_corr.txt\n",
      "2018-11-20 14:02:40,441 : INFO : Processing 110282_en_corr.txt\n",
      "2018-11-20 14:02:40,933 : INFO : Processing 110283_en_corr.txt\n",
      "2018-11-20 14:02:42,095 : INFO : Processing 110293_en_corr.txt\n",
      "2018-11-20 14:02:42,461 : INFO : Processing 110316_en_corr.txt\n",
      "2018-11-20 14:02:45,400 : INFO : Processing 110316_fr_corr.txt\n",
      "2018-11-20 14:02:48,551 : INFO : Processing 110368_en_corr.txt\n",
      "2018-11-20 14:02:49,217 : INFO : Processing 110368_fr_corr.txt\n",
      "2018-11-20 14:02:49,951 : INFO : Processing 110378_en_corr.txt\n",
      "2018-11-20 14:02:50,182 : INFO : Processing 110386_en_corr.txt\n",
      "2018-11-20 14:02:50,569 : INFO : Processing 110386_fr_corr.txt\n",
      "2018-11-20 14:02:51,094 : INFO : Processing 110401_en_corr.txt\n",
      "2018-11-20 14:02:51,435 : INFO : Processing 110402_en_corr.txt\n",
      "2018-11-20 14:02:53,051 : INFO : Processing 110413_en_corr.txt\n",
      "2018-11-20 14:02:53,589 : INFO : Processing 110419_en_corr.txt\n",
      "2018-11-20 14:02:53,943 : INFO : Processing 110461_en_corr.txt\n",
      "2018-11-20 14:02:54,764 : INFO : Processing 110461_fr_corr.txt\n",
      "2018-11-20 14:02:55,690 : INFO : Processing 110497_en_corr.txt\n",
      "2018-11-20 14:02:56,177 : INFO : Processing 110528_en_corr.txt\n",
      "2018-11-20 14:02:56,752 : INFO : Processing 110528_fr_corr.txt\n",
      "2018-11-20 14:02:57,458 : INFO : Processing 110533_en_corr.txt\n",
      "2018-11-20 14:02:57,837 : INFO : Processing 110533_fr_corr.txt\n",
      "2018-11-20 14:02:58,246 : INFO : Processing 110575_en_corr.txt\n",
      "2018-11-20 14:02:58,478 : INFO : Processing 110604_en_corr.txt\n",
      "2018-11-20 14:02:58,771 : INFO : Processing 110636_en_corr.txt\n",
      "2018-11-20 14:02:59,564 : INFO : Processing 110636_fr_corr.txt\n",
      "2018-11-20 14:03:00,441 : INFO : Processing 110644_en_corr.txt\n",
      "2018-11-20 14:03:00,935 : INFO : Processing 110644_fr_corr.txt\n",
      "2018-11-20 14:03:01,476 : INFO : Processing 110645_en_corr.txt\n",
      "2018-11-20 14:03:02,269 : INFO : Processing 110645_fr_corr.txt\n",
      "2018-11-20 14:03:03,213 : INFO : Processing 110707_en_corr.txt\n",
      "2018-11-20 14:03:03,971 : INFO : Processing 110707_fr_corr.txt\n",
      "2018-11-20 14:03:04,853 : INFO : Processing 110718_en_corr.txt\n",
      "2018-11-20 14:03:05,906 : INFO : Processing 110718_fr_corr.txt\n",
      "2018-11-20 14:03:07,061 : INFO : Processing 110726_en_corr.txt\n",
      "2018-11-20 14:03:07,707 : INFO : Processing 110726_fr_corr.txt\n",
      "2018-11-20 14:03:08,309 : INFO : Processing 110737_en_corr.txt\n",
      "2018-11-20 14:03:09,229 : INFO : Processing 110737_fr_corr.txt\n",
      "2018-11-20 14:03:10,282 : INFO : Processing 110739_en_corr.txt\n",
      "2018-11-20 14:03:11,586 : INFO : Processing 110739_fr_corr.txt\n",
      "2018-11-20 14:03:13,019 : INFO : Processing 110741_en_corr.txt\n",
      "2018-11-20 14:03:13,779 : INFO : Processing 110741_fr_corr.txt\n",
      "2018-11-20 14:03:14,687 : INFO : Processing 110742_en_corr.txt\n",
      "2018-11-20 14:03:15,095 : INFO : Processing 110742_fr_corr.txt\n",
      "2018-11-20 14:03:15,507 : INFO : Processing 110743_en_corr.txt\n",
      "2018-11-20 14:03:16,614 : INFO : Processing 110743_fr_corr.txt\n",
      "2018-11-20 14:03:17,879 : INFO : Processing 110885_en_corr.txt\n",
      "2018-11-20 14:03:18,271 : INFO : Processing 110885_fr_corr.txt\n",
      "2018-11-20 14:03:18,700 : INFO : Processing 110898_en_corr.txt\n",
      "2018-11-20 14:03:19,493 : INFO : Processing 110898_fr_corr.txt\n",
      "2018-11-20 14:03:20,374 : INFO : Processing 110906_en_corr.txt\n",
      "2018-11-20 14:03:20,559 : INFO : Processing 110924_en_corr.txt\n",
      "2018-11-20 14:03:20,819 : INFO : Processing 110968_en_corr.txt\n",
      "2018-11-20 14:03:21,075 : INFO : Processing 110969_en_corr.txt\n",
      "2018-11-20 14:03:21,311 : INFO : Processing 110971_en_corr.txt\n",
      "2018-11-20 14:03:21,572 : INFO : Processing 110972_en_corr.txt\n",
      "2018-11-20 14:03:21,871 : INFO : Processing 110973_en_corr.txt\n",
      "2018-11-20 14:03:22,136 : INFO : Processing 110975_en_corr.txt\n",
      "2018-11-20 14:03:23,117 : INFO : Processing 111002_en-corr.txt\n",
      "2018-11-20 14:03:23,349 : INFO : Processing 111002_fr_corr.txt\n",
      "2018-11-20 14:03:23,582 : INFO : Processing 111005_en_corr.txt\n",
      "2018-11-20 14:03:23,780 : INFO : Processing 111005_fr_corr.txt\n",
      "2018-11-20 14:03:23,999 : INFO : Processing 111025_en_corr.txt\n",
      "2018-11-20 14:03:24,637 : INFO : Processing 111025_fr_corr.txt\n",
      "2018-11-20 14:03:25,315 : INFO : Processing 111047_en_corr.txt\n",
      "2018-11-20 14:03:25,605 : INFO : Processing 111059_en_corr.txt\n",
      "2018-11-20 14:03:26,964 : INFO : Processing 111093_en_corr.txt\n",
      "2018-11-20 14:03:31,102 : INFO : Processing 111093_fr_corr.txt\n",
      "2018-11-20 14:03:35,215 : INFO : Processing 111143_en_corr.txt\n",
      "2018-11-20 14:03:36,103 : INFO : Processing 111143_fr_corr.txt\n",
      "2018-11-20 14:03:37,100 : INFO : Processing 111170_en_corr.txt\n",
      "2018-11-20 14:03:37,546 : INFO : Processing 111170_fr_corr.txt\n",
      "2018-11-20 14:03:37,974 : INFO : Processing 111176_en_corr.txt\n",
      "2018-11-20 14:03:38,312 : INFO : Processing 111195_en_corr.txt\n",
      "2018-11-20 14:03:38,673 : INFO : Processing 111227_en_corr.txt\n",
      "2018-11-20 14:03:39,023 : INFO : Processing 111244_en_corr.txt\n",
      "2018-11-20 14:03:39,423 : INFO : Processing 111284_en_corr.txt\n",
      "2018-11-20 14:03:39,885 : INFO : Processing 111284_fr_corr.txt\n",
      "2018-11-20 14:03:40,402 : INFO : Processing 111289_en_corr.txt\n",
      "2018-11-20 14:03:40,905 : INFO : Processing 111289_fr_corr.txt\n",
      "2018-11-20 14:03:41,516 : INFO : Processing 111298_en_corr.txt\n",
      "2018-11-20 14:03:42,979 : INFO : Processing 111298_fr_corr.txt\n",
      "2018-11-20 14:03:44,614 : INFO : Processing 111367_en_corr.txt\n",
      "2018-11-20 14:03:45,244 : INFO : Processing 111370_en_corr.txt\n",
      "2018-11-20 14:03:45,615 : INFO : Processing 111370_fr_corr.txt\n",
      "2018-11-20 14:03:46,024 : INFO : Processing 111380_en-corr.txt\n",
      "2018-11-20 14:03:46,709 : INFO : Processing 111380_fr_corr.txt\n",
      "2018-11-20 14:03:47,541 : INFO : Processing 111468_en_corr.txt\n",
      "2018-11-20 14:03:47,908 : INFO : Processing 111470_en_corr.txt\n",
      "2018-11-20 14:03:48,347 : INFO : Processing 111544_en_corr.txt\n",
      "2018-11-20 14:03:49,119 : INFO : Processing 111544_fr_corr.txt\n",
      "2018-11-20 14:03:49,968 : INFO : Processing 111557_en_corr.txt\n",
      "2018-11-20 14:03:50,355 : INFO : Processing 111557_fr_corr.txt\n",
      "2018-11-20 14:03:50,799 : INFO : Processing 111561_en_corr.txt\n",
      "2018-11-20 14:03:51,933 : INFO : Processing 111561_fr_corr.txt\n",
      "2018-11-20 14:03:53,288 : INFO : Processing 111563_en_corr.txt\n",
      "2018-11-20 14:03:54,003 : INFO : Processing 111563_fr_corr.txt\n",
      "2018-11-20 14:03:54,781 : INFO : Processing 111592_en_corr.txt\n",
      "2018-11-20 14:03:55,051 : INFO : Processing 111599_en_corr.txt\n",
      "2018-11-20 14:03:55,406 : INFO : Processing 111599_fr_corr.txt\n",
      "2018-11-20 14:03:55,809 : INFO : Processing 111606_en_corr.txt\n",
      "2018-11-20 14:03:56,286 : INFO : Processing 111606_fr_corr.txt\n",
      "2018-11-20 14:03:56,819 : INFO : Processing 111701_en_corr.txt\n",
      "2018-11-20 14:03:57,086 : INFO : Processing 111739_en_corr.txt\n",
      "2018-11-20 14:03:57,496 : INFO : Processing 111739_fr_corr.txt\n",
      "2018-11-20 14:03:57,888 : INFO : Processing 111752_en_corr.txt\n",
      "2018-11-20 14:03:58,229 : INFO : Processing 111752_fr_corr.txt\n",
      "2018-11-20 14:03:58,632 : INFO : Processing 111823_en_corr.txt\n",
      "2018-11-20 14:03:59,697 : INFO : Processing 111841_en_corr.txt\n",
      "2018-11-20 14:03:59,918 : INFO : Processing 111870_en_corr.txt\n",
      "2018-11-20 14:04:00,927 : INFO : Processing 111870_fr_corr.txt\n",
      "2018-11-20 14:04:02,068 : INFO : Processing 111887_en_corr.txt\n",
      "2018-11-20 14:04:02,842 : INFO : Processing 111896_en_corr.txt\n",
      "2018-11-20 14:04:03,258 : INFO : Processing 111898_en_corr.txt\n",
      "2018-11-20 14:04:03,537 : INFO : Processing 111899_en_corr.txt\n",
      "2018-11-20 14:04:03,988 : INFO : Processing 111907_en_corr.txt\n",
      "2018-11-20 14:04:04,236 : INFO : Processing 111907_fr_corr.txt\n",
      "2018-11-20 14:04:04,521 : INFO : Processing 111908_en_corr.txt\n",
      "2018-11-20 14:04:04,645 : INFO : Processing 111908_fr_corr.txt\n",
      "2018-11-20 14:04:04,778 : INFO : Processing 111918_en_corr.txt\n",
      "2018-11-20 14:04:05,312 : INFO : Processing 111918_fr_corr.txt\n",
      "2018-11-20 14:04:05,976 : INFO : Processing 111951_en_corr.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 14:04:06,550 : INFO : Processing 111953_en_corr.txt\n",
      "2018-11-20 14:04:07,052 : INFO : Processing 111959_en_corr.txt\n",
      "2018-11-20 14:04:08,037 : INFO : Processing 112033_en_corr.txt\n",
      "2018-11-20 14:04:08,342 : INFO : Processing 112135_en_corr.txt\n",
      "2018-11-20 14:04:08,768 : INFO : Processing 112261_en_corr.txt\n",
      "2018-11-20 14:04:09,325 : INFO : Processing 112263_en_corr.txt\n",
      "2018-11-20 14:04:09,995 : INFO : Processing 112292_en_corr.txt\n",
      "2018-11-20 14:04:12,009 : INFO : Processing 112292_fr_corr.txt\n",
      "2018-11-20 14:04:15,092 : INFO : Processing 112317_en_corr.txt\n",
      "2018-11-20 14:04:15,368 : INFO : Processing 112318_en_corr.txt\n",
      "2018-11-20 14:04:15,599 : INFO : Processing 112318_fr_corr.txt\n",
      "2018-11-20 14:04:15,865 : INFO : Processing 112361_en_corr.txt\n",
      "2018-11-20 14:04:16,229 : INFO : Processing 112361_fr_corr.txt\n",
      "2018-11-20 14:04:16,624 : INFO : Processing 112386_en-corr.txt\n",
      "2018-11-20 14:04:16,997 : INFO : Processing 112386_fr_corr.txt\n",
      "2018-11-20 14:04:17,490 : INFO : Processing 112414_en_corr.txt\n",
      "2018-11-20 14:04:17,784 : INFO : Processing 112416_en_corr.txt\n",
      "2018-11-20 14:04:17,957 : INFO : Processing 112443_en_corr.txt\n",
      "2018-11-20 14:04:19,132 : INFO : Processing 112505_en_corr.txt\n",
      "2018-11-20 14:04:19,440 : INFO : Processing 112648_en_corr.txt\n",
      "2018-11-20 14:04:19,918 : INFO : Processing 112726_en_corr.txt\n",
      "2018-11-20 14:04:20,265 : INFO : Processing 112726_fr_corr.txt\n",
      "2018-11-20 14:04:20,663 : INFO : Processing 112776_en_corr.txt\n",
      "2018-11-20 14:04:20,923 : INFO : Processing 112786_en_corr.txt\n",
      "2018-11-20 14:04:21,352 : INFO : Processing 112787_en_corr.txt\n",
      "2018-11-20 14:04:21,819 : INFO : Processing 112788_en_corr.txt\n",
      "2018-11-20 14:04:22,268 : INFO : Processing 112789_en_corr.txt\n",
      "2018-11-20 14:04:22,908 : INFO : Processing 112840_en_corr.txt\n",
      "2018-11-20 14:04:23,354 : INFO : Processing 112841_en_corr.txt\n",
      "2018-11-20 14:04:23,911 : INFO : Processing 112955_en_corr.txt\n",
      "2018-11-20 14:04:24,136 : INFO : Processing 113158_en_corr.txt\n",
      "2018-11-20 14:04:24,631 : INFO : Processing 113166_en_corr.txt\n",
      "2018-11-20 14:04:25,151 : INFO : Processing 113233_en_corr.txt\n",
      "2018-11-20 14:04:26,005 : INFO : Processing 113233_fr_corr.txt\n",
      "2018-11-20 14:04:26,963 : INFO : Processing 113418_en_corr.txt\n",
      "2018-11-20 14:04:28,008 : INFO : Processing 113418_fr_corr.txt\n",
      "2018-11-20 14:04:29,065 : INFO : Processing 113562_en_corr.txt\n",
      "2018-11-20 14:04:29,330 : INFO : Processing 113702_en_corr.txt\n",
      "2018-11-20 14:04:29,632 : INFO : Processing 113704_en_corr.txt\n",
      "2018-11-20 14:04:29,920 : INFO : Processing 113705_en_corr.txt\n",
      "2018-11-20 14:04:30,266 : INFO : Processing 113715_en_corr.txt\n",
      "2018-11-20 14:04:30,565 : INFO : Processing 113718_en_corr.txt\n",
      "2018-11-20 14:04:30,847 : INFO : Processing 114138_en_corr.txt\n",
      "2018-11-20 14:04:31,343 : INFO : Processing 114140_en_corr.txt\n",
      "2018-11-20 14:04:31,618 : INFO : Processing 115407_en_corr.txt\n",
      "2018-11-20 14:04:32,098 : INFO : Processing 115884_en_corr.txt\n",
      "2018-11-20 14:04:32,847 : INFO : Processing 115884_fr_corr.txt\n",
      "2018-11-20 14:04:33,584 : INFO : Processing 116844_en_corr.txt\n",
      "2018-11-20 14:04:33,837 : INFO : Processing 116938_en_corr.txt\n",
      "2018-11-20 14:04:34,068 : INFO : Processing 117574_en_corr.txt\n",
      "2018-11-20 14:04:34,454 : INFO : Processing 117574_fr_corr.txt\n",
      "2018-11-20 14:04:34,893 : INFO : Processing 117575_de_corr.txt\n",
      "2018-11-20 14:04:35,647 : INFO : Processing 117575_en_corr.txt\n",
      "2018-11-20 14:04:36,755 : INFO : Processing 117575_fr_corr.txt\n",
      "2018-11-20 14:04:38,098 : INFO : Processing 117928_en-corr.txt\n",
      "2018-11-20 14:04:38,275 : INFO : Processing 117928_fr_corr.txt\n",
      "2018-11-20 14:04:38,478 : INFO : Processing 118354_en_corr.txt\n",
      "2018-11-20 14:04:39,237 : INFO : Processing 118811_en_corr.txt\n",
      "2018-11-20 14:04:39,487 : INFO : Processing 121203_en_corr.txt\n",
      "2018-11-20 14:04:40,223 : INFO : Processing 121975_en_corr.txt\n",
      "2018-11-20 14:04:40,637 : INFO : Processing 122060_en_corr.txt\n",
      "2018-11-20 14:04:40,921 : INFO : Processing 122322_en.txt\n",
      "2018-11-20 14:04:41,122 : INFO : Processing 122353_en_corr.txt\n",
      "2018-11-20 14:04:41,377 : INFO : Processing 122377_en_corr.txt\n",
      "2018-11-20 14:04:41,884 : INFO : Processing 122467_en_corr.txt\n",
      "2018-11-20 14:04:42,133 : INFO : Processing 122839_en_corr.txt\n",
      "2018-11-20 14:04:43,313 : INFO : Processing 122839_fr_corr.txt\n",
      "2018-11-20 14:04:44,545 : INFO : Processing 122879_en_corr.txt\n",
      "2018-11-20 14:04:44,948 : INFO : Processing 122883_en_corr.txt\n",
      "2018-11-20 14:04:45,334 : INFO : Processing 122911_en_corr.txt\n",
      "2018-11-20 14:04:45,871 : INFO : Processing 122913_en_corr.txt\n",
      "2018-11-20 14:04:46,241 : INFO : Processing 122913_fr_corr.txt\n",
      "2018-11-20 14:04:46,646 : INFO : Processing 122916_en_corr.txt\n",
      "2018-11-20 14:04:46,922 : INFO : Processing 122917_en_corr.txt\n",
      "2018-11-20 14:04:47,419 : INFO : Processing 122918_en_corr.txt\n",
      "2018-11-20 14:04:47,837 : INFO : Processing 122919_en_corr.txt\n",
      "2018-11-20 14:04:48,258 : INFO : Processing 122989_en_corr.txt\n",
      "2018-11-20 14:04:48,585 : INFO : Processing 123016_en_corr.txt\n",
      "2018-11-20 14:04:48,860 : INFO : Processing 123018_en_corr.txt\n",
      "2018-11-20 14:04:49,114 : INFO : Processing 123061_en_corr.txt\n",
      "2018-11-20 14:04:49,531 : INFO : Processing 123114_en_corr.txt\n",
      "2018-11-20 14:04:49,989 : INFO : Processing 123115_en_corr.txt\n",
      "2018-11-20 14:04:50,261 : INFO : Processing 123151_en_corr.txt\n",
      "2018-11-20 14:04:50,565 : INFO : Processing 123229_en_corr.txt\n",
      "2018-11-20 14:04:51,433 : INFO : Processing 123232_en_corr.txt\n",
      "2018-11-20 14:04:51,892 : INFO : Processing 123472_en_corr.txt\n",
      "2018-11-20 14:04:52,246 : INFO : Processing 124261_en.txt\n",
      "2018-11-20 14:04:52,540 : INFO : Processing 124345_en-corr.txt\n",
      "2018-11-20 14:04:53,242 : INFO : Processing 124345_fr_corr.txt\n",
      "2018-11-20 14:04:54,031 : INFO : Processing 126257_en_corr.txt\n",
      "2018-11-20 14:04:54,482 : INFO : Processing 126257_fr_corr.txt\n",
      "2018-11-20 14:04:55,037 : INFO : Processing 126263_en_corr.txt\n",
      "2018-11-20 14:04:55,358 : INFO : Processing 126263_fr_corr.txt\n",
      "2018-11-20 14:04:55,636 : INFO : Processing 126274_en_corr.txt\n",
      "2018-11-20 14:04:55,938 : INFO : Processing 126274_fr_corr.txt\n",
      "2018-11-20 14:04:56,303 : INFO : Processing 126503_en_corr.txt\n",
      "2018-11-20 14:04:56,687 : INFO : Processing 126503_fr_corr.txt\n",
      "2018-11-20 14:04:57,066 : INFO : Processing 127532_en_corr.txt\n",
      "2018-11-20 14:04:58,113 : INFO : Processing 127584_en.txt\n",
      "2018-11-20 14:04:58,714 : INFO : Processing 128127_en_corr.txt\n",
      "2018-11-20 14:04:59,233 : INFO : Processing 128127_fr_corr.txt\n",
      "2018-11-20 14:04:59,808 : INFO : Processing 129079_en_corr.txt\n",
      "2018-11-20 14:05:00,637 : INFO : Processing 129079_fr_corr.txt\n",
      "2018-11-20 14:05:01,545 : INFO : Processing 129982_en_corr.txt\n",
      "2018-11-20 14:05:01,903 : INFO : Processing 129982_fr_corr.txt\n",
      "2018-11-20 14:05:02,300 : INFO : Processing 132090_en.txt\n",
      "2018-11-20 14:05:02,970 : INFO : Processing 132119_en_corr.txt\n",
      "2018-11-20 14:05:03,975 : INFO : Processing 132119_fr_corr.txt\n",
      "2018-11-20 14:05:04,819 : INFO : Processing 135702_en.txt\n",
      "2018-11-20 14:05:05,337 : INFO : Processing 135899_en_corr.txt\n",
      "2018-11-20 14:05:05,908 : INFO : Processing 135899_fr_corr.txt\n",
      "2018-11-20 14:05:06,542 : INFO : Processing 200785_en.txt\n",
      "2018-11-20 14:05:06,646 : INFO : Processing 200785_fr_corr.txt\n",
      "2018-11-20 14:05:06,760 : INFO : Processing 303758_en.txt\n",
      "2018-11-20 14:05:07,557 : INFO : Processing 303758_it_corr.txt\n",
      "2018-11-20 14:05:08,348 : INFO : Processing 304127_de_corr.txt\n",
      "2018-11-20 14:05:09,427 : INFO : Processing 304127_en.txt\n",
      "2018-11-20 14:05:10,461 : INFO : Processing 304572_en.txt\n",
      "2018-11-20 14:05:11,009 : INFO : Processing 304572_fr_corr.txt\n",
      "2018-11-20 14:05:11,601 : INFO : Processing 304676_en_corr.txt\n",
      "2018-11-20 14:05:12,020 : INFO : Processing 304676_fr_corr.txt\n",
      "2018-11-20 14:05:12,576 : INFO : Processing 405295_en_corr.txt\n",
      "2018-11-20 14:05:13,069 : INFO : Processing 415051_fr_corr.txt\n",
      "2018-11-20 14:05:13,443 : INFO : Processing 415207_fr_corr.txt\n",
      "2018-11-20 14:05:13,738 : INFO : Processing 415256_fr_corr.txt\n",
      "2018-11-20 14:05:14,341 : INFO : Processing 415293_fr_corr.txt\n",
      "2018-11-20 14:05:14,893 : INFO : Processing 415355_fr_corr.txt\n",
      "2018-11-20 14:05:15,604 : INFO : Processing 415399_fr_corr.txt\n",
      "2018-11-20 14:05:16,156 : INFO : Processing 415488_fr_corr.txt\n",
      "2018-11-20 14:05:16,742 : INFO : Processing 415515_fr_corr.txt\n",
      "2018-11-20 14:05:17,401 : INFO : Processing 416003_fr_corr.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 14:05:21,464 : INFO : Processing 416075_fr_corr.txt\n",
      "2018-11-20 14:05:22,230 : INFO : Processing 416089_fr_corr.txt\n",
      "2018-11-20 14:05:22,837 : INFO : Processing 416108_fr_corr.txt\n",
      "2018-11-20 14:05:23,378 : INFO : Processing 416132_fr_corr.txt\n",
      "2018-11-20 14:05:24,488 : INFO : Processing 416160_fr_corr.txt\n",
      "2018-11-20 14:05:24,992 : INFO : Processing 416254_fr_corr.txt\n",
      "2018-11-20 14:05:25,390 : INFO : Processing 416268_fr_corr.txt\n",
      "2018-11-20 14:05:25,810 : INFO : Processing 416337_fr_corr.txt\n",
      "2018-11-20 14:05:26,028 : INFO : Processing 416338_fr_corr.txt\n",
      "2018-11-20 14:05:26,559 : INFO : Processing 418117_fr_corr.txt\n",
      "2018-11-20 14:05:27,015 : INFO : Processing 418126_fr_corr.txt\n",
      "2018-11-20 14:05:28,343 : INFO : Processing 418360_fr_corr.txt\n",
      "2018-11-20 14:05:28,804 : INFO : Processing 418426_fr_corr.txt\n",
      "2018-11-20 14:05:29,054 : INFO : Processing 418470_fr_corr.txt\n",
      "2018-11-20 14:05:29,442 : INFO : Processing 418504_fr_corr.txt\n",
      "2018-11-20 14:05:32,158 : INFO : Processing 418590_fr_corr.txt\n",
      "2018-11-20 14:05:32,589 : INFO : Processing 418678_fr_corr.txt\n",
      "2018-11-20 14:05:32,789 : INFO : Processing 418722_fr_corr.txt\n",
      "2018-11-20 14:05:33,085 : INFO : Processing 418812_fr_corr.txt\n",
      "2018-11-20 14:05:34,416 : INFO : Processing 419008_de_corr.txt\n",
      "2018-11-20 14:05:34,737 : INFO : Processing 419033_de_corr.txt\n",
      "2018-11-20 14:05:35,110 : INFO : Processing 419058_de_corr.txt\n",
      "2018-11-20 14:05:35,547 : INFO : Processing 419110_de_corr.txt\n",
      "2018-11-20 14:05:36,027 : INFO : Processing 419139_de_corr.txt\n",
      "2018-11-20 14:05:36,492 : INFO : Processing 419157_de_corr.txt\n",
      "2018-11-20 14:05:37,022 : INFO : Processing 419168_de_corr.txt\n",
      "2018-11-20 14:05:37,196 : INFO : Processing 419175_de_corr.txt\n",
      "2018-11-20 14:05:37,498 : INFO : Processing 419179_de_corr.txt\n",
      "2018-11-20 14:05:37,893 : INFO : Processing 419200_de_corr.txt\n",
      "2018-11-20 14:05:38,391 : INFO : Processing 419214_de_corr.txt\n",
      "2018-11-20 14:05:38,716 : INFO : Processing 419230_de_corr.txt\n",
      "2018-11-20 14:05:38,970 : INFO : Processing 419260_de_corr.txt\n",
      "2018-11-20 14:05:39,370 : INFO : Processing 419281_de_corr.txt\n",
      "2018-11-20 14:05:39,689 : INFO : Processing 419328_de_corr.txt\n",
      "2018-11-20 14:05:39,979 : INFO : Processing 419335_de_corr.txt\n",
      "2018-11-20 14:05:40,696 : INFO : Processing 419350_de_corr.txt\n",
      "2018-11-20 14:05:41,409 : INFO : Processing 419425_de_corr.txt\n",
      "2018-11-20 14:05:42,204 : INFO : Processing 419435_de_corr.txt\n",
      "2018-11-20 14:05:42,783 : INFO : Processing 419488_de_corr.txt\n",
      "2018-11-20 14:05:43,140 : INFO : Processing 420028_de_corr.txt\n",
      "2018-11-20 14:05:43,653 : INFO : Processing 420068_de_corr.txt\n",
      "2018-11-20 14:05:44,408 : INFO : Processing 420323_de_corr.txt\n",
      "2018-11-20 14:05:44,701 : INFO : Processing 420470_de_corr.txt\n",
      "2018-11-20 14:05:45,525 : INFO : Processing 420623_de_corr.txt\n",
      "2018-11-20 14:05:45,884 : INFO : Processing 420627_de_corr.txt\n",
      "2018-11-20 14:05:46,034 : INFO : Processing 425191_fr_corr.txt\n",
      "2018-11-20 14:05:46,676 : INFO : Processing 425260_en_corr.txt\n",
      "2018-11-20 14:05:47,151 : INFO : Processing 431022_en.txt\n",
      "2018-11-20 14:05:47,544 : INFO : Processing 431029_en.txt\n",
      "2018-11-20 14:05:47,896 : INFO : Processing 431040_en.txt\n",
      "2018-11-20 14:05:48,223 : INFO : Processing 431053_en.txt\n",
      "2018-11-20 14:05:48,439 : INFO : Processing 431075_en.txt\n",
      "2018-11-20 14:05:48,643 : INFO : Processing 431086_en.txt\n",
      "2018-11-20 14:05:48,834 : INFO : Processing 431120_en_corr.txt\n",
      "2018-11-20 14:05:49,145 : INFO : Processing 431220_en_corr.txt\n",
      "2018-11-20 14:05:49,403 : INFO : Processing 431318_en_corr.txt\n",
      "2018-11-20 14:05:49,683 : INFO : Processing 431334_en_corr.txt\n",
      "2018-11-20 14:05:49,874 : INFO : Processing 433056_en.txt\n",
      "2018-11-20 14:05:50,243 : INFO : Processing 435117_fr_corr.txt\n",
      "2018-11-20 14:05:51,089 : INFO : Processing 435142_fr_corr.txt\n",
      "2018-11-20 14:05:51,458 : INFO : Processing 451059_fr_corr.txt\n",
      "2018-11-20 14:05:52,050 : INFO : Processing 475071_fr_corr.txt\n",
      "2018-11-20 14:05:52,560 : INFO : Processing 475194_fr_corr.txt\n",
      "2018-11-20 14:05:52,923 : INFO : Processing 475237_fr_corr.txt\n",
      "2018-11-20 14:05:53,152 : INFO : Processing 475491_fr_corr.txt\n",
      "2018-11-20 14:06:15,525 : INFO : Working: Merging named entities...\n",
      "2018-11-20 14:06:22,064 : INFO : Done!\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "from textacy.spacier.utils import merge_spans\n",
    "\n",
    "def preprocess_text(source_filename, target_filename=None):\n",
    "    filenames = get_filenames(source_filename)\n",
    "    basename, extension = os.path.splitext(source_filename)\n",
    "    target_filename = target_filename or basename + '_preprocessed' + extension\n",
    "    texts = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    with zipfile.ZipFile(target_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for filename, text in texts:\n",
    "            logger.info('Processing ' + filename)\n",
    "            text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "            text = textacy.preprocess.normalize_whitespace(text)   \n",
    "            text = textacy.preprocess.fix_bad_unicode(text)   \n",
    "            text = textacy.preprocess.replace_currency_symbols(text)\n",
    "            text = textacy.preprocess.unpack_contractions(text)\n",
    "            text = textacy.preprocess.replace_urls(text)\n",
    "            text = textacy.preprocess.replace_emails(text)\n",
    "            text = textacy.preprocess.replace_phone_numbers(text)\n",
    "            text = textacy.preprocess.remove_accents(text)\n",
    "            zf.writestr(filename, text)\n",
    "            \n",
    "def create_textacy_corpus(source_filename, language, preprocess_args):\n",
    "    make_title = lambda filename: filename.replace('_', ' ').replace('.txt', '').title()\n",
    "    filenames = get_filenames(source_filename)\n",
    "    corpus = textacy.Corpus(language)\n",
    "    text_stream = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    for filename, text in text_stream:\n",
    "        logger.info('Processing ' + filename)\n",
    "        text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "        text = textacy.preprocess.preprocess_text(text, **preprocess_args)\n",
    "        corpus.add_text(text, dict(filename=filename, title=make_title(filename)))\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    return corpus\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [ e for e in doc.ents if not e.text.isspace() ]\n",
    "    return doc\n",
    "\n",
    "def generate_textacy_corpus(source_filename, language, corpus_args, preprocess_args, merge_named_entities=True, force=False):\n",
    "    \n",
    "    corpus_tag = '_'.join([ k for k in preprocess_args if preprocess_args[k] ]) + \\\n",
    "        '_disable(' + ','.join(corpus_args.get('disable', [])) +')'\n",
    "    \n",
    "    textacy_corpus_filename = os.path.join(SOURCE_FOLDER, 'corpus_{}_{}.pkl'.format(language, corpus_tag))\n",
    "    \n",
    "    Language.factories['remove_whitespace_entities'] = lambda nlp, **cfg: remove_whitespace_entities\n",
    "    \n",
    "    logger.info('Loading model: english...')\n",
    "    nlp = textacy.load_spacy('en_core_web_sm', **corpus_args)\n",
    "    pipeline = lambda: [ x[0] for x in nlp.pipeline ]\n",
    "        \n",
    "    logger.info('Using pipeline: ' + ' '.join(pipeline()))\n",
    "\n",
    "    if force or not os.path.isfile(textacy_corpus_filename):\n",
    "        logger.info('Working: Computing new corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = create_textacy_corpus(source_filename, nlp, preprocess_args)\n",
    "        corpus.save(textacy_corpus_filename)\n",
    "    else:\n",
    "        logger.info('Working: Loading corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = textacy.Corpus.load(textacy_corpus_filename)\n",
    "        \n",
    "    if merge_named_entities:\n",
    "        logger.info('Working: Merging named entities...')\n",
    "        for doc in corpus:\n",
    "            named_entities = textacy.extract.named_entities(doc)\n",
    "            merge_spans(named_entities, doc.spacy_doc)\n",
    "    else:\n",
    "        logger.info('Note: named entities not merged')\n",
    "        \n",
    "    logger.info('Done!')\n",
    "    return textacy_corpus_filename, corpus\n",
    "\n",
    "def assign_document_titles(corpus):\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    \n",
    "def get_corpus_documents(corpus):\n",
    "    df = pd.DataFrame([\n",
    "        (document_id, doc.metadata['title'], doc.metadata['filename'])\n",
    "                for document_id, doc in enumerate(corpus) ], columns=['document_id', 'title', 'filename']\n",
    "    ).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "#if not os.path.isfile(PREPROCESSED_TEXT_FILENAME):\n",
    "#    logger.info(\"Preprocessing text archive...\")\n",
    "#    preprocess_text(EDITED_TEXT_FILENAME, PREPROCESSED_TEXT_FILENAME)\n",
    "    \n",
    "TEXTACY_CORPUS_FILENAME, CORPUS = generate_textacy_corpus(PREPROCESSED_TEXT_FILENAME, LANGUAGE, corpus_args=dict(), preprocess_args=dict(), merge_named_entities=True, force=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7420043799424916975838b3e4273c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(Dropdownâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':'tight'}\n",
    "def display_cleanup_text_gui(corpus, callback):\n",
    "    \n",
    "    documents = get_corpus_documents(corpus)\n",
    "    document_options = {v: k for k, v in documents['title'].to_dict().items()}\n",
    "    \n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.FloatSlider(value=0, min=0, max=1.0, step=0.01, description='Min frequency', layout=widgets.Layout(width='400px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=[1,2,3], value=1, layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ False, 'lemma', 'lower' ], value=False, layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=False, description='Filter nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=False, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    uix = widgets.interactive(\n",
    "\n",
    "        callback,\n",
    "\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        document_id=gui.document_id,\n",
    "        \n",
    "        ngrams=gui.ngrams,\n",
    "        named_entities=gui.named_entities,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        filter_nums=gui.filter_nums,\n",
    "        include_pos=gui.include_pos,\n",
    "        min_freq=gui.min_freq,\n",
    "        drop_determiners=gui.drop_determiners\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.document_id,\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "                                  \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(8, 9 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='75')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def display_cleaned_up_text(corpus, gui, display_type, document_id, **kwargs): # ngrams, named_entities, normalize, include_pos):\n",
    "    \n",
    "    gui.output_text.clear_output()\n",
    "    gui.output_statistics.clear_output()\n",
    "    \n",
    "    #Additional candidates;\n",
    "    #is_alpha\tbool\tDoes the token consist of alphabetic characters? Equivalent to token.text.isalpha().\n",
    "    #is_ascii\tbool\tDoes the token consist of ASCII characters? Equivalent to [any(ord(c) >= 128 for c in token.text)].\n",
    "    #like_url\tbool\tDoes the token resemble a URL?\n",
    "    #like_email\tbool\tDoes the token resemble an email address?\n",
    "\n",
    "    doc = corpus[document_id]\n",
    "    \n",
    "    terms = [ x for x in doc.to_terms_list(as_strings=True, **kwargs) ]\n",
    "    \n",
    "    if display_type.startswith('source_text'):\n",
    "        \n",
    "        source_filename = SOURCE_FILES[display_type]['filename']\n",
    "        description =  SOURCE_FILES[display_type]['description']\n",
    "        text = get_text(source_filename, doc.metadata['filename'])\n",
    "        with gui.output_text:\n",
    "            #print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            #print(doc)\n",
    "            print('[ ' + description.upper() + ' ]')\n",
    "            print(text)\n",
    "        return\n",
    "\n",
    "    if len(terms) == 0:\n",
    "        with gui.output_text:\n",
    "            print(\"No text. Please change selection.\")\n",
    "        return\n",
    "    \n",
    "    if display_type in ['sanitized_text', 'statistics']:\n",
    "\n",
    "        if display_type == 'sanitized_text':\n",
    "            with gui.output_text:\n",
    "                #display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                #    ' '.join(tokens[:word_count]),\n",
    "                #    ' '.join(tokens[-word_count:])\n",
    "                #))\n",
    "                print(' '.join(list(terms)))\n",
    "                return\n",
    "\n",
    "        if display_type == 'statistics':\n",
    "\n",
    "            wf = nltk.FreqDist(terms)\n",
    "\n",
    "            with gui.output_text:\n",
    "\n",
    "                print('Word count (number of terms): {}'.format(wf.N()))\n",
    "                print('Unique word count (vocabulary): {}'.format(wf.B()))\n",
    "                print(' ')\n",
    "\n",
    "                df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                display(df)\n",
    "\n",
    "            with gui.output_statistics:\n",
    "\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "xgui, xuix = display_cleanup_text_gui(CORPUS, display_cleaned_up_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Compute an LDA Topic Model<span style='color: red; float: right'>MANDATORY RUN</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "class LdaDataCompiler():\n",
    "    \n",
    "    @staticmethod\n",
    "    def compile_dictionary(model):\n",
    "        logger.info('Compiling dictionary...')\n",
    "        token_ids, tokens = list(zip(*model.id2word.items()))\n",
    "        dfs = model.id2word.dfs.values() if model.id2word.dfs is not None else [0] * len(tokens)\n",
    "        dictionary = pd.DataFrame({\n",
    "            'token_id': token_ids,\n",
    "            'token': tokens,\n",
    "            'dfs': list(dfs)\n",
    "        }).set_index('token_id')[['token', 'dfs']]\n",
    "        return dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_weights(tm, dictionary, num_words=200):\n",
    "        logger.info('Compiling topic-tokens weights...')\n",
    "\n",
    "        df_topic_weights = pd.DataFrame(\n",
    "            [ (topic_id, token, weight)\n",
    "                for topic_id, tokens in (tm.show_topics(tm.num_topics, num_words=num_words, formatted=False))\n",
    "                    for token, weight in tokens if weight > 0.0 ],\n",
    "            columns=['topic_id', 'token', 'weight']\n",
    "        )\n",
    "\n",
    "        df = pd.merge(\n",
    "            df_topic_weights.set_index('token'),\n",
    "            dictionary.reset_index().set_index('token'),\n",
    "            how='inner',\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "        return df.reset_index()[['topic_id', 'token_id', 'token', 'weight']]\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_overview(topic_token_weights, alpha, n_words=200):\n",
    "        \"\"\"\n",
    "        Group by topic_id and concatenate n_words words within group sorted by weight descending.\n",
    "        There must be a better way of doing this...\n",
    "        \"\"\"\n",
    "        logger.info('Compiling topic-tokens overview...')\n",
    "\n",
    "        df = topic_token_weights.groupby('topic_id')\\\n",
    "            .apply(lambda x: sorted(list(zip(x[\"token\"], x[\"weight\"])), key=lambda z: z[1], reverse=True))\\\n",
    "            .apply(lambda x: ' '.join([z[0] for z in x][:n_words])).reset_index()\n",
    "        df['alpha'] = df.topic_id.apply(lambda topic_id: alpha[topic_id])\n",
    "        df.columns = ['topic_id', 'tokens', 'alpha']\n",
    "\n",
    "        return df.set_index('topic_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_document_topics(model, corpus, documents, minimum_probability=0.001):\n",
    "\n",
    "        def document_topics_iter(model, corpus, minimum_probability):\n",
    "\n",
    "            data_iter = model.get_document_topics(corpus, minimum_probability=minimum_probability)\\\n",
    "                if hasattr(model, 'get_document_topics')\\\n",
    "                else model.load_document_topics()\n",
    "\n",
    "            for i, topics in enumerate(data_iter):\n",
    "                for x in topics:\n",
    "                    yield (i, x[0], x[1])\n",
    "        '''\n",
    "        Get document topic weights for all documents in corpus\n",
    "        Note!  minimum_probability=None filters less probable topics, set to 0 to retrieve all topcs\n",
    "\n",
    "        If gensim model then use 'get_document_topics', else 'load_document_topics' for mallet model\n",
    "        '''\n",
    "        logger.info('Compiling document topics...')\n",
    "        logger.info('  Creating data iterator...')\n",
    "        data = document_topics_iter(model, corpus, minimum_probability)\n",
    "        logger.info('  Creating frame from iterator...')\n",
    "        df_doc_topics = pd.DataFrame(data, columns=[ 'document_id', 'topic_id', 'weight' ]).set_index('document_id')\n",
    "        logger.info('  Merging data...')\n",
    "        df = pd.merge(documents, df_doc_topics, how='inner', left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_compiled_data(model, corpus, id2term, documents):\n",
    "\n",
    "        dictionary = LdaDataCompiler.compile_dictionary(model)\n",
    "        topic_token_weights = LdaDataCompiler.compile_topic_token_weights(model, dictionary, num_words=200)\n",
    "        topic_token_overview = LdaDataCompiler.compile_topic_token_overview(topic_token_weights, model.alpha)\n",
    "        document_topic_weights = LdaDataCompiler.compile_document_topics(model, corpus, documents, minimum_probability=0.001)\n",
    "\n",
    "        return types.SimpleNamespace(\n",
    "            dictionary=dictionary,\n",
    "            documents=documents,\n",
    "            topic_token_weights=topic_token_weights,\n",
    "            topic_token_overview=topic_token_overview,\n",
    "            document_topic_weights=document_topic_weights\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_topic_titles(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id==topic_id)]\n",
    "        df = df_temp\\\n",
    "                .sort_values('weight', ascending=False)\\\n",
    "                .groupby('topic_id')\\\n",
    "                .apply(lambda x: ' '.join(x.token[:n_words].str.title()))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_title(topic_token_weights, topic_id, n_words=100):\n",
    "        return LdaDataCompiler.get_topic_titles(topic_token_weights, topic_id, n_words=n_words).iloc[0]\n",
    "\n",
    "    #get_topics_tokens_as_text = get_topic_titles\n",
    "    #get_topic_tokens_as_text = get_topic_title\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_tokens(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id == topic_id)]\n",
    "        df = df_temp.sort_values('weight', ascending=False)[:n_words]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lda_topics(model, n_tokens=20):\n",
    "        return pd.DataFrame({\n",
    "            'Topic#{:02d}'.format(topic_id+1) : [ word[0] for word in model.show_topic(topic_id, topn=n_tokens) ]\n",
    "                for topic_id in range(model.num_topics)\n",
    "        })\n",
    "\n",
    "# OBS OBS! https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "DEFAULT_VECTORIZE_PARAMS = dict(tf_type='linear', apply_idf=False, idf_type='smooth', norm='l2', min_df=1, max_df=0.95)\n",
    "\n",
    "def compute_topic_model(corpus, tick=utility.noop, method='sklearn_lda', vec_args=None, term_args=None, tm_args=None, **args):\n",
    "    \n",
    "    tick()\n",
    "    vec_args = utility.extend({}, DEFAULT_VECTORIZE_PARAMS, vec_args)\n",
    "    \n",
    "    terms_iter = lambda: (filter_terms(doc, term_args) for doc in corpus)\n",
    "    tick()\n",
    "    \n",
    "    vectorizer = textacy.Vectorizer(**vec_args)\n",
    "    doc_term_matrix = vectorizer.fit_transform(terms_iter())\n",
    "\n",
    "    if method.startswith('sklearn'):\n",
    "        tm_model = textacy.TopicModel(method.split('_')[1], **tm_args)\n",
    "        tm_model.fit(doc_term_matrix)\n",
    "        tick()\n",
    "        doc_topic_matrix = tm_model.transform(doc_term_matrix)\n",
    "        tick()\n",
    "        tm_id2word = vectorizer.id_to_term\n",
    "        tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        compiled_data = None # FIXME\n",
    "    else:\n",
    "        doc_topic_matrix = None # ?\n",
    "        tm_id2word = corpora.Dictionary(terms_iter())\n",
    "        tm_corpus = [ tm_id2word.doc2bow(text) for text in terms_iter() ]\n",
    "        #tm_id2word = vectorizer.id_to_term\n",
    "        #tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        tm_model = models.LdaModel(\n",
    "            tm_corpus, \n",
    "            num_topics  =  tm_args.get('n_topics', 0),\n",
    "            id2word     =  tm_id2word,\n",
    "            iterations  =  tm_args.get('max_iter', 0),\n",
    "            passes      =  20,\n",
    "            alpha       = 'asymmetric'\n",
    "        )\n",
    "        documents = get_corpus_documents(corpus)\n",
    "        compiled_data = LdaDataCompiler.compute_compiled_data(tm_model, tm_corpus, tm_id2word, documents)\n",
    "    \n",
    "    tm_data = types.SimpleNamespace(\n",
    "        tm_model=tm_model,\n",
    "        tm_id2term=tm_id2word,\n",
    "        tm_corpus=tm_corpus,\n",
    "        doc_term_matrix=doc_term_matrix,\n",
    "        doc_topic_matrix=doc_topic_matrix,\n",
    "        vectorizer=vectorizer,\n",
    "        compiled_data=compiled_data\n",
    "    )\n",
    "    \n",
    "    tick(0)\n",
    "    \n",
    "    return tm_data\n",
    "\n",
    "def get_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    topic_ids = range(0,doc_topic_matrix.shape[1])\n",
    "    for document_id in range(0,doc_topic_matrix.shape[1]):\n",
    "        topic_weights = doc_topic_matrix[document_id, :]\n",
    "        for topic_id in topic_ids:\n",
    "            if topic_weights[topic_id] >= threshold:\n",
    "                yield (document_id, topic_id, topic_weights[topic_id])\n",
    "\n",
    "def get_df_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    it = get_doc_topic_weights(doc_topic_matrix, threshold)\n",
    "    df = pd.DataFrame(list(it), columns=['document_id', 'topic_id', 'weight']).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "def display_topic_model_gui(corpus, compute_callback):\n",
    "    \n",
    "    pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]\n",
    "    # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    engine_options = { 'gensim': 'gensim' } #, 'sklearn_lda': 'sklearn_lda'}\n",
    "    normalize_options = { 'None': False, 'Use lemma': 'lemma', 'Lowercase': 'lower'}\n",
    "    ngrams_options = { '1': [1], '1, 2': [1, 2], '1,2,3': [1, 2, 3] }\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        n_topics=widgets.IntSlider(description='#topics', min=5, max=50, value=20, step=1),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1),\n",
    "        max_iter=widgets.IntSlider(description='Max iterations', min=100, max=1000, value=20, step=10),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=widgets.Layout(width='200px')),\n",
    "        filter_stops=widgets.ToggleButton(value=True, description='Remove stopword',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=True, description='Remove nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        apply_idf=widgets.ToggleButton(value=False, description='Apply IDF',  tooltip='Apply TF-IDF', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=['NOUN', 'PROPN'], rows=7, layout=widgets.Layout(width='200px')),\n",
    "        method=widgets.Dropdown(description='Engine', options=engine_options, value='gensim', layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute'),\n",
    "        boxes=None,\n",
    "        output = widgets.Output(), # layout={'height': '500px'}),\n",
    "        model=None\n",
    "    )\n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.n_topics,\n",
    "                gui.min_freq,\n",
    "                gui.max_iter\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners,\n",
    "                gui.apply_idf\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.method\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.VBox([gui.output]), # ,layout=widgets.Layout(top='20px', height='500px',width='100%'))\n",
    "    ])\n",
    "    fx = lambda *args: compute_callback(corpus, gui, *args)\n",
    "    gui.compute.on_click(fx)\n",
    "    return gui\n",
    "    \n",
    "\n",
    "def compute_callback(corpus, gui, *args):\n",
    "    \n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "    \n",
    "    tick(1)\n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        vec_args = dict(apply_idf=gui.apply_idf.value)\n",
    "        term_args = dict(\n",
    "            args=dict(\n",
    "                ngrams=gui.ngrams.value,\n",
    "                named_entities=gui.named_entities.value,\n",
    "                normalize=gui.normalize.value,\n",
    "                as_strings=True\n",
    "            ),\n",
    "            kwargs=dict(\n",
    "                filter_nums=gui.filter_nums.value,\n",
    "                drop_determiners=gui.drop_determiners.value,\n",
    "                min_freq=gui.min_freq.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "                filter_stops=gui.filter_stops.value,\n",
    "                filter_punct=True\n",
    "            )\n",
    "        )\n",
    "        tm_args = dict(\n",
    "            n_topics=gui.n_topics.value,\n",
    "            max_iter=gui.max_iter.value,\n",
    "            learning_method='online', \n",
    "            n_jobs=1\n",
    "        )\n",
    "        method = gui.method.value\n",
    "        gui.model = compute_topic_model(\n",
    "            corpus=corpus,\n",
    "            tick=tick,\n",
    "            method=method,\n",
    "            vec_args=vec_args,\n",
    "            term_args=term_args,\n",
    "            tm_args=tm_args\n",
    "        )\n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        #display(gui.model.compiled_data.topic_token_overview)\n",
    "        display(LdaDataCompiler.get_lda_topics(gui.model.tm_model, n_tokens=20))\n",
    "        \n",
    "TM_GUI_MODEL = display_topic_model_gui(CORPUS, compute_callback)\n",
    "display(TM_GUI_MODEL.boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_document_entities_gui(corpus):\n",
    "    \n",
    "    def display_document_entities(document_id, corpus):\n",
    "        displacy.render(corpus[document_id].spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    df_documents = get_corpus_documents(corpus)\n",
    "\n",
    "    document_widget = widgets.Dropdown(description='Paper', options={v: k for k, v in df_documents['title'].to_dict().items()}, value=0, layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(display_document_entities,document_id=document_widget, corpus=widgets.fixed(corpus))\n",
    "\n",
    "    display(widgets.VBox([document_widget, widgets.VBox([itw.children[-1]],layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))]))\n",
    "\n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    display_document_entities_gui(CORPUS)\n",
    "except Except as ex:\n",
    "    logger.error(ec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Wordcloud<span style='color: red; float: right'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import common.widgets_utility as widgets_utility\n",
    "\n",
    "def display_wordcloud_gui(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    model = tm_data.tm_model\n",
    "    output_options = output_options or []\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(\n",
    "    tm_data,\n",
    "    topic_id=0,\n",
    "    n_words=100,\n",
    "    output_format='Wordcloud',\n",
    "    widget_container=None\n",
    "):\n",
    "    container = tm_data.compiled_data\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = container.topic_token_weights.loc[(container.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=n_words)\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id, n_words=n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(LdaDataCompiler.get_topic_tokens(topic_id, n_words)))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    display_wordcloud_gui(display_wordcloud, tm_data, 'tx02', ['Wordcloud', 'Table', 'Pivot'])\n",
    "except TopicModelNotComputed as ex:\n",
    "    logger.info(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Chart<span style='color: red; float: right'>TRY IT</span>\n",
    "The following chart shows the word distribution for each selected topic. You can zoom in on the left chart. The distribution seems to follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) as (perhaps) expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display topic's word distribution\n",
    "if False:\n",
    "    from common.model_utility import ModelUtility\n",
    "    from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "    from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "    import math\n",
    "\n",
    "    from itertools import product\n",
    "    \n",
    "    import bokeh.models as bm\n",
    "    import bokeh.palettes\n",
    "    from bokeh.io import output_file, push_notebook\n",
    "    from bokeh.core.properties import value, expr\n",
    "    from bokeh.transform import transform, jitter\n",
    "    from bokeh.layouts import row, column, widgetbox\n",
    "    from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "    from bokeh.models import ColumnDataSource, CustomJS\n",
    "    \n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(tokens)\n",
    "\n",
    "    p = bokeh.plotting.figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = bokeh.models.ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bokeh.models.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def display_topic_tokens(tm_data, topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        p = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        bokeh.plotting.show(p)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "    widget_container.reset()\n",
    "    \n",
    "def display_topic_distribution_widgets(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "TM_DATA = TM_GUI_MODEL.model\n",
    "\n",
    "display_topic_distribution_widgets(display_topic_tokens, TM_DATA, 'wc01', ['Chart', 'Table'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Trend Over Time or Documents<span style='color: red; float: right'>RUN</span>\n",
    "- Displays topic's share over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "#import numpy as np\n",
    "#import math\n",
    "#import bokeh.plotting\n",
    "#from bokeh.models import ColumnDataSource, DataRange1d, Plot, LinearAxis, Grid\n",
    "#from bokeh.models.glyphs import VBar\n",
    "#from bokeh.io import curdoc, show\n",
    "\n",
    "import math\n",
    "\n",
    "def plot_topic_trend(df, pivot_column, value_column, x_label=None, y_label=None):\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "\n",
    "    xs = df[pivot_column].astype(np.str)\n",
    "    p = bokeh.plotting.figure(x_range=xs, plot_width=1000, plot_height=700, title='', tools=tools, toolbar_location=\"right\")\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=df[value_column], width=0.5, fill_color=\"#b3de69\")\n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or '').title()\n",
    "    p.yaxis[0].axis_label = (y_label or '').title()\n",
    "    p.y_range.start = 0.0\n",
    "    #p.y_range.end = 1.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    return p\n",
    "\n",
    "def display_topic_trend(topic_id, widgets_container, output_format='Chart', tm_data=None, threshold=0.01):\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=200)\n",
    "    widgets_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    value_column = 'weight'\n",
    "    category_column = 'author'\n",
    "    df = container.document_topic_weights[(container.document_topic_weights.topic_id==topic_id)]\n",
    "    df = df[(df.weight > threshold)].reset_index()\n",
    "    df[category_column] = df.title.apply(slim_title)\n",
    "\n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        x_label = category_column.title()\n",
    "        y_label = value_column.title()\n",
    "        p = plot_topic_trend(df, category_column, value_column, x_label=x_label, y_label=y_label)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "def create_topic_trend_widgets(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id='topic_share_plot',\n",
    "        text=wf.create_text_widget('topic_share_plot'),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=wc.topic_id,\n",
    "        widgets_container=widgets.fixed(wc),\n",
    "        output_format=wc.output_format,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=wc.threshold\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.output_format]),\n",
    "        widgets.HBox([wc.topic_id, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "create_topic_trend_widgets(tm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic to Document Network<span style='color: red; float: right'>TRY IT</span>\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "def plot_document_topic_network(network, layout, scale=1.0, titles=None):\n",
    "\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = bokeh.plotting.figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=TOOLS)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(renderers=[r_topics], tooltips=None, callback=widgets_utility.wf.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(x='x', y='y', text='name', level='overlay', x_offset=0, y_offset=0, text_font_size='8pt')\n",
    "    \n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "\n",
    "def main_topic_network(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    text_id = 'nx_id1'\n",
    "    layout_options = [ 'Circular', 'Kamada-Kawai', 'Fruchterman-Reingold']\n",
    "    text_widget = widgets_utility.wf.create_text_widget(text_id)  # style=\"display: inline; height='400px'\"),\n",
    "    scale_widget = widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False)\n",
    "    threshold_widget = widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False)\n",
    "    output_format_widget = widgets_utility.dropdown('Output', { 'Network': 'network', 'Table': 'table' }, 'network')\n",
    "    layout_widget = widgets_utility.dropdown('Layout', layout_options, 'Fruchterman-Reingold')\n",
    "    progress_widget = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"40%\"))\n",
    "    \n",
    "    def tick(x=None):\n",
    "        progress_widget.value = progress_widget.value + 1 if x is None else x\n",
    "        \n",
    "    def display_topic_network(layout_algorithm, tm_data, threshold=0.10, scale=1.0, output_format='network'):\n",
    "            \n",
    "        tick(1)\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights)\n",
    "\n",
    "        df = container.document_topic_weights[container.document_topic_weights.weight > threshold].reset_index()\n",
    "        \n",
    "        df['slim_title'] = df.title.apply(slim_title)\n",
    "        network = NetworkUtility.create_bipartite_network(df, 'slim_title', 'topic_id')\n",
    "        \n",
    "        tick()\n",
    "\n",
    "        if output_format == 'network':\n",
    "            \n",
    "            args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "            layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "            \n",
    "            tick()\n",
    "            \n",
    "            p = plot_document_topic_network(network, layout, scale=scale, titles=titles)\n",
    "            bokeh.plotting.show(p)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            display(df)\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "\n",
    "        tick(0)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_network,\n",
    "        layout_algorithm=layout_widget,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=threshold_widget,\n",
    "        scale=scale_widget,\n",
    "        output_format=output_format_widget\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        text_widget,\n",
    "        widgets.HBox([layout_widget, threshold_widget]), \n",
    "        widgets.HBox([output_format_widget, scale_widget, progress_widget]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "main_topic_network(tm_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Trends - Heatmap\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanfordâ€™s Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "import bokeh.transform\n",
    "\n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = ['#ffffff', '#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b']\n",
    "    mapper = bokeh.models.LinearColorMapper(palette=colors, low=0.0, high=1.0) # low=df.weight.min(), high=max_weight)\n",
    "    color_transform = bokeh.transform.transform('weight', mapper)\n",
    "    color_bar = bokeh.models.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bokeh.models.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bokeh.models.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, glyph, titles, text_id):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "    \n",
    "    ''' Setup axis categories '''\n",
    "    x_range = list(map(str, df[xs].unique()))\n",
    "    y_range = list(map(str, df[ys].unique()))\n",
    "    \n",
    "    ''' Setup coloring and color bar '''\n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = bokeh.models.ColumnDataSource(df)\n",
    "\n",
    "    plot_height = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = bokeh.plotting.figure(title=\"Topic heatmap\", tools=TOOLS, toolbar_location=\"right\", x_range=x_range,\n",
    "           y_range=y_range, x_axis_location=\"above\", plot_width=1000, plot_height=plot_height)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    if glyph == 'Circle':\n",
    "        cr = p.circle(color=color_transform, **args)\n",
    "    else:\n",
    "        cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"8pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(tooltips=None, callback=widgets_utility.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "    \n",
    "def display_doc_topic_heatmap(tm_data, key='max', flip_axis=False, glyph='Circle'):\n",
    "    try:\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights, n_words=100)\n",
    "        df = container.document_topic_weights.copy().reset_index()\n",
    "        df['document_id'] = df.document_id.astype(str)\n",
    "        df['topic_id'] = df.topic_id.astype(str)\n",
    "        df['author'] = df.title.apply(slim_title)\n",
    "        p = plot_topic_relevance_by_year(df, xs='author', ys='topic_id', flip_axis=flip_axis, glyph=glyph, titles=titles, text_id='topic_relevance')\n",
    "        bokeh.plotting.show(p)\n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        logger.error(ex)\n",
    "            \n",
    "def doc_topic_heatmap_gui(tm_data):\n",
    "    \n",
    "    def text_widget(element_id=None, default_value='', style='', line_height='20px'):\n",
    "        value = \"<span class='{}' style='line-height: {};{}'>{}</span>\".format(element_id, line_height, style, default_value) if element_id is not None else ''\n",
    "        return widgets.HTML(value=value, placeholder='', description='', layout=widgets.Layout(height='150px'))\n",
    "\n",
    "    text_id = 'topic_relevance'\n",
    "    #text_widget = widgets_utility.wf.create_text_widget(text_id)\n",
    "    text_widget = text_widget(text_id)\n",
    "    glyph = widgets.Dropdown(options=['Circle', 'Square'], value='Square', description='Glyph', layout=widgets.Layout(width=\"180px\"))\n",
    "    flip_axis = widgets.ToggleButton(value=True, description='Flip XY', tooltip='Flip X and Y axis', icon='', layout=widgets.Layout(width=\"80px\"))\n",
    "\n",
    "    iw = widgets.interactive(display_doc_topic_heatmap, tm_data=widgets.fixed(tm_data), glyph=glyph, flip_axis=flip_axis)\n",
    "\n",
    "    display(widgets.VBox([widgets.HBox([flip_axis, glyph ]), text_widget, iw.children[-1]]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "doc_topic_heatmap_gui(get_current_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Key Terms \n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d3eb74277945aeb6d50f71fbf0801d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Paper', index=555, layout=Layout(width='40%'), options={'1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy.keyterms\n",
    "\n",
    "def display_document_key_terms_gui(corpus):\n",
    "    \n",
    "    df_documents = get_corpus_documents(corpus)\n",
    "    methods = { 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    document_options = {v: k for k, v in df_documents['title'].to_dict().items()}\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=widgets.Layout(width='240px')),\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'TextRank', 'SingleRank' ], value='TextRank', layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=0, normalize='lemma', n_keyterms=10):\n",
    "        keyterms = methods[method](corpus[document_id], normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ' '.join([ x for x, y in keyterms ])\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            display(terms)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_key_terms_gui(CORPUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
