{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Culture of International Relations - Corpus statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-10 18:22:00,574 : INFO : WTI index loaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import common.treaty_utility as treaty_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import treaty_corpus\n",
    "import types, glob\n",
    "import textacy.keyterms\n",
    "import qgrid\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import pickle\n",
    "import topic_model\n",
    "import topic_model_utility\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "#pd.options.display.max_colwidth = -1\n",
    "pd.options.display.colheader_justify = 'left'\n",
    "#pd.options.display.precision = 4\n",
    "\n",
    "DATA_FOLDER = '../data'\n",
    "PATTERN = '*.txt'\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "\n",
    "FIXED_STOPWORDS = ['', '\\n', 'et', 'al', 'et.al.' ]\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')   \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "class TopicModelNotComputed(Exception):\n",
    "    @staticmethod\n",
    "    def check():\n",
    "        if 'TM_GUI_MODEL' in globals():\n",
    "            gui =  globals()['TM_GUI_MODEL']\n",
    "            if None not in (gui, gui.model):\n",
    "                return True\n",
    "        msg = 'A topic model must be computed using step \"MODEL Compute a Topic Model\"'\n",
    "        raise TopicModelNotComputed(msg)\n",
    "        \n",
    "class CorpusNotLoaded(Exception):\n",
    "    pass\n",
    "\n",
    "def get_current_model():\n",
    "    TopicModelNotComputed.check()\n",
    "    return globals()['TM_GUI_MODEL'].model\n",
    "\n",
    "def get_current_corpus():\n",
    "    if 'CURRENT_CORPUS' in globals():\n",
    "        if globals()['CURRENT_CORPUS'].textacy_corpus is not None:\n",
    "            return globals()['CURRENT_CORPUS']\n",
    "    raise CorpusNotLoaded('Corpus not loaded or computed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f62a296c014fcf88a7071b5aeb47ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(Dropdown(description='C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy_corpus_utility \n",
    "import textacy_corpus_gui\n",
    "\n",
    "CURRENT_CORPUS = types.SimpleNamespace(\n",
    "    language=None,\n",
    "    source_path=None,\n",
    "    prepped_source_path=None,\n",
    "    textacy_corpus_path=None,\n",
    "    textacy_corpus=None,\n",
    "    nlp=None\n",
    ")\n",
    "\n",
    "try:\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, WTI_INDEX, CURRENT_CORPUS)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Find Key Terms <span style='float: right; color: green'>OPTIONAL</span>\n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n",
    "- [RAKE]\tRose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In M. W. Berry & J. Kogan (Eds.), Text Mining: Theory and Applications: John Wiley & Son\n",
    "https://github.com/csurfer/rake-nltk\n",
    "https://github.com/aneesha/RAKE\n",
    "https://github.com/vgrabovets/multi_rake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>RAKE <span style='float: right; color: green'>WORK IN PROGRESS</span>\n",
    "\n",
    "https://github.com/JRC1995/RAKE-Keyword-Extraction\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f959db1f0542019a758e4b05c8c8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='95%'), max=1), HBox(children=(Dropdown(description='T…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Document Key Terms\n",
    "from rake_nltk import Rake, Metric\n",
    "import string\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import gui_utility\n",
    "\n",
    "def textacy_rake(doc, language='english', normalize='lemma', n_keyterms=20, stopwords=None, metric=Metric.DEGREE_TO_FREQUENCY_RATIO):\n",
    "    punctuations = string.punctuation + \"\\\"\"\n",
    "    r = Rake(\n",
    "        stopwords=stopwords,  # NLTK stopwords if None\n",
    "        punctuations=punctuations, # NLTK by default\n",
    "        language=\"english\",\n",
    "        ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,\n",
    "        max_length=100000,\n",
    "        min_length=1\n",
    "    )\n",
    "    text = ' '.join([ x.lemma_ for x in doc.spacy_doc ] if normalize == 'lemma' else [ x.lower_ for x in doc.spacy_doc ])\n",
    "    r.extract_keywords_from_text(doc.text)\n",
    "    keyterms = [ (y, x) for (x, y) in r.get_ranked_phrases_with_scores() ]\n",
    "    return keyterms[:n_keyterms]\n",
    "\n",
    "\n",
    "def display_rake_gui(corpus, language):\n",
    "    \n",
    "    document_options = gui_utility.get_treaty_dropdown_options(WTI_INDEX, corpus)\n",
    "    metric_options = [\n",
    "        ('Degree / Frequency', Metric.DEGREE_TO_FREQUENCY_RATIO),\n",
    "        ('Degree', Metric.WORD_DEGREE),\n",
    "        ('Frequency', Metric.WORD_FREQUENCY)\n",
    "    ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=widgets.Layout(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=10, step=1, layout=widgets.Layout(width='340px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='40%')),\n",
    "        metric=widgets.Dropdown(description='Metric', options=metric_options, value=Metric.DEGREE_TO_FREQUENCY_RATIO, layout=widgets.Layout(width='300px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def compute_textacy_rake(corpus, treaty_id, language, normalize, n_keyterms, metric):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            doc = textacy_utility.get_treaty_doc(corpus, treaty_id)\n",
    "            phrases = textacy_rake(doc, language=language, normalize=normalize, n_keyterms=n_keyterms, stopwords=None, metric=metric)\n",
    "            df = pd.DataFrame(phrases, columns=['phrase', 'score'])\n",
    "            display(df.set_index('phrase'))\n",
    "            return df\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        compute_textacy_rake,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        treaty_id=gui.document_id,\n",
    "        language=widgets.fixed(language),\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "        metric=gui.metric\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.metric, gui.normalize]),\n",
    "        widgets.HBox([gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "try:\n",
    "    corpus = get_current_corpus().textacy_corpus\n",
    "    display_rake_gui(corpus, language='english')\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>TextRank/SingleRank <span style='float: right; color: green'>OPTIONAL</span>\n",
    "\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef195c76d5d492ea6738b20676387e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='95%'), max=1), HBox(children=(Dropdown(description='T…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "\n",
    "def display_document_key_terms_gui(corpus, wti_index):\n",
    "    \n",
    "    methods = { 'RAKE': textacy_rake, 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=widgets.Layout(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=widgets.Layout(width='240px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'RAKE', 'TextRank', 'SingleRank' ], value='TextRank', layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def get_keyterms(method, doc, normalize, n_keyterms):\n",
    "        keyterms = methods[method](doc, normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ', '.join([ x for x, y in keyterms ])\n",
    "        gui.progress.value += 1\n",
    "        return terms\n",
    "    \n",
    "    def get_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        treaty_ids = [ document_id ] if document_id is not None else [ doc.metadata['treaty_id'] for doc in corpus ]\n",
    "        gui.progress.value = 0\n",
    "        gui.progress.max = len(treaty_ids)\n",
    "        keyterms = [\n",
    "            get_keyterms(method, textacy_utility.get_treaty_doc(corpus, treaty_id), normalize, n_keyterms) for treaty_id in treaty_ids\n",
    "        ]\n",
    "        df = pd.DataFrame({ 'treaty_id': treaty_ids, 'keyterms': keyterms}).set_index('treaty_id')\n",
    "        gui.progress.value = 0\n",
    "        return df\n",
    "\n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df = get_document_key_terms(corpus, method, document_id, normalize, n_keyterms)\n",
    "            #qgrid_widget = qgrid.show_grid(df, show_toolbar=False)\n",
    "            #display(qgrid_widget)\n",
    "            table = TableDisplay(df)\n",
    "            display(table)\n",
    "            \n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_key_terms_gui(CURRENT_CORPUS.textacy_corpus, WTI_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e50e9d93e1c484cba2f9fe13f4d0314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':'tight'}\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(8, 9 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='75')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def display_cleaned_up_text(container, gui, display_type, treaty_id, **kwargs): # ngrams, named_entities, normalize, include_pos):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    gui.output_text.clear_output()\n",
    "    gui.output_statistics.clear_output()\n",
    "    \n",
    "    doc = textacy_utility.get_treaty_doc(corpus, treaty_id)\n",
    "    \n",
    "    if doc is None:\n",
    "        return\n",
    "    \n",
    "    terms = [ x for x in doc.to_terms_list(as_strings=True, **kwargs) ]\n",
    "    \n",
    "    if display_type.startswith('source_text'):\n",
    "        \n",
    "        source_files = {\n",
    "            'source_text_raw': { 'filename': container.source_path, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "            'source_text_edited': { 'filename': container.source_path, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "            'source_text_preprocessed': { 'filename': container.prepped_source_path, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "        }        \n",
    "        \n",
    "        source_filename = source_files[display_type]['filename']\n",
    "        description =  source_files[display_type]['description']\n",
    "        text = utility.zip_get_text(source_filename, doc.metadata['filename'])\n",
    "        \n",
    "        with gui.output_text:\n",
    "            #print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            #print(doc)\n",
    "            print('[ ' + description.upper() + ' ]')\n",
    "            print(text)\n",
    "        return\n",
    "\n",
    "    if len(terms) == 0:\n",
    "        with gui.output_text:\n",
    "            print(\"No text. Please change selection.\")\n",
    "        return\n",
    "    \n",
    "    if display_type in ['sanitized_text', 'statistics']:\n",
    "\n",
    "        if display_type == 'sanitized_text':\n",
    "            with gui.output_text:\n",
    "                #display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                #    ' '.join(tokens[:word_count]),\n",
    "                #    ' '.join(tokens[-word_count:])\n",
    "                #))\n",
    "                print(' '.join([ t.replace(' ', '_') for t in terms ]))\n",
    "                return\n",
    "\n",
    "        if display_type == 'statistics':\n",
    "\n",
    "            wf = nltk.FreqDist(terms)\n",
    "\n",
    "            with gui.output_text:\n",
    "\n",
    "                df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                print('Token count: {} Vocab count: {}'.format(wf.N(), wf.B()))\n",
    "                display(df)\n",
    " \n",
    "            with gui.output_statistics:\n",
    "\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "def display_cleanup_text_gui(container, wti_index):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "    \n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = [('(All)', None)] + sorted([(k + ' (' + v + ')', k) for k,v in pos_tags.items() ])\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    gui = types.SimpleNamespace(\n",
    "        treaty_id=widgets.Dropdown(description='Treaty', options=document_options, value=None, layout=widgets.Layout(width='400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.FloatSlider(value=0, min=0, max=1.0, step=0.01, description='Min frequency', layout=widgets.Layout(width='400px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ False, 'lemma', 'lower' ], value=False, layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=False, description='Filter nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=False, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    uix = widgets.interactive(\n",
    "\n",
    "        display_cleaned_up_text,\n",
    "\n",
    "        container=widgets.fixed(container),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        treaty_id=gui.treaty_id,\n",
    "        \n",
    "        ngrams=gui.ngrams,\n",
    "        named_entities=gui.named_entities,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        filter_nums=gui.filter_nums,\n",
    "        include_pos=gui.include_pos,\n",
    "        min_freq=gui.min_freq,\n",
    "        drop_determiners=gui.drop_determiners\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.treaty_id,\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "                                  \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "try:\n",
    "    xgui, xuix = display_cleanup_text_gui(get_current_corpus(), WTI_INDEX)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> List of most frequent words<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025d1c97ed164eeea7cb127ac314190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import attrs\n",
    "\n",
    "ADDITIONAL_STOPWORDS = []\n",
    "\n",
    "def textacy_doc_to_bow(doc, target='lemma', weighting='count', as_strings=False, include=None):\n",
    "\n",
    "    spacy_doc = doc.spacy_doc\n",
    "    \n",
    "    weighing_keys = { 'count', 'freq' }\n",
    "    target_keys = { 'lemma': attrs.LEMMA, 'lower': attrs.LOWER, 'orth': attrs.ORTH }\n",
    "    \n",
    "    default_exclude = lambda x: x.is_stop or x.is_punct or x.is_space\n",
    "    exclude = default_exclude if include is None else lambda x: default_exclude(x) or not include(x)\n",
    "    \n",
    "    assert weighting in weighing_keys\n",
    "    assert target in target_keys\n",
    "\n",
    "    target_weights = spacy_doc.count_by(target_keys[target], exclude=exclude)\n",
    "    \n",
    "    if weighting == 'freq':\n",
    "        n_tokens = sum(target_weights.values())\n",
    "        target_weights = {id_: weight / n_tokens for id_, weight in target_weights.items()}\n",
    "\n",
    "    if as_strings:\n",
    "        bow = { doc.spacy_stringstore[word_id]: count for word_id, count in target_weights.items() }\n",
    "    else:\n",
    "        bow = { word_id: count for word_id, count in target_weights.items() }\n",
    "        \n",
    "    return bow\n",
    "    \n",
    "#def word_weights(corpus, docs, normalize='lemma'):\n",
    "#    \n",
    "#    docs = list(docs)\n",
    "#    word_weights = collections.Counter()\n",
    "#    \n",
    "#    for doc in docs:\n",
    "#        textacy_filter_terms(doc, term_args, chunk_size=None, min_length=2)\n",
    "#        word_weights.update(doc.to_bag_of_words(normalize=normalize, weighting='freq', as_strings=False))\n",
    "#\n",
    "#    n_documents = len(docs)\n",
    "#    word_weights = { word: weight / n_documents for word, weight in word_weights.items() }\n",
    "#    \n",
    "#    df = pd.DataFrame({'word_id': list(word_weights.keys()),  'weight': list(word_weights.values()) })\n",
    "#    df['token'] = df.word_id.apply(lambda x: corpus.spacy_vocab[x].text)\n",
    "#    \n",
    "#    return df\n",
    "\n",
    "def trunc_year_by(series, divisor):\n",
    "    return (series - series.mod(divisor)).astype(int) \n",
    "\n",
    "TREATY_TIME_GROUPINGS = {\n",
    "    'treaty_id': { 'column': 'treaty_id', 'divisor': None, 'title': 'Treaty', 'fx': None},\n",
    "    'signed_year': { 'column': 'signed_year', 'divisor': 1, 'title': 'Year', 'fx': None },\n",
    "    'signed_lustrum': { 'column': 'signed_lustrum', 'divisor': 5, 'title': 'Lustrum', 'fx': lambda df: trunc_year_by(df.signed_year, 5) },\n",
    "    'signed_decade': { 'column': 'signed_decade', 'divisor': 10, 'title': 'Decade', 'fx': lambda df: trunc_year_by(df.signed_year, 10) }\n",
    "}\n",
    "\n",
    "def display_list_of_most_frequent_words(\n",
    "    data_folder,\n",
    "    wti_index,\n",
    "    container,\n",
    "    gui,\n",
    "    group_by_column='signed_year',\n",
    "    parties=None,\n",
    "    target='lemma',\n",
    "    weighting='count',\n",
    "    include_pos=None,\n",
    "    stop_words=None\n",
    "):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    stop_words = stop_words or set()\n",
    "    \n",
    "    def include(token):\n",
    "        flag = True\n",
    "        if not include_pos is None:\n",
    "             flag = flag and token.pos_ in include_pos\n",
    "        flag = flag and token.lemma_ not in stop_words\n",
    "        return flag\n",
    "    \n",
    "    gui.progress.max = len(corpus)\n",
    "    \n",
    "    df_freqs = pd.DataFrame({ 'treaty_id': [], 'signed_year': [], 'token': [], 'count': [] })\n",
    "    \n",
    "    parties_set = set(parties or [])\n",
    "    \n",
    "    docs = corpus if len(parties_set) == 0 \\\n",
    "        else ( x for x in corpus if len(set((x.metadata['party1'], x.metadata['party2'])) & parties_set) > 0 )\n",
    "                                                   \n",
    "    for doc in docs:\n",
    "        \n",
    "        doc_freqs = textacy_doc_to_bow(doc, target=target, weighting=weighting, as_strings=True, include=include)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'treaty_id': doc.metadata['treaty_id'],\n",
    "            'signed_year': int(doc.metadata['signed_year']),\n",
    "            'token': list(doc_freqs.keys()),\n",
    "            'count': list(doc_freqs.values())\n",
    "        })\n",
    "        \n",
    "        df_freqs = df_freqs.append(df)\n",
    "        gui.progress.value = gui.progress.value + 1\n",
    "        \n",
    "    df_freqs['signed_year'] = df_freqs.signed_year.astype(int)\n",
    "    \n",
    "    for key, group in TREATY_TIME_GROUPINGS.items():\n",
    "        if key in df_freqs.columns:\n",
    "            continue\n",
    "        df_freqs[key] = (group['fx'])(df_freqs)\n",
    "        \n",
    "    df_freqs['term'] = df_freqs.token # if True else df_freqs.token\n",
    "    \n",
    "    df_freqs = df_freqs.groupby([group_by_column, 'term']).mean().reset_index()[[group_by_column, 'term', 'count']]\n",
    "    \n",
    "    df_freqs['position'] = df_freqs.sort_values(by=[group_by_column, 'count'], ascending=False).groupby([group_by_column]).cumcount() + 1\n",
    "    \n",
    "    gui.progress.value = 0\n",
    "    \n",
    "    return df_freqs\n",
    "    \n",
    "def get_most_frequent_words(corpus, n_top, normalize='lemma', include_pos=None):\n",
    "    include_pos = include_pos or [ 'VERB', 'NOUN', 'PROPN' ]\n",
    "    include = lambda x: x.pos_ in include_pos\n",
    "    word_counts = collections.Counter()\n",
    "    for doc in corpus:\n",
    "        bow = textacy_doc_to_bow(doc, target=normalize, weighting='count', as_strings=True, include=include)\n",
    "        word_counts.update(bow)\n",
    "    return word_counts.most_common(n_top)\n",
    "\n",
    "def list_of_most_frequent_words_gui(data_folder, wti_index, container):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    include_pos_tags = [ 'ADJ', 'VERB', 'NUM', 'ADV', 'NOUN', 'PROPN' ]\n",
    "    \n",
    "    #pos_tags = DF_TAGSET[DF_TAGSET.POS.isin(include_pos_tags)].groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    #pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    counter = collections.Counter(corpus.word_freqs(normalize='lemma', weighting='count', as_strings=True))\n",
    "    frequent_words = [ x[0] for x in get_most_frequent_words(corpus, 100) ]\n",
    "\n",
    "    group_by_options = { TREATY_TIME_GROUPINGS[k]['title']: k for k in TREATY_TIME_GROUPINGS }\n",
    "    output_type_options = [\n",
    "        ( 'List', 'table' ),\n",
    "        ( 'Pivot', 'pivot' ),\n",
    "        ( 'Excel', 'excel' ),\n",
    "    ]\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('200px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('200px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=lw('200px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=lw('200px')),\n",
    "        target=widgets.Dropdown(description='Normalize', options={ '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }, value='lemma', layout=lw('200px')),\n",
    "        weighting=widgets.Dropdown(description='Weighting', options={ 'Count': 'count', 'Frequency': 'freq' }, value='freq', layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(['NOUN']), rows=7, layout=widgets.Layout(width='150px')),\n",
    "        stop_words=widgets.SelectMultiple(description='STOP', options=frequent_words, value=list([]), rows=7, layout=lw('200px')),\n",
    "        group_by_column=widgets.Dropdown(description='Group by', value='signed_year', options=group_by_options, layout=lw('200px')),\n",
    "        output_type=widgets.Dropdown(description='Output', value='pivot', options=output_type_options, layout=widgets.Layout(width='180px')),\n",
    "        n_tokens=widgets.IntSlider(description='#tokens', value=25, min=3, max=500, layout=lw('250px')),\n",
    "        compute=widgets.Button(description='Compute', layout=lw('220px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.target,\n",
    "                gui.ngrams,\n",
    "                gui.weighting,\n",
    "                gui.group_by_column,\n",
    "                gui.output_type,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.parties,\n",
    "                gui.party_preset,\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            gui.stop_words,\n",
    "            widgets.VBox([\n",
    "                gui.n_tokens,\n",
    "                gui.compute\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "    \n",
    "    def compute_callback(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df_freqs = display_list_of_most_frequent_words(\n",
    "                data_folder=data_folder,\n",
    "                wti_index=wti_index,\n",
    "                container=container,\n",
    "                gui=gui,\n",
    "                target=gui.target.value,\n",
    "                group_by_column=gui.group_by_column.value,\n",
    "                parties=gui.parties.value,\n",
    "                weighting=gui.weighting.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "                stop_words=set(gui.stop_words.value)\n",
    "            )\n",
    "            if gui.output_type.value == 'table':\n",
    "                display(df_freqs)\n",
    "            elif gui.output_type.value == 'pivot':\n",
    "                group_by_column = gui.group_by_column.value\n",
    "                df_freqs = df_freqs[df_freqs.position <= gui.n_tokens.value]\n",
    "                df_unstacked_freqs = df_freqs[[group_by_column, 'position', 'term']].set_index([group_by_column, 'position']).unstack()\n",
    "                display(df_unstacked_freqs)\n",
    "            else:\n",
    "                filename = '../data/word_trend_data.xlsx'\n",
    "                df_freqs.to_excel(filename)\n",
    "                print('Excel written: ' + filename)\n",
    "\n",
    "\n",
    "    gui.compute.on_click(compute_callback)\n",
    "    \n",
    "try:\n",
    "    list_of_most_frequent_words_gui(DATA_FOLDER, WTI_INDEX, get_current_corpus())\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display Named Entities\n",
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus, wti_index):\n",
    "    \n",
    "    def display_document_entities(corpus, treaty_id):\n",
    "        \n",
    "        doc = textacy_utility.get_treaty_doc(corpus, treaty_id)\n",
    "        \n",
    "        displacy.render(doc.spacy_doc, style='ent', jupyter=True)\n",
    "\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "            \n",
    "    treaty_ids = widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_document_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        treaty_id=treaty_ids\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        treaty_ids,\n",
    "        widgets.VBox([itw.children[-1]], layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    corpus = get_current_corpus().textacy_corpus\n",
    "    display_document_entities_gui(corpus, WTI_INDEX)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Compute or Load a Topic Model<span style='color: red; float: right'>MANDATORY RUN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>MODEL</span> Compute a new Topic Model<span style='color: red; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde32405f41b4412b2960216a4258176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(IntSlide…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import topic_model_gui\n",
    "\n",
    "try:\n",
    "    TM_GUI_MODEL = topic_model_gui.display_topic_model_gui(get_current_corpus().textacy_corpus, DF_TAGSET)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>MODEL</span> Store or Load a Topic Model<span style='color: red; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf696706d61641f7961e636f8f1e6d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Path', layout=Layout(width='40%'), options=('../data/topic…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import topic_model\n",
    "import topic_model_utility\n",
    "\n",
    "def get_persisted_model_paths():\n",
    "    return sorted([ x for x in glob.glob(os.path.join(DATA_FOLDER, '*.pickle')) ])\n",
    "\n",
    "def load_model(gui, model_container, *args):\n",
    "    \n",
    "    gui.output.clear_output()\n",
    "    \n",
    "    with gui.output:\n",
    "       \n",
    "        if gui.stored_path.value is None:\n",
    "            print(\"Please specify which model to load.\")\n",
    "            return\n",
    "\n",
    "        model_container.model = topic_model.load_model(gui.stored_path.value)\n",
    "    \n",
    "        topics = topic_model_utility.get_lda_topics(model_container.model.tm_model, n_tokens=20)\n",
    "        \n",
    "        display(topics)\n",
    "        \n",
    "        print('Model was loaded!')\n",
    "        \n",
    "def store_model(gui, model_container, *args):\n",
    "    \n",
    "    gui.output.clear_output()\n",
    "    \n",
    "    with gui.output:\n",
    "        if gui.identifier.value == '':\n",
    "            print(\"Please specify a unique identifier for the model.\")\n",
    "            return\n",
    "\n",
    "        if gui.identifier.value != utility.filename_whitelist(gui.identifier.value):\n",
    "            print(\"Please use ONLY valid filename characters in identifier.\")\n",
    "            return\n",
    "\n",
    "        filename = os.path.join(DATA_FOLDER, 'topic_model.pickle')\n",
    "        filename = utility.path_add_date(filename)\n",
    "        filename = utility.path_add_suffix(filename, gui.identifier.value)\n",
    "\n",
    "        topic_model.store_model(model_container.model, filename)\n",
    "\n",
    "        gui.stored_path.value = None\n",
    "        gui.stored_path.options = get_persisted_model_paths()\n",
    "        \n",
    "        print('Model stored in file {}'.format(filename))\n",
    "    \n",
    "def display_persist_topic_model_gui(model_container):\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        stored_path=widgets.Dropdown(description='Path', options=get_persisted_model_paths(), layout=widgets.Layout(width='40%')),\n",
    "        load=widgets.Button(description='Load', layout=widgets.Layout(width='80px')),\n",
    "        store=widgets.Button(description='Store', layout=widgets.Layout(width='80px')),\n",
    "        identifier=widgets.Text(description='Identifier', layout=widgets.Layout(width='300px')),\n",
    "        output=widgets.Output()\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        widgets.HBox([gui.stored_path, gui.load, gui.store, gui.identifier ]),\n",
    "        widgets.HBox([\n",
    "            widgets.Label(value=\"\", layout=widgets.Layout(width='40%')),\n",
    "            widgets.Label(value=\"Stored models will be named ./data/topic_model_yyyymmdd_$identifier$.pickle\"),\n",
    "        ]),\n",
    "        widgets.VBox([gui.output])\n",
    "    ])\n",
    "    \n",
    "    fx = lambda *args: load_model(gui, model_container, *args)\n",
    "    gui.load.on_click(fx)\n",
    "\n",
    "    fy = lambda *args: store_model(gui, model_container, *args)\n",
    "    gui.store.on_click(fy)\n",
    "    \n",
    "    display(boxes)\n",
    "\n",
    "if 'TM_GUI_MODEL' not in globals():\n",
    "    TM_GUI_MODEL = types.SimpleNamespace(\n",
    "        model=None\n",
    "    )\n",
    "    \n",
    "display_persist_topic_model_gui(TM_GUI_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Wordcloud<span style='color: red; float: right'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c634fe771d6545e69bbf65de4a504a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='tx02'></span>\", placeholder=''), HBox(children=(Button(description='<<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_wordcloud_gui(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    model = tm_data.tm_model\n",
    "    output_options = output_options or []\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(\n",
    "    tm_data,\n",
    "    topic_id=0,\n",
    "    n_words=100,\n",
    "    output_format='Wordcloud',\n",
    "    widget_container=None\n",
    "):\n",
    "    container = tm_data.compiled_data\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = container.topic_token_weights.loc[(container.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = topic_model_utility.get_topic_title(container.topic_token_weights, topic_id, n_words=n_words)\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = topic_model_utility.get_topic_tokens(container.topic_token_weights, topic_id=topic_id, n_words=n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    display_wordcloud_gui(display_wordcloud, tm_data, 'tx02', ['Wordcloud', 'Table'])\n",
    "except TopicModelNotComputed as ex:\n",
    "    logger.info(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Chart<span style='color: red; float: right'>TRY IT</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ad20b5e34041a0a9b798308e334af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='wc01'></span>\", placeholder=''), HBox(children=(Button(description='<<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display topic's word distribution\n",
    "import numpy as np\n",
    "\n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(tokens)\n",
    "\n",
    "    p = bokeh.plotting.figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = bokeh.models.ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bokeh.models.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def display_topic_tokens(tm_data, topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = topic_model_utility.get_topic_tokens(container.topic_token_weights, topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        p = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        bokeh.plotting.show(p)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "        \n",
    "    # Added code for missing method: widget_container.reset()\n",
    "    if 'progress' in widget_container.__dict__.keys():\n",
    "        widget_container.progress.value = 0\n",
    "    \n",
    "    \n",
    "def display_topic_distribution_widgets(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0),\n",
    "        word_count=widgets.IntSlider(description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2]),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "TM_DATA = TM_GUI_MODEL.model\n",
    "\n",
    "display_topic_distribution_widgets(display_topic_tokens, TM_DATA, 'wc01', ['Chart', 'Table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Trend Over Time or Documents<span style='color: red; float: right'>RUN</span>\n",
    "- Displays topic's share over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671762e580c54fc49a597e8ef70c96f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='topic_share_plot'></span>\", placeholder=''), HBox(children=(Button(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "import math\n",
    "\n",
    "def plot_topic_trend(df, category_column, value_column, x_label=None, y_label=None, **figopts):\n",
    "    \n",
    "    xs = df[category_column].astype(np.str)\n",
    "    ys = df[value_column]\n",
    "    \n",
    "    figopts = utility.extend(dict(title='', toolbar_location=\"right\"), figopts)\n",
    "    \n",
    "    p = bokeh.plotting.figure(**figopts)\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=ys, width=0.5, fill_color=\"#b3de69\")\n",
    "    \n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or category_column.title().replace('_', ' ')).title()\n",
    "    p.yaxis[0].axis_label = (y_label or value_column.title().replace('_', ' ')).title()\n",
    "    p.y_range.start = 0.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    \n",
    "    return p\n",
    "\n",
    "def display_topic_trend(\n",
    "    topic_id,\n",
    "    year,\n",
    "    year_aggregate,\n",
    "    gui,\n",
    "    output_format='Chart',\n",
    "    document_topic_weights=None,\n",
    "    topic_token_weights=None,\n",
    "    threshold=0.01\n",
    "):\n",
    "    figopts = dict(plot_width=1000, plot_height=700, title='', toolbar_location=\"right\")\n",
    "\n",
    "    tokens = topic_model_utility.get_topic_title(topic_token_weights, topic_id, n_words=200)\n",
    "    gui.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    \n",
    "    pivot_column = 'signed_year' if year is None else None\n",
    "    value_column = year_aggregate if year is None else 'weight'\n",
    "\n",
    "    df = document_topic_weights[(document_topic_weights.topic_id == topic_id)]\n",
    "    \n",
    "    if year is not None:\n",
    "        df = df[(df.signed_year == year)]\n",
    "        \n",
    "    df = df[(df.weight > threshold)].reset_index()\n",
    "    \n",
    "    if pivot_column is not None:\n",
    "        df = df.groupby([pivot_column, 'topic_id']).agg([np.mean, np.max])['weight'].reset_index()\n",
    "        df.columns = [pivot_column, 'topic_id', 'mean', 'max']\n",
    "        category_column = pivot_column\n",
    "        min_year = document_topic_weights.signed_year.min()\n",
    "        max_year = document_topic_weights.signed_year.max()\n",
    "        figopts['x_range'] = list(map(str, range(min_year, max_year+1))) # utility.complete_value_range(df[category_column].unique(), str)\n",
    "    else:\n",
    "        df['treaty'] = df.treaty_id + ' ' + df.party1 + ' ' + df.party2\n",
    "        category_column = 'treaty'\n",
    "        figopts['x_range'] = df['treaty'].unique()\n",
    "        \n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        p = plot_topic_trend(df, category_column, value_column, **figopts)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "def create_topic_trend_widgets(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    model_data = tm_data.compiled_data\n",
    "    document_topic_weights = tm_data.compiled_data.document_topic_weights\n",
    "    topic_token_weights = tm_data.compiled_data.topic_token_weights\n",
    "\n",
    "    year_options = [ ('all years', None) ] + [ (x,x) for x in range(model_data.year_period[0], model_data.year_period[1] + 1)]\n",
    "    \n",
    "    element_id = 'topic_share_plot'\n",
    "    gui = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=element_id,\n",
    "        text=widgets_config.text(dom_id=element_id),\n",
    "        year=widgets.Dropdown(description='Year', options=year_options, value=None),\n",
    "        year_aggregate=widgets.Dropdown(description='Aggregate', options=['mean', 'max'], value='max'),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=widgets.Dropdown(description='Format', options=['Chart', 'Table'], value='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "    )\n",
    "    \n",
    "    gui.prev_topic_id = gui.create_prev_id_button('topic_id', model.num_topics)\n",
    "    gui.next_topic_id = gui.create_next_id_button('topic_id', model.num_topics)\n",
    "    \n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=gui.topic_id,\n",
    "        year=gui.year,\n",
    "        year_aggregate=gui.year_aggregate,\n",
    "        gui=widgets.fixed(gui),\n",
    "        output_format=gui.output_format,\n",
    "        document_topic_weights=widgets.fixed(model_data.document_topic_weights),\n",
    "        topic_token_weights=widgets.fixed(model_data.topic_token_weights),\n",
    "        threshold=gui.threshold\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.text,\n",
    "        widgets.HBox([gui.prev_topic_id, gui.next_topic_id, gui.year, gui.year_aggregate, gui.output_format]),\n",
    "        widgets.HBox([gui.topic_id, gui.threshold, gui.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "create_topic_trend_widgets(tm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic to Document Network<span style='color: red; float: right'>TRY IT</span>\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224e17f88eda4316ad725ae0f879f5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<span class='nx_id1'></span>\", placeholder=''), HBox(children=(VBox(children=(Dropd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "def plot_document_topic_network(network, layout, scale=1.0, titles=None):\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = bokeh.plotting.figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=tools)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(renderers=[r_topics], tooltips=None, callback=widgets_utility.wf.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(x='x', y='y', text='name', level='overlay', x_offset=0, y_offset=0, text_font_size='8pt')\n",
    "    \n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "        \n",
    "def display_document_topic_network(layout_algorithm, tm_data, threshold=0.10, parties=None, period=None, ignores=None, scale=1.0, output_format='network', tick=utility.noop):\n",
    "\n",
    "    tick(1)\n",
    "    \n",
    "    container = tm_data.compiled_data\n",
    "    \n",
    "    titles = topic_model_utility.get_topic_titles(container.topic_token_weights)\n",
    "\n",
    "    df = container.document_topic_weights[container.document_topic_weights.weight > threshold].reset_index()\n",
    "\n",
    "    if len(parties or []) > 0:\n",
    "        df = df[df.party1.isin(parties) | df.party2.isin(parties)]\n",
    "\n",
    "    if len(period or []) == 2:\n",
    "        df = df[(df.signed_year>=period[0]) & (df.signed_year<=period[1])]\n",
    "        \n",
    "    if len(ignores or []) > 0:\n",
    "        df = df[~df.topic_id.isin(ignores)]\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print('No data')\n",
    "        return\n",
    "    \n",
    "    df['title'] = df.treaty_id + ' ' + df.party1 + ' ' + df.party2\n",
    "\n",
    "    network = NetworkUtility.create_bipartite_network(df, 'title', 'topic_id')\n",
    "    tick()\n",
    "\n",
    "    if output_format == 'network':\n",
    "        args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "        layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "        tick()\n",
    "        p = plot_document_topic_network(network, layout, scale=scale, titles=titles)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "    elif output_format == 'table':\n",
    "        display(df)\n",
    "\n",
    "    tick(0)\n",
    "        \n",
    "def document_topic_network_gui(wti_index, tm_data):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    text_id = 'nx_id1'\n",
    "    layout_options = [ 'Circular', 'Kamada-Kawai', 'Fruchterman-Reingold']\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    year_min, year_max = tm_data.compiled_data.year_period\n",
    "    \n",
    "    n_topics = tm_data.tm_model.num_topics\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        text=widgets_utility.wf.create_text_widget(text_id),\n",
    "        period=widgets.IntRangeSlider(description='Time', min=year_min, max=year_max, step=1, value=(year_min, year_max), continues_update=False),\n",
    "        scale=widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False),\n",
    "        output_format=widgets_utility.dropdown('Output', { 'Network': 'network', 'Table': 'table' }, 'network', layout=lw('200px')),\n",
    "        layout=widgets_utility.dropdown('Layout', layout_options, 'Fruchterman-Reingold', layout=lw('250px')),\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('180px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('180px')),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"99%\")),\n",
    "        ignores=widgets.SelectMultiple(description='Ignore', options=[('', None)] + [ ('Topic #'+str(i), i) for i in range(0, n_topics) ], value=[], rows=8, layout=lw('180px')),\n",
    "\n",
    "    )\n",
    "    \n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "        \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "    \n",
    "    iw = widgets.interactive(\n",
    "        display_document_topic_network,\n",
    "        layout_algorithm=gui.layout,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=gui.threshold,\n",
    "        parties=gui.parties,\n",
    "        period=gui.period,\n",
    "        ignores=gui.ignores,\n",
    "        scale=gui.scale,\n",
    "        output_format=gui.output_format,\n",
    "        tick=widgets.fixed(tick)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.text,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([gui.layout, gui.threshold, gui.scale, gui.period]), \n",
    "            widgets.VBox([gui.parties, gui.party_preset]), \n",
    "            widgets.VBox([gui.ignores]), \n",
    "            widgets.VBox([gui.output_format, gui.progress]),\n",
    "        ]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    document_topic_network_gui(WTI_INDEX, tm_data)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Topic Trends Overview<span style='color: red; float: right'>TRY IT</span>\n",
    "\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanford’s Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d984542701994ed79572899573214d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Year', layout=Layout(width='160px'), options=(('all years'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "import bokeh.transform\n",
    "\n",
    "def isint(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def get_topic_weight_by_year_or_document(document_topic_weights, key='mean', year=None):\n",
    "    pivot_column = 'year' if year is None else 'document_id'\n",
    "    #if df[(df.year == year)]\n",
    "    df = self.get_document_topic_weights(year) \\\n",
    "        .groupby([pivot_column,'topic_id']) \\\n",
    "        .agg(config.AGGREGATES[key])[['weight']].reset_index()\n",
    "    return df, pivot_column\n",
    "    \n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = ['#ffffff', '#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b']\n",
    "    mapper = bokeh.models.LinearColorMapper(palette=colors, low=0.0, high=1.0) # low=df.weight.min(), high=max_weight)\n",
    "    color_transform = bokeh.transform.transform('weight', mapper)\n",
    "    color_bar = bokeh.models.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bokeh.models.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bokeh.models.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def compute_int_range_categories(values):\n",
    "    categories = values.unique()\n",
    "    if all(map(utility.isint, categories)):\n",
    "        categories = sorted(list(map(int, categories)))\n",
    "        return list(map(str, categories))\n",
    "    else:\n",
    "        return sorted(list(categories))\n",
    "\n",
    "HEATMAP_FIGOPTS = dict(title=\"Topic heatmap\", toolbar_location=\"right\",  x_axis_location=\"above\", plot_width=1000)\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, titles, text_id, **figopts):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "\n",
    "    x_range = compute_int_range_categories(df[xs])\n",
    "    y_range = compute_int_range_categories(df[ys])\n",
    "    \n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = bokeh.models.ColumnDataSource(df)\n",
    "\n",
    "    if x_range is not None:\n",
    "        figopts['x_range'] = x_range\n",
    "\n",
    "    if y_range is not None:\n",
    "        figopts['y_range'] = y_range\n",
    "        figopts['plot_height'] = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = bokeh.plotting.figure(**figopts)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"8pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(tooltips=None, callback=widgets_utility.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def display_doc_topic_heatmap(model_data, key='max', flip_axis=False, glyph='Circle', year=None, year_aggregate=None, output_format=None):\n",
    "    try:\n",
    "\n",
    "        titles = topic_model_utility.get_topic_titles(model_data.topic_token_weights, n_words=100)\n",
    "        \n",
    "        df = model_data.document_topic_weights.copy()\n",
    "\n",
    "        if year is not None:\n",
    "            df = df[(df.signed_year == year)]\n",
    "\n",
    "        if year is None:\n",
    "            \n",
    "            ''' Display aggregate value grouped by year  '''\n",
    "            df = df.groupby(['signed_year', 'topic_id']).agg([np.mean, np.max])['weight'].reset_index()\n",
    "            df.columns = ['signed_year', 'topic_id', 'mean', 'max']\n",
    "            df['weight'] = df[year_aggregate]\n",
    "            df['signed_year'] = df.signed_year.astype(str)\n",
    "            category_column = 'signed_year'\n",
    "            \n",
    "        else:\n",
    "            ''' Display individual treaties for selected year  '''\n",
    "            df['treaty'] = df.treaty_id + ' ' + df.party1 + ' ' + df.party2\n",
    "            df = df[['treaty', 'treaty_id', 'topic_id', 'weight']]\n",
    "            category_column = 'treaty'  \n",
    "        \n",
    "        df['document_id'] = df.index.astype(str)\n",
    "        df['topic_id'] = df.topic_id.astype(str)\n",
    "         \n",
    "        if output_format.lower() == 'heatmap':\n",
    "            \n",
    "            p = plot_topic_relevance_by_year(\n",
    "                df,\n",
    "                xs=category_column,\n",
    "                ys='topic_id',\n",
    "                flip_axis=flip_axis,\n",
    "                titles=titles,\n",
    "                text_id='topic_relevance',\n",
    "                **HEATMAP_FIGOPTS)\n",
    "\n",
    "            bokeh.plotting.show(p)\n",
    "            \n",
    "        else:\n",
    "            display(df)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        logger.error(ex)\n",
    "        \n",
    "def doc_topic_heatmap_gui(model_data):\n",
    "\n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    text_id = 'topic_relevance'\n",
    "    \n",
    "    def text_widget(element_id=None, default_value='', style='', line_height='20px'):\n",
    "        value = \"<span class='{}' style='line-height: {};{}'>{}</span>\".format(element_id, line_height, style, default_value) if element_id is not None else ''\n",
    "        return widgets.HTML(value=value, placeholder='', description='', layout=widgets.Layout(height='150px'))\n",
    "    \n",
    "    year_options = [ ('all years', None) ] + [ (x,x) for x in range(model_data.year_period[0], model_data.year_period[1] + 1)]\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        text_id=text_id,\n",
    "        text=text_widget(text_id),\n",
    "        flip_axis=widgets.ToggleButton(value=True, description='Flip', icon='', layout=lw(\"80px\")),\n",
    "        year=widgets.Dropdown(description='Year', options=year_options, value=None, layout=lw(\"160px\")),\n",
    "        year_aggregate=widgets.Dropdown(description='Aggregate', options=['mean', 'max'], value='max', layout=lw(\"160px\")),\n",
    "        output_format=widgets.Dropdown(description='Output', options=['Heatmap', 'Table'], value='Heatmap', layout=lw(\"180px\"))\n",
    "    )\n",
    "    \n",
    "    iw = widgets.interactive(\n",
    "        display_doc_topic_heatmap,\n",
    "        model_data=widgets.fixed(model_data),\n",
    "        flip_axis=gui.flip_axis,\n",
    "        year=gui.year,\n",
    "        year_aggregate=gui.year_aggregate,\n",
    "        output_format=gui.output_format\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.year, gui.year_aggregate, gui.output_format, gui.flip_axis ]),\n",
    "        widgets.HBox([iw.children[-1]]), gui.text\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "try:\n",
    "    doc_topic_heatmap_gui(get_current_model().compiled_data)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
