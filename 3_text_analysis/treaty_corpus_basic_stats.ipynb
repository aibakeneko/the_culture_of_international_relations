{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Culture of International Relations - Corpus statistics\n",
    "\n",
    "#### About this notebook\n",
    "spaCy tutorial: https://github.com/NirantK/nlp-python-deep-learning/blob/master/Part-03%20NLP%20with%20spaCy%20and%20Textacy.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 11:13:19,538 : INFO : Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import typing.re\n",
    "import collections\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import zipfile\n",
    "import common.utility as utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import common.treaty_utility as treaty_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import treaty_corpus\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "DATA_FOLDER = '../data'\n",
    "CORPUS_PATH = os.path.join(DATA_FOLDER, 'treaty_text_corpora_20181115.zip')\n",
    "PATTERN = '*.txt'\n",
    "LANGUAGE = 'en'\n",
    "LANGUAGE_MAP = { 'en': 'english', 'fr': 'french', 'it': 'other', 'de': 'other' }\n",
    "LANGUAGE_MODEL_MAP = { 'en': 'en_core_web_sm', 'fr': 'fr_core_web_sm', 'it': 'it_core_web_sm', 'de': 'de_core_web_sm' }\n",
    "\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "\n",
    "# sudo python3 -m spacy download en_core_web_lg\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def get_filenames(zip_filename, extension='.txt'):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return [ x for x in zf.namelist() if x.endswith(extension) ]\n",
    "    \n",
    "def get_text(zip_filename, filename):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return zf.read(filename).decode(encoding='utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treaty data utilities\n",
    "def get_treaties(wti_index, language, period_group='years_1945-1972', treaty_filter='is_cultural', parties=None):\n",
    "    period_group = config.PERIOD_GROUPS_ID_MAP[period_group]\n",
    "    treaties = wti_index.get_treaties_within_division(\n",
    "        period_group=period_group,\n",
    "        treaty_filter=treaty_filter,\n",
    "        recode_is_cultural=False,\n",
    "        parties=parties\n",
    "    )\n",
    "    treaties = treaties[treaties[LANGUAGE_MAP[language]]==language]\n",
    "    return treaties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 13:34:34,171 : INFO : Loading model: english...\n",
      "2018-11-15 13:34:34,172 : INFO : Using pipeline: tagger parser ner\n",
      "2018-11-15 13:34:34,173 : INFO : Working: Computing new corpus ../data/corpus_en__disable().pkl...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_textacy_corpus() missing 1 required positional argument: 'period_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4ed575e6eaa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCORPUS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPREPPED_CORPUS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mTEXTACY_CORPUS_FILENAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCORPUS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_textacy_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREPPED_CORPUS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLANGUAGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_named_entities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-4ed575e6eaa4>\u001b[0m in \u001b[0;36mgenerate_textacy_corpus\u001b[0;34m(source_filename, language, corpus_args, preprocess_args, merge_named_entities, force)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextacy_corpus_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Working: Computing new corpus '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtextacy_corpus_filename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_textacy_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextacy_corpus_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_textacy_corpus() missing 1 required positional argument: 'period_group'"
     ]
    }
   ],
   "source": [
    "# %writefile: spacy_utility\n",
    "from spacy.language import Language\n",
    "from textacy.spacier.utils import merge_spans\n",
    "\n",
    "HYPHEN_REGEXP = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "\n",
    "def preprocess_text(source_filename, target_filename):\n",
    "    filenames = get_filenames(source_filename)\n",
    "    texts = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    with zipfile.ZipFile(target_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for filename, text in texts:\n",
    "            logger.info('Processing ' + filename)\n",
    "            text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "            text = textacy.preprocess.normalize_whitespace(text)   \n",
    "            text = textacy.preprocess.fix_bad_unicode(text)   \n",
    "            text = textacy.preprocess.replace_currency_symbols(text)\n",
    "            text = textacy.preprocess.unpack_contractions(text)\n",
    "            #text = textacy.preprocess.replace_urls(text)\n",
    "            #text = textacy.preprocess.replace_emails(text)\n",
    "            #text = textacy.preprocess.replace_phone_numbers(text)\n",
    "            text = textacy.preprocess.remove_accents(text)\n",
    "            zf.writestr(filename, text)\n",
    "\n",
    "def get_document_stream(corpus_path, lang, wti_index, period_group, treaty_filter='is_cultural', parties=None)\n",
    "\n",
    "    treaties = get_treaties(wti_index, language=lang, period_group=period_group, treaty_filter=treaty_filter, parties=parties)\n",
    "    treaties['treaty_id'] = treaties.index\n",
    "    treaty_ids = list(treaties.index)\n",
    "    \n",
    "    documents = treaty_corpus.TreatyCompressedFileReader(corpus_path, lang, treaty_ids)\n",
    "\n",
    "    for treaty_id, language, filename, text in documents:\n",
    "        assert language == lang\n",
    "        metadata = treaties.loc[treaty_id]\n",
    "        yield filename, text, metadata\n",
    "        \n",
    "def create_textacy_corpus(documents, nlp, preprocess_args=None):\n",
    "    corpus = textacy.Corpus(nlp)\n",
    "    for filename, text, metadata in documents:\n",
    "        if not preprocess_args is None:\n",
    "            text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "            text = textacy.preprocess.preprocess_text(text, **preprocess_args)\n",
    "        corpus.add_text(text, utility.extend(dict(filename=filename), metadata))\n",
    "    return corpus\n",
    "\n",
    "def get_textacy_corpus_filename(source_path, language, nlp_args=None, preprocess_args=None):\n",
    "    nlp_args = nlp_args or {}\n",
    "    preprocess_args = preprocess_args or {}\n",
    "    disabled_pipes = nlp_args.get('disable', [])\n",
    "    suffix = '_{}_{}{}'.format(\n",
    "        language,\n",
    "        '_'.join([ k for k in preprocess_args if preprocess_args[k] ]),\n",
    "        '_disable({})'.format(','.join(disabled_pipes)) if len(disabled_pipes) > 0 else ''\n",
    "    )\n",
    "    return path_add_suffix(source_path, suffix, new_extension='.pkl')\n",
    "\n",
    "def setup_nlp_language_model(language, **nlp_args):\n",
    "    \n",
    "    def remove_whitespace_entities(doc):\n",
    "        doc.ents = [ e for e in doc.ents if not e.text.isspace() ]\n",
    "        return doc\n",
    "\n",
    "    logger.info('Loading model: %s...', language)\n",
    "    \n",
    "    Language.factories['remove_whitespace_entities'] = lambda nlp, **cfg: remove_whitespace_entities\n",
    "    \n",
    "    nlp = textacy.load_spacy(LANGUAGE_MODEL_MAP[language], **nlp_args)\n",
    "    pipeline = lambda: [ x[0] for x in nlp.pipeline ]\n",
    "    \n",
    "    logger.info('Using pipeline: ' + ' '.join(pipeline()))\n",
    "    \n",
    "    return nlp\n",
    "\n",
    "def generate_textacy_corpus(source_path, language, options):\n",
    "    \n",
    "    nlp = setup_nlp_language_model(language, options.get('nlp_args', {}))\n",
    "    \n",
    "    if force or not os.path.isfile(textacy_corpus_filename):\n",
    "        logger.info('Working: Computing new corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = create_textacy_corpus(document_stream, nlp, preprocess_args)\n",
    "        corpus.save(textacy_corpus_filename)\n",
    "    else:\n",
    "        logger.info('Working: Loading corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = textacy.Corpus.load(textacy_corpus_filename)\n",
    "        \n",
    "    if kwargs.get('merge_named_entities', False):\n",
    "        logger.info('Working: Merging named entities...')\n",
    "        for doc in corpus:\n",
    "            named_entities = textacy.extract.named_entities(doc)\n",
    "            merge_spans(named_entities, doc.spacy_doc)\n",
    "    else:\n",
    "        logger.info('Note: named entities not merged')\n",
    "        \n",
    "    logger.info('Done!')\n",
    "    \n",
    "    return textacy_corpus_filename, corpus\n",
    "\n",
    "def propagate_document_attributes(corpus):\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['treaty_id']\n",
    "        doc.spacy_doc.user_data['treaty_id'] = doc.metadata['treaty_id']\n",
    "    \n",
    "def get_corpus_documents(corpus):\n",
    "    df = pd.DataFrame([\n",
    "        (document_id, doc.metadata['treaty_id'], doc.metadata['filename'])\n",
    "                for document_id, doc in enumerate(corpus) ], columns=['treaty_id', 'title', 'filename']\n",
    "    ).set_index('treaty_id')\n",
    "    return df\n",
    "\n",
    "CORPUS_OPTIONS = {\n",
    "    'language': LANGUAGE,\n",
    "    PREPPED_CORPUS_PATH = utility.path_add_suffix(CORPUS_PATH, '_preprocessed')\n",
    "}\n",
    "\n",
    "PREPPED_CORPUS_PATH = utility.path_add_suffix(CORPUS_PATH, '_preprocessed')\n",
    "if not os.path.isfile(PREPPED_CORPUS_PATH):\n",
    "    preprocess_text(CORPUS_PATH, PREPPED_CORPUS_PATH)\n",
    "\n",
    "def corpus_stream():\n",
    "    return get_document_stream(PREPPED_CORPUS_PATH, LANGUAGE, WTI_INDEX, PERIOD_GROUP, treaty_filter='is_cultural', parties=None)\n",
    "\n",
    "target_name = get_textacy_corpus_filename(source_path, language, corpus_args, preprocess_args):\n",
    "\n",
    "TEXTACY_CORPUS_FILENAME, CORPUS = generate_textacy_corpus(\n",
    "    PREPPED_CORPUS_PATH,\n",
    "    LANGUAGE,\n",
    "    preprocess_args=dict(),\n",
    "    merge_named_entities=True,\n",
    "    force=False,\n",
    "    target_folder=DATA_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %writefile: spacy_utility\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "def doc_to_dataframe(spacy_doc, ignore_tags=None, ignore_pos=None):\n",
    "    ignore_tags = ignore_tags or ['_SP']\n",
    "    ignore_pos = ignore_pos or []\n",
    "    df_source = ({\n",
    "        \"text\": w.text.lower(),\n",
    "        \"lemma\": w.lemma_.lower(),\n",
    "        \"pos\": w.pos_,\n",
    "        \"tag\": w.tag_,\n",
    "        \"dep\": w.dep_,\n",
    "        \"is_alpha\": w.is_alpha,\n",
    "        \"is_stop\": w.is_stop\n",
    "    } for w in spacy_doc\n",
    "        if w.tag_ not in ignore_tags\n",
    "        and w.pos_ not in ignore_pos)\n",
    "    return pd.DataFrame(df_source)\n",
    "\n",
    "def document_statistics(spacy_doc):\n",
    "    \n",
    "    spacy_sentences = list(spacy_doc.sents)\n",
    "    token_length_histogram = collections.Counter((len(x) for x in spacy_doc))\n",
    "    \n",
    "    return {\n",
    "        'word_count': len(spacy_doc),\n",
    "        'sentence_count': len(spacy_sentences),\n",
    "        'avg_words_per_sentence': len(spacy_doc) / len(spacy_sentences),\n",
    "        'token_lengths': dict(token_length_histogram)\n",
    "    }\n",
    "\n",
    "def examplary_pipeline_component(doc):\n",
    "    '''Logs length of  document\n",
    "    \n",
    "    '''\n",
    "    logger.info(\"Doc has %s tokens.\" % len(doc))\n",
    "    return doc\n",
    "\n",
    "class ExamplaryPipelineClass(object):\n",
    "    ''' A custom pipeline component can also be a class. A custom class compontent must be added to the \"Language.factories\" so spaCy know how to create them.\n",
    "    \n",
    "    from spacy.language import Language\n",
    "    Language.factories['entity_matcher'] = lambda nlp, **cfg: ExamplaryPipelineClass(nlp, **cfg)\n",
    "    \n",
    "    In this example the instance is initialised with the nlp object,\n",
    "    a terminology list and an entity label. Using the PhraseMatcher, it then matches the terms in the Doc and adds them to the existing entities.\n",
    "    '''\n",
    "    name = 'entity_matcher'\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp(text) for text in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        return doc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Store a TextaCy Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7b244a42724d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_textacy_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwti_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCORPUS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperiod_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreaty_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'is_cultural'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcorpus_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7b244a42724d>\u001b[0m in \u001b[0;36mcreate_textacy_corpus\u001b[0;34m(wti_index, corpus_path, language, period_group, treaty_filter, parties)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtext_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtreaty_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreatyCompressedFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreaty_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/textacy/corpus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, texts, docs, metadatas)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/textacy/corpus.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, n_threads, batch_size)\u001b[0m\n\u001b[1;32m    289\u001b[0m             texts, n_threads=n_threads, batch_size=batch_size)\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspacy_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m                 self._add_textacy_doc(\n\u001b[1;32m    293\u001b[0m                     Doc(spacy_doc, lang=self.spacy_lang, metadata=metadata))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/cytoolz/itertoolz.pyx\u001b[0m in \u001b[0;36mcytoolz.itertoolz.partition_all.__next__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/spacy/_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         Yf = self.ops.xp.dot(X,\n\u001b[0;32m--> 149\u001b[0;31m             self.W.reshape((self.nF*self.nO*self.nP, self.nI)).T)\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a textacy corpus\n",
    "\n",
    "def create_textacy_corpus(wti_index, corpus_path, language, period_group, treaty_filter='is_cultural', parties=None):\n",
    "    \n",
    "    treaties = get_treaties(wti_index, language=language, period_group=period_group, treaty_filter=treaty_filter, parties=parties)\n",
    "    treaties['treaty_id'] = treaties.index\n",
    "\n",
    "    treaty_ids = list(treaties.index)\n",
    "    metadata_stream = ( treaties.loc[treaty_id].to_dict() for treaty_id in treaty_ids )\n",
    "    text_stream = ( content for _, _, _, content in treaty_corpus.TreatyCompressedFileReader(corpus_path, language, treaty_ids))\n",
    "\n",
    "    corpus = textacy.Corpus(language, texts=text_stream, metadatas=metadata_stream)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "period_group = 'years_1945-1972'\n",
    "language = 'en'\n",
    "\n",
    "corpus = create_textacy_corpus(wti_index, CORPUS_PATH, language, period_group=period_group, treaty_filter='is_cultural', parties=None)\n",
    "\n",
    "corpus_tag = 'test1'\n",
    "corpus_name = os.path.join('./data', 'corpus_textacy_{}_{}_{}.pkl'.format(language, period_group, corpus_tag))\n",
    "corpus.save(corpus_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = create_textacy_corpus(wti_index, CORPUS_PATH, LANGUAGE, period_group='years_1945-1972', treaty_filter='is_cultural', parties=None)\n",
    "\n",
    "CORPUS_TAG = 'test1'\n",
    "corpus_name = os.path.join('./data', 'corpus_textacy_{}_{}_{}.pkl'.format(LANGUAGE, PERIOD_GROUP, CORPUS_TAG))\n",
    "corpus = textacy.Corpus.load(corpus_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "') of Czechoslovak professors of higher education, who will work in French universities, institutes and laboratories, in order to mitigate the effects of the interruption of their scientific and professional training owing to the closing of Czechoslovak higher educational establishments by the Hitlers regime.\\r\\nThe Czechoslovak Government and the French Government will take the necessary measures to ensure that the exchange of work and study posts in specialised institutes of the two States should be encouraged as far as possible by savants and professors on a reciprocal basis.\\r\\nThe Czechoslovak Government and the'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][400:500].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus1960 = textacy.Corpus(nlp, texts=[d.text for d in corpus.get(lambda x: bool(x.metadata['signed_year'] == 1960))])\n",
    "\n",
    "#corpus1960 = textacy.Corpus('en')\n",
    "#docs = corpus.get(lambda x: bool(x.metadata['signed_year'] == 1960))\n",
    "#for doc in docs:\n",
    "#    corpus1960.add_doc(doc)\n",
    "    \n",
    "for doc in corpus1960:\n",
    "    print([x.lemma_ for x in textacy.extract.words(doc, include_pos=['NOUN'], min_freq=2) ])\n",
    "    #print([x for x in textacy.extract.acronyms_and_definitions(doc) ])\n",
    "    #print(set([x.lemma_ for x,y,z in textacy.extract.subject_verb_object_triples(doc) ]))\n",
    "    #print([x for x in doc.spacy_doc.noun_chunks]) #  x for x in textacy.extract.noun_chunks(doc, drop_determiners=True, min_freq=1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].spacy_doc.count_by(attr_id=spacy.attrs.IS_CURRENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'is_stopword'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-610d1a65656a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeta_columns\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-610d1a65656a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeta_columns\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-610d1a65656a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stopword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeta_columns\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-610d1a65656a>\u001b[0m in \u001b[0;36mf\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stopword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'is_stopword'"
     ]
    }
   ],
   "source": [
    "#ts = textacy.TextStats(corpus[0])\n",
    "#ts.basic_counts\n",
    "\n",
    "import spacy\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc\n",
    "import numpy\n",
    "\n",
    "def remove_tokens_on_match(doc):\n",
    "    indexes = []\n",
    "    \n",
    "    for index, token in enumerate(doc):\n",
    "        if (token.pos_  in ('PUNCT', 'NUM', 'SYM')):\n",
    "            indexes.append(index)\n",
    "            \n",
    "    np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
    "    np_array = numpy.delete(np_array, indexes, axis = 0)\n",
    "    \n",
    "    doc2 = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in indexes])\n",
    "    doc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)\n",
    "    \n",
    "    return doc2\n",
    "\n",
    "meta_columns = [ 'treaty_id', 'is_cultural', 'signed_year', 'signed', 'topic' ]\n",
    "\n",
    "def f(d):\n",
    "    return not d.spacy_doc.is_stopword\n",
    "\n",
    "documents = ( d for d in corpus if f(d) )\n",
    "\n",
    "df = pd.DataFrame([ utility.extend({ k: x.metadata[k] for k in meta_columns }, textacy.TextStats(x).basic_counts) for x in documents ])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame Corpus from spaCy Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataframe_corpus(treaties, corpus_path, language, ticker=utility.noop):\n",
    "    \n",
    "    nlp = spacy.load(language)\n",
    "    treaty_ids = list(treaties.index)\n",
    "    text_stream = ( content for _, _, _, content in treaty_corpus.TreatyCompressedFileReader(corpus_path, language, treaty_ids))\n",
    "\n",
    "    df_corpus = None\n",
    "    try:\n",
    "        for (_, treaty_id, _, content) in stream:\n",
    "            spacy_doc = spacy_model(content)\n",
    "            df = spacy_doc_2_df(spacy_doc)\n",
    "            df['treaty_id'] = treaty_id\n",
    "            if df_corpus is None:\n",
    "                df_corpus = df\n",
    "            else:\n",
    "                df_corpus = df_corpus.append(df, ignore_index=True)\n",
    "            tick()\n",
    "    finally:\n",
    "        tick(0)\n",
    "    return df_corpus\n",
    "\n",
    "progress_widget = widgets.IntProgress(min=0,max=treaties.shape[0],step=1, layout=widgets.Layout(width='95%'))\n",
    "display(progress_widget)\n",
    "\n",
    "def ticker(w):\n",
    "    def tick(x=None):\n",
    "        w.value = w.value + 1 if x is None else x\n",
    "    return tick\n",
    "\n",
    "tick = ticker(progress_widget)\n",
    "df_corpus = create_dataframe_corpus(treaties, corpus_path=CORPUS_PATH, language=LANGUAGE, ticker=tick)\n",
    "df_corpus.to_csv('spaCy_PoS_corpus.csv', sep='\\t', encoding='utf-8') # , compression='zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus2 = pd.read_csv('spaCy_PoS_corpus.csv', index_col=0, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treaty_pos_tokens = df_corpus[~df_corpus.Stop&~df_corpus.POS.isin(['SPACE', 'PUNCT'])].groupby(['treaty_id', 'Lemma', 'POS']).size()\n",
    "df_treaty_pos_tokens = df_treaty_pos_tokens.reset_index().rename(columns={'Lemma': 'lemma', 'POS': 'pos', 0: 'count'})\n",
    "df_treaty_pos_tokens = df_treaty_pos_tokens.set_index(['treaty_id'])\n",
    "df_treaty_pos_tokens = df_treaty_pos_tokens.merge(pd.DataFrame(treaties['signed_year']), how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_treaty_pos_tokens.head()\n",
    "\n",
    "df_year_pos_tokens = df_treaty_pos_tokens.groupby(['signed_year', 'lemma', 'pos']).sum()\n",
    "df_year_pos_tokens = df_year_pos_tokens.reset_index()\n",
    "df_year_pos_tokens[df_year_pos_tokens.pos.isin(['NOUN'])].nlargest(50, 'count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cb783eb8982e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_token_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SPACE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PUNCT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lemma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "df_token_pos = df_corpus[~df_corpus.POS.isin(['SPACE', 'PUNCT'])].groupby(['Lemma', 'POS']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Corpus vs WTI Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in corpus, but not in WTI: 304127/en, 400039/de, 415293/fr, XXX042/de, XXX046/de\n",
      "Found in WTI, but not in corpus: XXX010/de\n"
     ]
    }
   ],
   "source": [
    "corpus_documents = corpus.documents.set_index(['treaty_id', 'language'])\n",
    "treaty_text_languages = wti_index.get_treaty_text_languages().set_index(['treaty_id', 'language'])\n",
    "\n",
    "treaties_in_corpus_not_in_wti = corpus_documents.index.difference(treaty_text_languages.index).get_values()\n",
    "treaties_in_wti_not_in_corpus = treaty_text_languages.index.difference(corpus_documents.index).get_values()\n",
    "\n",
    "print(  'Found in corpus, but not in WTI: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_corpus_not_in_wti ]))\n",
    "\n",
    "print(  'Found in WTI, but not in corpus: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_wti_not_in_corpus ]))\n",
    "\n",
    "#corpus_documents.loc[corpus_text_not_in_wti]\n",
    "#treaty_text_languages.loc[wti_not_in_corpus]\n",
    "\n",
    "#wti_not_in_corpus\n",
    "\n",
    "# Duplicates:\n",
    "#corpus_documents.index.get_duplicates()\n",
    "#treaty_text_languages.corpus_documents.index.get_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Basic Corpus Statistics\n",
    "See https://www.nltk.org/book/ch01.html\n",
    "\n",
    "* Size of treaties over time\n",
    "* Unique word, unique words per word class\n",
    "* Lexical diversity\n",
    "* Frequency distribution\n",
    "* Average word length, sentence length\n",
    "\n",
    "\n",
    "```python\n",
    " \t\n",
    ">>> len(texts) / count(docs)\n",
    "0.06230453042623537\n",
    ">>>\n",
    "\n",
    ">>> len(set(text3)) / len(text3)\n",
    "0.06230453042623537\n",
    ">>>\n",
    "\n",
    ">>> > def lexical_diversity(text): [1]\n",
    "...     return len(set(text)) / len(text) [2]\n",
    "...\n",
    ">>> def percentage(count, total): [3]\n",
    "...     return 100 * count / total\n",
    "\n",
    "# Most common words\n",
    "fdist1 = FreqDist(text1)\n",
    "fdist1.most_common(50)\n",
    "\n",
    "# Word length frequencies\n",
    ">>> fdist = FreqDist(len(w) for w in text1)  [2]\n",
    ">>> print(fdist)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82760878b374cc78a2756f91b6343bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Language:', index=1, layout=Layout(width='260px'), options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code \n",
    "\n",
    "corpus = None\n",
    "def display_token_toplist_interact(source_folder):\n",
    "    global corpus\n",
    "    progress_widget = None\n",
    "    \n",
    "    def display_token_toplist(source_folder, language, statistics='', remove_stopwords=False):\n",
    "        global corpus\n",
    "\n",
    "        try:\n",
    "\n",
    "            progress_widget.value = 1\n",
    "\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0]).load_mm_corpus()\n",
    "\n",
    "            progress_widget.value = 2\n",
    "            service = MmCorpusStatisticsService(corpus, dictionary=corpus.dictionary, language=language)\n",
    "\n",
    "            print(\"Corpus consists of {} documents, {} words in total and a vocabulary size of {} tokens.\"\\\n",
    "                      .format(len(corpus), corpus.dictionary.num_pos, len(corpus.dictionary)))\n",
    "\n",
    "            progress_widget.value = 3\n",
    "            if statistics == 'word_freqs':\n",
    "                display(service.compute_word_frequencies(remove_stopwords))\n",
    "            elif statistics == 'documents':\n",
    "                display(service.compute_document_stats())\n",
    "            elif statistics == 'word_count':\n",
    "                display(service.compute_word_stats())\n",
    "            else:\n",
    "                print('Unknown: ' + statistics)\n",
    "\n",
    "        except Exception as ex:\n",
    "            logger.error(ex)\n",
    "\n",
    "        progress_widget.value = 5\n",
    "        progress_widget.value = 0\n",
    "        return corpus\n",
    "    \n",
    "    language_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    statistics_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'Word freqs': 'word_freqs',\n",
    "            'Documents': 'documents',\n",
    "            'Word count': 'word_count'\n",
    "        },\n",
    "        value='word_count',\n",
    "        description='Statistics:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    remove_stopwords_widget=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist'\n",
    "    )\n",
    "    \n",
    "    progress_widget=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    "\n",
    "    wi = widgets.interactive(\n",
    "        display_token_toplist,\n",
    "        source_folder=source_folder,\n",
    "        language=language_widget,\n",
    "        statistics=statistics_widget,\n",
    "        remove_stopwords=remove_stopwords_widget\n",
    "    )\n",
    "\n",
    "    boxes = widgets.HBox(\n",
    "        [\n",
    "            language_widget, statistics_widget, remove_stopwords_widget, progress_widget\n",
    "        ]\n",
    "    )\n",
    "    display(widgets.VBox([boxes, wi.children[-1]]))\n",
    "    wi.update()\n",
    "\n",
    "display_token_toplist_interact('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_predicate = None\n",
    "filename_predicate = filename_predicate or (lambda x: True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Treaty Keyword Extraction (using TF-IDF weighing)\n",
    "- [ML Wiki.org](http://mlwiki.org/index.php/TF-IDF)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- Spärck Jones, K. (1972). \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\".\n",
    "- Manning, C.D.; Raghavan, P.; Schutze, H. (2008). \"Scoring, term weighting, and the vector space model\". ([PDF](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf))\n",
    "- https://markroxor.github.io/blog/tfidf-pivoted_norm/\n",
    "$\\frac{tf-idf}{\\sqrt(rowSums( tf-idf^2 ) )}$\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/pivoted-normalized-document-length-1.html\n",
    "\n",
    "Neural Network Methods in Natural Language Processing, Yoav Goldberg:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2bd4d39be54eb08a67fb69cc66730f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Language:', index=3, layout=Layout(width='260px'), options={'German': ('de', 'german'), 'French': ('fr', 'french'), 'Italian': ('it', 'italian'), 'English': ('en', 'english')}, value=('en', 'english')), Dropdown(description='Period:', index=3, layout=Layout(width='260px'), options={'': None, 'Alt. division': 'signed_period_alt', 'Year': 'signed_year', 'Default division': 'signed_period'}, value='signed_period'))), VBox(children=(IntSlider(value=5, continuous_update=False, description='Top #:', max=25, min=1), FloatSlider(value=0.001, continuous_update=False, description='Threshold:', max=0.5, readout_format='.3f', step=0.01))), VBox(children=(IntProgress(value=0, max=5), Dropdown(description='Output:', index=3, layout=Layout(width='260px'), options={'': None, 'Alt. division': 'signed_period_alt', 'Year': 'signed_year', 'Default division': 'signed_period'}, value='signed_period'))))), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "from scipy.sparse import csr_matrix\n",
    "%timeit\n",
    "\n",
    "    \n",
    "def get_top_tfidf_words(data, n_top=5):\n",
    "    top_list = data.groupby(['treaty_id'])\\\n",
    "        .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "        .reset_index(level=0, drop=True)\n",
    "    return top_list\n",
    "\n",
    "def compute_tfidf_scores(corpus, dictionary, smartirs='ntc'):\n",
    "    #model = gensim.models.logentropy_model.LogEntropyModel(corpus, normalize=True)\n",
    "    model = gensim.models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary, normalize=True) #, smartirs=smartirs)\n",
    "    rows, cols, scores = [], [], []\n",
    "    for r, document in enumerate(corpus): \n",
    "        vector = model[document]\n",
    "        c, v = zip(*vector)\n",
    "        rows += (len(c) * [ int(r) ])\n",
    "        cols += c\n",
    "        scores += v\n",
    "        \n",
    "    return csr_matrix((scores, (rows, cols)))\n",
    "    \n",
    "if True: #'tfidf_cache' not in globals():\n",
    "    tfidf_cache = {\n",
    "    }\n",
    "    \n",
    "def display_tfidf_scores(source_folder, language, period, n_top=5, threshold=0.001):\n",
    "    \n",
    "    global state, tfw, tfidf_cache\n",
    "    \n",
    "    try:\n",
    "        treaties = state.treaties\n",
    "\n",
    "        tfw.progress.value = 0\n",
    "        tfw.progress.value += 1\n",
    "        if language[0] not in tfidf_cache.keys():\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0])\\\n",
    "                .load_mm_corpus(normalize_by_D=True)\n",
    "            document_names = corpus.document_names\n",
    "            dictionary = corpus.dictionary\n",
    "            _ = dictionary[0]\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            A = compute_tfidf_scores(corpus, dictionary)\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            scores = pd.DataFrame(\n",
    "                [ (i, j, dictionary.id2token[j], A[i, j]) for i, j in zip(*A.nonzero())],\n",
    "                columns=['document_id', 'token_id', 'token', 'score']\n",
    "            )\n",
    "            tfw.progress.value += 1\n",
    "            scores = scores.merge(document_names, how='inner', left_on='document_id', right_index=True)\\\n",
    "                .drop(['document_id', 'token_id', 'document_name'], axis=1)\n",
    "\n",
    "            scores = scores[['treaty_id', 'token', 'score']]\\\n",
    "                .sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "            tfidf_cache[language[0]] = scores\n",
    "\n",
    "        scores = tfidf_cache[language[0]]\n",
    "        if threshold > 0:\n",
    "            scores = scores.loc[scores.score >= threshold]\n",
    "\n",
    "        tfw.progress.value += 1\n",
    "\n",
    "        #scores = get_top_tfidf_words(scores, n_top=5)\n",
    "        #scores = scores.groupby(['treaty_id']).sum() \n",
    "\n",
    "        scores = scores.groupby(['treaty_id'])\\\n",
    "            .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            .set_index('treaty_id')\n",
    "\n",
    "        if period is not None:\n",
    "            periods = state.treaties[period]\n",
    "            scores = scores.merge(periods.to_frame(), left_index=True, right_index=True, how='inner')\\\n",
    "                .groupby([period, 'token']).score.agg([np.mean])\\\n",
    "                .reset_index().rename(columns={0:'score'}) #.sort_values('token')\n",
    "\n",
    "        #['token'].apply(' '.join)\n",
    "\n",
    "        display(scores)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    tfw.progress.value = 0\n",
    "\n",
    "#if 'tfidf_scores' not in globals():\n",
    "#    tfidf_scores = compute_document_tfidf(corpus, corpus.dictionary, state.treaties)\n",
    "#    tfidf_scores = tfidf_scores.sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "tfw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    n_top=widgets.IntSlider(\n",
    "        value=5, min=1, max=25, step=1,\n",
    "        description='Top #:',\n",
    "        continuous_update=False\n",
    "    ),\n",
    "    threshold=widgets.FloatSlider(\n",
    "        value=0.001, min=0.0, max=0.5, step=0.01,\n",
    "        description='Threshold:',\n",
    "        tooltip='Word having a TF-IDF score below this value is filtered out',\n",
    "        continuous_update=False,\n",
    "        readout_format='.3f',\n",
    "    ), \n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Output:', **drop_style\n",
    "    ),\n",
    "    progress=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itfw = widgets.interactive(\n",
    "    display_tfidf_scores,\n",
    "    source_folder='./data',\n",
    "    language=tfw.language,\n",
    "    n_top=tfw.n_top,\n",
    "    threshold=tfw.threshold,\n",
    "    period=tfw.period\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([tfw.language, tfw.period]),\n",
    "        widgets.VBox([tfw.n_top, tfw.threshold]),\n",
    "        widgets.VBox([tfw.progress, tfw.output])\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(widgets.VBox([boxes, itfw.children[-1]]))\n",
    "itfw.update()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
