{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-19 11:26:14,291 : INFO : WTI index loaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import common.treaty_utility as treaty_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import treaty_corpus\n",
    "import types, glob\n",
    "import textacy.keyterms\n",
    "import qgrid\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import pickle\n",
    "import topic_model\n",
    "import topic_model_utility\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "#pd.options.display.max_colwidth = -1\n",
    "pd.options.display.colheader_justify = 'left'\n",
    "#pd.options.display.precision = 4\n",
    "\n",
    "DATA_FOLDER = '../data'\n",
    "PATTERN = '*.txt'\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "\n",
    "%matplotlib inline\n",
    "# set_matplotlib_formats('svg')   \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "def trunc_year_by(series, divisor):\n",
    "    return (series - series.mod(divisor)).astype(int) \n",
    "\n",
    "TREATY_TIME_GROUPINGS = {\n",
    "    'treaty_id': { 'column': 'treaty_id', 'divisor': None, 'title': 'Treaty', 'fx': None},\n",
    "    'signed_year': { 'column': 'signed_year', 'divisor': 1, 'title': 'Year', 'fx': None },\n",
    "    'signed_lustrum': { 'column': 'signed_lustrum', 'divisor': 5, 'title': 'Lustrum', 'fx': lambda df: trunc_year_by(df.signed_year, 5) },\n",
    "    'signed_decade': { 'column': 'signed_decade', 'divisor': 10, 'title': 'Decade', 'fx': lambda df: trunc_year_by(df.signed_year, 10) }\n",
    "}\n",
    "\n",
    "class TopicModelNotComputed(Exception):\n",
    "    @staticmethod\n",
    "    def check():\n",
    "        if 'TM_GUI_MODEL' in globals():\n",
    "            gui =  globals()['TM_GUI_MODEL']\n",
    "            if None not in (gui, gui.model):\n",
    "                return True\n",
    "        msg = 'A topic model must be computed using step \"MODEL Compute a Topic Model\"'\n",
    "        raise TopicModelNotComputed(msg)\n",
    "        \n",
    "class CorpusNotLoaded(Exception):\n",
    "    pass\n",
    "\n",
    "def get_current_model():\n",
    "    TopicModelNotComputed.check()\n",
    "    return globals()['TM_GUI_MODEL'].model\n",
    "\n",
    "def get_current_corpus():\n",
    "    if 'CURRENT_CORPUS' in globals():\n",
    "        if globals()['CURRENT_CORPUS'].textacy_corpus is not None:\n",
    "            return globals()['CURRENT_CORPUS']\n",
    "    raise CorpusNotLoaded('Corpus not loaded or computed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062cee5dd96e48758d7c65450f60d545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(Dropdown(description='C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "if 'CURRENT_CORPUS' not in globals():\n",
    "    CURRENT_CORPUS = types.SimpleNamespace(\n",
    "        language=None,\n",
    "        source_path=None,\n",
    "        prepped_source_path=None,\n",
    "        textacy_corpus_path=None,\n",
    "        textacy_corpus=None,\n",
    "        nlp=None\n",
    "    )\n",
    "\n",
    "try:\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, WTI_INDEX, CURRENT_CORPUS)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Find Key Terms <span style='float: right; color: green'>OPTIONAL</span>\n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n",
    "- [RAKE]\tRose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In M. W. Berry & J. Kogan (Eds.), Text Mining: Theory and Applications: John Wiley & Son\n",
    "https://github.com/csurfer/rake-nltk\n",
    "https://github.com/aneesha/RAKE\n",
    "https://github.com/vgrabovets/multi_rake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>RAKE <span style='float: right; color: green'>WORK IN PROGRESS</span>\n",
    "\n",
    "https://github.com/JRC1995/RAKE-Keyword-Extraction\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38dda5f6cba408ebf9605b85c1614f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='95%'), max=1), HBox(children=(Dropdown(description='T…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Document Key Terms\n",
    "from rake_nltk import Rake, Metric\n",
    "import string\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import gui_utility\n",
    "\n",
    "def textacy_rake(doc, language='english', normalize='lemma', n_keyterms=20, stopwords=None, metric=Metric.DEGREE_TO_FREQUENCY_RATIO):\n",
    "    punctuations = string.punctuation + \"\\\"\"\n",
    "    r = Rake(\n",
    "        stopwords=stopwords,  # NLTK stopwords if None\n",
    "        punctuations=punctuations, # NLTK by default\n",
    "        language=language,\n",
    "        ranking_metric=metric,\n",
    "        max_length=100000,\n",
    "        min_length=1\n",
    "    )\n",
    "    text = ' '.join([ x.lemma_ for x in doc.spacy_doc ] if normalize == 'lemma' else [ x.lower_ for x in doc.spacy_doc ])\n",
    "    r.extract_keywords_from_text(doc.text)\n",
    "    keyterms = [ (y, x) for (x, y) in r.get_ranked_phrases_with_scores() ]\n",
    "    return keyterms[:n_keyterms]\n",
    "\n",
    "\n",
    "def display_rake_gui(corpus, language):\n",
    "    \n",
    "    document_options = gui_utility.get_treaty_dropdown_options(WTI_INDEX, corpus)\n",
    "    metric_options = [\n",
    "        ('Degree / Frequency', Metric.DEGREE_TO_FREQUENCY_RATIO),\n",
    "        ('Degree', Metric.WORD_DEGREE),\n",
    "        ('Frequency', Metric.WORD_FREQUENCY)\n",
    "    ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=widgets.Layout(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=10, step=1, layout=widgets.Layout(width='340px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='40%')),\n",
    "        metric=widgets.Dropdown(description='Metric', options=metric_options, value=Metric.DEGREE_TO_FREQUENCY_RATIO, layout=widgets.Layout(width='300px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def compute_textacy_rake(corpus, treaty_id, language, normalize, n_keyterms, metric):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            doc = textacy_utility.get_treaty_doc(corpus, treaty_id)\n",
    "            phrases = textacy_rake(doc, language=language, normalize=normalize, n_keyterms=n_keyterms, stopwords=None, metric=metric)\n",
    "            df = pd.DataFrame(phrases, columns=['phrase', 'score'])\n",
    "            display(df.set_index('phrase'))\n",
    "            return df\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        compute_textacy_rake,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        treaty_id=gui.document_id,\n",
    "        language=widgets.fixed(language),\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "        metric=gui.metric\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.metric, gui.normalize]),\n",
    "        widgets.HBox([gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "try:\n",
    "    corpus = get_current_corpus().textacy_corpus\n",
    "    display_rake_gui(corpus, language='english')\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>TextRank/SingleRank <span style='float: right; color: green'>OPTIONAL</span>\n",
    "\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "\n",
    "def display_document_key_terms_gui(corpus, wti_index):\n",
    "    \n",
    "    methods = { 'RAKE': textacy_rake, 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=widgets.Layout(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=widgets.Layout(width='240px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'RAKE', 'TextRank', 'SingleRank' ], value='TextRank', layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def get_keyterms(method, doc, normalize, n_keyterms):\n",
    "        keyterms = methods[method](doc, normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ', '.join([ x for x, y in keyterms ])\n",
    "        gui.progress.value += 1\n",
    "        return terms\n",
    "    \n",
    "    def get_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        treaty_ids = [ document_id ] if document_id is not None else [ doc.metadata['treaty_id'] for doc in corpus ]\n",
    "        gui.progress.value = 0\n",
    "        gui.progress.max = len(treaty_ids)\n",
    "        keyterms = [\n",
    "            get_keyterms(method, textacy_utility.get_treaty_doc(corpus, treaty_id), normalize, n_keyterms) for treaty_id in treaty_ids\n",
    "        ]\n",
    "        df = pd.DataFrame({ 'treaty_id': treaty_ids, 'keyterms': keyterms}).set_index('treaty_id')\n",
    "        gui.progress.value = 0\n",
    "        return df\n",
    "\n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df = get_document_key_terms(corpus, method, document_id, normalize, n_keyterms)\n",
    "            #qgrid_widget = qgrid.show_grid(df, show_toolbar=False)\n",
    "            #display(qgrid_widget)\n",
    "            table = TableDisplay(df)\n",
    "            display(table)\n",
    "            \n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_key_terms_gui(CURRENT_CORPUS.textacy_corpus, WTI_INDEX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f667a1690b1145578e8257cb37021a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':'tight'}\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(8, 9 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='75')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_cleaned_up_text(container, gui, display_type, treaty_id): # ngrams, named_entities, normalize, include_pos):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    gui.output_text.clear_output()\n",
    "    gui.output_statistics.clear_output()\n",
    "    \n",
    "    doc = textacy_utility.get_treaty_doc(corpus, treaty_id)\n",
    "    \n",
    "    if doc is None:\n",
    "        return\n",
    "    \n",
    "    if display_type.startswith('source_text'):\n",
    "        \n",
    "        source_files = {\n",
    "            'source_text_raw': { 'filename': container.source_path, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "            'source_text_edited': { 'filename': container.source_path, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "            'source_text_preprocessed': { 'filename': container.prepped_source_path, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "        }        \n",
    "        \n",
    "        source_filename = source_files[display_type]['filename']\n",
    "        description =  source_files[display_type]['description']\n",
    "        text = utility.zip_get_text(source_filename, doc.metadata['filename'])\n",
    "        \n",
    "        with gui.output_text:\n",
    "            #print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            #print(doc)\n",
    "            print('[ ' + description.upper() + ' ]')\n",
    "            print(text)\n",
    "        return\n",
    "    \n",
    "    args = dict(\n",
    "        args=dict(\n",
    "            ngrams=gui.ngrams.value,\n",
    "            named_entities=gui.named_entities.value,\n",
    "            normalize=gui.normalize.value,\n",
    "            as_strings=True\n",
    "        ),\n",
    "        kwargs=dict(\n",
    "            min_freq=gui.min_freq.value,\n",
    "            include_pos=gui.include_pos.value,\n",
    "            filter_stops=gui.filter_stops.value,\n",
    "            filter_punct=gui.filter_punct.value\n",
    "        ),\n",
    "        extra_stop_words=set(gui.stop_words.value),\n",
    "        substitutions=gpe_substitutions if gui.mask_gpe.value else set([]),\n",
    "        min_freq=gui.min_freq.value,\n",
    "        max_doc_freq=gui.max_doc_freq.value\n",
    "    )\n",
    "\n",
    "    terms = [ x for x in textacy_utility.extract_corpus_terms(corpus, extract_args)]\n",
    "    \n",
    "    if len(terms) == 0:\n",
    "        with gui.output_text:\n",
    "            print(\"No text. Please change selection.\")\n",
    "        return\n",
    "    \n",
    "    if display_type in ['sanitized_text', 'statistics']:\n",
    "\n",
    "        if display_type == 'sanitized_text':\n",
    "            with gui.output_text:\n",
    "                #display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                #    ' '.join(tokens[:word_count]),\n",
    "                #    ' '.join(tokens[-word_count:])\n",
    "                #))\n",
    "                print(' '.join([ t.replace(' ', '_') for t in terms ]))\n",
    "                return\n",
    "\n",
    "        if display_type == 'statistics':\n",
    "\n",
    "            wf = nltk.FreqDist(terms)\n",
    "\n",
    "            with gui.output_text:\n",
    "\n",
    "                df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                print('Token count: {} Vocab count: {}'.format(wf.N(), wf.B()))\n",
    "                display(df)\n",
    " \n",
    "            with gui.output_statistics:\n",
    "\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "def display_cleanup_text_gui(container, wti_index):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "    \n",
    "    gpe_substitutions = get_gpe_names(corpus, filename='country_lemmas.txt')\n",
    "    word_counts = corpus.word_freqs(normalize=normalize, weighting=weighting, as_strings=as_strings)\n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = [('(All)', None)] + sorted([(k + ' (' + v + ')', k) for k,v in pos_tags.items() ])\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    gui = types.SimpleNamespace(\n",
    "        treaty_id=widgets.Dropdown(description='Treaty', options=document_options, value=None, layout=widgets.Layout(width='400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1, layout=widgets.Layout(width='400px'), tooltip='TEST'),\n",
    "        max_doc_freq=widgets.FloatSlider(description='Min doc freq', min=0.75, max=1.0, value=1.0, step=0.01, layout=widgets.Layout(width='400px')),\n",
    "        mask_gpe=widgets.ToggleButton(value=False, description='Mask GPE',  tooltip='Replace geographical entites with `_gpe_`', icon='check'),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ False, 'lemma', 'lower' ], value=False, layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    uix = widgets.interactive(\n",
    "        display_cleaned_up_text,\n",
    "        container=widgets.fixed(container),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        treaty_id=gui.treaty_id\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.treaty_id,\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq,\n",
    "                gui.max_doc_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.mask_gpe,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "                                  \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "try:\n",
    "    xgui, xuix = display_cleanup_text_gui(get_current_corpus(), WTI_INDEX)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> Most Discriminating Terms<span style='color: blue; float: right'>OPTIONAL</span>\n",
    "References\n",
    "King, Gary, Patrick Lam, and Margaret Roberts. “Computer-Assisted Keyword and Document Set Discovery from Unstructured Text.” (2014). http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.1445&rep=rep1&type=pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527300e108954664815d442a8d35bdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(SelectMultiple(description='Group 1', layout=Layout(width='250px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import most_discriminating_terms_gui\n",
    "\n",
    "try:\n",
    "    most_discriminating_terms_gui.display_gui(WTI_INDEX, get_current_corpus().textacy_corpus)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accession': 5,\n",
       " 'share': 35,\n",
       " 'subscribe': 1,\n",
       " 'cecil': 2,\n",
       " '68-479': 1,\n",
       " 'date': 880,\n",
       " 'anti-slav': 2,\n",
       " '4720': 1,\n",
       " 'prevail': 75,\n",
       " 'macedo': 7,\n",
       " 'drafting': 4,\n",
       " '1381': 2,\n",
       " 'cloud': 4,\n",
       " 'circle': 4,\n",
       " 'similar': 173,\n",
       " 'auswartige': 1,\n",
       " 'gliga': 9,\n",
       " 'word': 11,\n",
       " 'aalborg': 1,\n",
       " '3707': 1,\n",
       " 'passengers': 1,\n",
       " 'ingon': 1,\n",
       " 'attempt': 2,\n",
       " 'mission': 296,\n",
       " 'sweden': 4,\n",
       " 'fadil': 2,\n",
       " 'naszkowski': 2,\n",
       " 'ceiling': 1,\n",
       " 'sur': 1,\n",
       " 'isle': 2,\n",
       " 'unpacking': 1,\n",
       " 'polish-bulgarian': 1,\n",
       " 'definition': 2,\n",
       " 'ate': 1,\n",
       " 'soviet-indonesian': 1,\n",
       " 'prof': 5,\n",
       " 'herr': 1,\n",
       " 'gratuitously': 1,\n",
       " 'stamp': 6,\n",
       " 'school-leaver': 1,\n",
       " 'kanjongnok': 1,\n",
       " 'trading': 1,\n",
       " 'systematic': 20,\n",
       " '170': 1,\n",
       " 'romanian-italian': 2,\n",
       " 'cyril': 2,\n",
       " 'guy-robert': 1,\n",
       " '1936': 2,\n",
       " 'standard': 27,\n",
       " 'faculties': 5,\n",
       " 'incoming': 1,\n",
       " 'mannichok': 1,\n",
       " 'healing': 1,\n",
       " 'append': 3,\n",
       " 'august': 142,\n",
       " 'expert': 358,\n",
       " 'golda': 11,\n",
       " 'reside': 18,\n",
       " 'an4': 1,\n",
       " 'satisfy': 17,\n",
       " 'carrying': 7,\n",
       " 'szl': 1,\n",
       " 'bearing': 6,\n",
       " 'leader': 20,\n",
       " 'nationals': 4,\n",
       " 'necessitate': 2,\n",
       " '.embassy': 1,\n",
       " 'medal': 1,\n",
       " 'primer': 1,\n",
       " 'grandes': 5,\n",
       " 'nakhai': 1,\n",
       " 'rohal': 1,\n",
       " 'fulfillment': 7,\n",
       " 'henceforward': 1,\n",
       " 'file': 7,\n",
       " 'schooten': 2,\n",
       " '1960': 128,\n",
       " 'dia': 1,\n",
       " 'verbale': 9,\n",
       " 'directive': 1,\n",
       " 'invoke': 1,\n",
       " 'exchangeable': 1,\n",
       " 'bring': 63,\n",
       " 'opportunity': 104,\n",
       " 'rapid': 1,\n",
       " 'respondence': 1,\n",
       " 'assouan': 3,\n",
       " 'sing': 3,\n",
       " 'enrolment': 17,\n",
       " 'cation': 4,\n",
       " 'guti': 4,\n",
       " '1967': 120,\n",
       " 'sign': 819,\n",
       " '19581': 1,\n",
       " '9073': 2,\n",
       " 'bracelet': 3,\n",
       " 'stuttgart': 1,\n",
       " 'balafrej': 2,\n",
       " 'yugoslav': 18,\n",
       " 'albornoz': 1,\n",
       " 'misrepresent': 2,\n",
       " 'particular': 345,\n",
       " 'endorse': 1,\n",
       " 'magnetic': 7,\n",
       " 'holiday': 88,\n",
       " 'semi-annually': 1,\n",
       " '1367': 1,\n",
       " 'veterinary': 1,\n",
       " 'counter-claim': 1,\n",
       " 'restore': 6,\n",
       " 'apportionment': 4,\n",
       " 'gielen': 2,\n",
       " 'event': 277,\n",
       " 'scouting': 1,\n",
       " 'deco': 3,\n",
       " 'edu': 1,\n",
       " 'subsisting': 1,\n",
       " 'fellows': 1,\n",
       " 'specialization': 40,\n",
       " 'certification': 1,\n",
       " 'doudou': 2,\n",
       " 'conservation': 7,\n",
       " 'token': 1,\n",
       " 'pre-history': 1,\n",
       " 'follow': 842,\n",
       " 'linen': 2,\n",
       " '1344': 1,\n",
       " 'president': 196,\n",
       " '18th': 1,\n",
       " 'contemplate': 5,\n",
       " 'music': 147,\n",
       " 'popovic': 2,\n",
       " 'joaquin': 2,\n",
       " 'lessening': 1,\n",
       " 'finan': 1,\n",
       " 'sejo': 1,\n",
       " 'telegraphic': 4,\n",
       " 'papazov': 2,\n",
       " 'inevitably': 1,\n",
       " 'mitigate': 1,\n",
       " 'secretariat': 33,\n",
       " 'address': 33,\n",
       " 'provisions': 8,\n",
       " 'chang': 6,\n",
       " 'incapacitate': 5,\n",
       " 'attraction': 8,\n",
       " 'hsingmingkueichih': 1,\n",
       " '308': 2,\n",
       " 'list': 105,\n",
       " 'equality': 56,\n",
       " 'reports': 1,\n",
       " 'identify': 3,\n",
       " 'ber': 2,\n",
       " 'summer': 43,\n",
       " '218': 1,\n",
       " '1652': 1,\n",
       " 'parallel': 1,\n",
       " 'factor': 11,\n",
       " 'spanish-french': 1,\n",
       " 'carthage': 1,\n",
       " 'one-year': 3,\n",
       " 'el': 43,\n",
       " 'die': 2,\n",
       " 'independent': 4,\n",
       " 'deserve': 3,\n",
       " 'radio-broadcast': 1,\n",
       " 'mcbride': 2,\n",
       " 'produce': 53,\n",
       " 'lectorship': 6,\n",
       " 'living': 18,\n",
       " 'p': 169,\n",
       " 'ivan': 3,\n",
       " 'joachim': 2,\n",
       " 'nian': 1,\n",
       " 'expedite': 2,\n",
       " 'mengtzu': 1,\n",
       " '852': 1,\n",
       " 'pamphlet': 16,\n",
       " 'prevoyance': 1,\n",
       " 'vol': 118,\n",
       " '3': 772,\n",
       " 'ernst': 1,\n",
       " 'attachment': 1,\n",
       " 'odette': 2,\n",
       " 'unestablish': 3,\n",
       " 'demetrios': 1,\n",
       " 'inspection': 29,\n",
       " 'british-argentine': 1,\n",
       " 'linking': 1,\n",
       " 'yungcheng': 3,\n",
       " 'empire': 10,\n",
       " '11002': 1,\n",
       " 'soviet-danish': 1,\n",
       " 'mechanization': 4,\n",
       " 'czech': 58,\n",
       " 'cabul': 1,\n",
       " 'senes': 1,\n",
       " 'round': 4,\n",
       " 'satisfactory': 18,\n",
       " 'signatory': 10,\n",
       " 'car!as': 1,\n",
       " 'nigeria': 3,\n",
       " 'simple': 1,\n",
       " 'contributions': 2,\n",
       " 'affixed': 1,\n",
       " 'necessarily': 10,\n",
       " 'pasteur': 1,\n",
       " 'recherche': 3,\n",
       " 'nucleus': 1,\n",
       " 'continuing-education': 1,\n",
       " 'catholic': 2,\n",
       " 'december': 162,\n",
       " 'roczei': 1,\n",
       " '106': 1,\n",
       " 'saharan': 2,\n",
       " 'pastor': 4,\n",
       " 'fuad': 3,\n",
       " 'indi-czechoslovak': 1,\n",
       " 'quality': 5,\n",
       " 'reasonable': 8,\n",
       " 'skill': 14,\n",
       " 'crew': 2,\n",
       " 'telecommunication': 3,\n",
       " '5ion': 2,\n",
       " 'pharmacist': 12,\n",
       " 'blood': 1,\n",
       " 'conducive': 13,\n",
       " 'mihai': 2,\n",
       " 'guillaume': 1,\n",
       " 'automation': 1,\n",
       " 'anglais': 1,\n",
       " 'educate': 1,\n",
       " 'monu': 1,\n",
       " 'les': 2,\n",
       " 'successively': 1,\n",
       " '327': 1,\n",
       " 'rzym0wski': 1,\n",
       " 'height': 1,\n",
       " 'bratislava': 2,\n",
       " 'iranian': 9,\n",
       " 'monuments': 14,\n",
       " 'brussels': 100,\n",
       " 'confinement': 4,\n",
       " 'sponsor': 43,\n",
       " 'argentine-british': 1,\n",
       " 'homero': 1,\n",
       " 'aspect': 62,\n",
       " 'e60': 1,\n",
       " 'ministerial': 1,\n",
       " 'preferably': 3,\n",
       " 'manuscripts': 1,\n",
       " 'jesus': 3,\n",
       " 'german-italian': 1,\n",
       " 'mulo': 1,\n",
       " 'hsii': 1,\n",
       " 'invite': 134,\n",
       " 'romana': 1,\n",
       " 'woodblock': 1,\n",
       " '6221': 2,\n",
       " 'professore': 1,\n",
       " 'polish-hungarian': 2,\n",
       " 'immediately': 38,\n",
       " 'employed': 1,\n",
       " 'doubt': 24,\n",
       " 'scries': 1,\n",
       " 'abroad': 14,\n",
       " 'scholar': 164,\n",
       " 'arieh': 2,\n",
       " 'chinese': 33,\n",
       " 'engineer': 46,\n",
       " 'arranging': 7,\n",
       " 'termless': 1,\n",
       " '6589': 2,\n",
       " 'lothar': 2,\n",
       " 'cancer': 4,\n",
       " 'stockholm': 5,\n",
       " 'soiree': 1,\n",
       " 'nationality': 16,\n",
       " 'djakarta': 12,\n",
       " '8th': 1,\n",
       " 'solve': 5,\n",
       " 'subparagraph': 11,\n",
       " '23229': 1,\n",
       " 'collaborate': 28,\n",
       " 'happily': 4,\n",
       " 'protectorate': 9,\n",
       " 'hochschulstudiengesetz': 1,\n",
       " 'divergence': 13,\n",
       " 'operating': 2,\n",
       " 'alliances': 1,\n",
       " 'payable': 34,\n",
       " 'henry': 5,\n",
       " 'feel': 4,\n",
       " 'samjiarajip': 1,\n",
       " 'allowance': 142,\n",
       " '156': 1,\n",
       " 'normal': 23,\n",
       " 'n': 63,\n",
       " 'republic': 3343,\n",
       " '1989': 2,\n",
       " 'intensi': 1,\n",
       " 'delegation': 218,\n",
       " 'chieh': 2,\n",
       " 'politic': 1,\n",
       " 'make-up': 1,\n",
       " 'round-trip': 12,\n",
       " 'country': 4494,\n",
       " 'studytour': 1,\n",
       " 'alternation': 3,\n",
       " 'non-national': 1,\n",
       " 'practice': 63,\n",
       " 'palar': 2,\n",
       " 'co-production': 3,\n",
       " 'reporter': 6,\n",
       " 'adjust': 5,\n",
       " 'correction': 5,\n",
       " 'heung': 1,\n",
       " 'worthy': 1,\n",
       " 'passing-out': 2,\n",
       " 'chairmen': 7,\n",
       " 'abstract': 6,\n",
       " 'walter': 6,\n",
       " 'departement': 1,\n",
       " 'dino': 2,\n",
       " '360': 2,\n",
       " 'indirectly': 4,\n",
       " '11951': 2,\n",
       " 'reduced': 2,\n",
       " 'adjunct': 2,\n",
       " 'obligation': 69,\n",
       " 'hatem': 5,\n",
       " 'follow-up': 1,\n",
       " 'technically': 1,\n",
       " 'romance': 1,\n",
       " '201': 1,\n",
       " 'philosopher': 2,\n",
       " 'thailand': 5,\n",
       " 'projector': 17,\n",
       " 'television': 441,\n",
       " 'organise': 246,\n",
       " 'corporate': 19,\n",
       " '10969': 1,\n",
       " 'liquefied': 1,\n",
       " 'administrations': 2,\n",
       " 'historian': 5,\n",
       " 'quartet': 3,\n",
       " 'resolution': 22,\n",
       " 'acceptable': 101,\n",
       " 'konomi': 2,\n",
       " 'wit': 1,\n",
       " '9849': 2,\n",
       " 'deal': 54,\n",
       " 'craft': 2,\n",
       " '404': 1,\n",
       " 'priority': 15,\n",
       " 'peng': 3,\n",
       " '11289': 1,\n",
       " 'chipte': 1,\n",
       " 'series__________1971': 1,\n",
       " '8': 411,\n",
       " '4394': 1,\n",
       " 'spirit': 85,\n",
       " 'kosachwaryo': 1,\n",
       " 'approval': 254,\n",
       " 'joint': 433,\n",
       " 'announce': 17,\n",
       " 'propinquity': 1,\n",
       " 'dur': 1,\n",
       " 'che': 5,\n",
       " 'vasilevich': 3,\n",
       " '5874': 1,\n",
       " 'october-november': 2,\n",
       " 'llewellyn': 2,\n",
       " 'proclaim': 1,\n",
       " 'mathematics': 9,\n",
       " 'excavation': 28,\n",
       " 'behalf': 65,\n",
       " 'lavrov': 1,\n",
       " 'signed': 386,\n",
       " '-exchange': 1,\n",
       " 'essential': 9,\n",
       " 'bylandt': 1,\n",
       " 'group': 338,\n",
       " 'encyclopaedia': 1,\n",
       " 'rosianu': 1,\n",
       " 'nudenkhugin': 1,\n",
       " 'sunwutzuchihchieh': 1,\n",
       " 'gymnastic': 2,\n",
       " 'instruction': 100,\n",
       " 'approve': 164,\n",
       " 'denes': 1,\n",
       " 'legally': 4,\n",
       " 'mahmood': 2,\n",
       " 'alteration': 4,\n",
       " '4676': 1,\n",
       " 'nicosia': 3,\n",
       " 'world': 21,\n",
       " 'auguste': 2,\n",
       " 'fraternal': 17,\n",
       " 'averoff-tossiza': 1,\n",
       " 'mentally': 2,\n",
       " 'student': 853,\n",
       " 'constructioninstallation': 1,\n",
       " 'chungyunghuochu': 1,\n",
       " 'juan': 10,\n",
       " '1961/62': 3,\n",
       " 'iraq': 76,\n",
       " 'montserrat': 1,\n",
       " 'factory': 6,\n",
       " 'engineers': 3,\n",
       " 'illustrate': 5,\n",
       " 'abeles': 2,\n",
       " 'yang': 1,\n",
       " 'potentiary': 3,\n",
       " 'aggregation': 2,\n",
       " 'museography': 3,\n",
       " 'wilski': 2,\n",
       " '248': 2,\n",
       " 'amerika': 1,\n",
       " 'quires': 2,\n",
       " '9501': 1,\n",
       " 'manol': 2,\n",
       " 'entitlement': 10,\n",
       " 'add': 16,\n",
       " 'faculty': 30,\n",
       " 'brotherly': 2,\n",
       " 'realgymnasium': 4,\n",
       " 'risk': 8,\n",
       " 'shall': 10179,\n",
       " 'conductor': 13,\n",
       " 'cincinnati': 1,\n",
       " 'davis': 3,\n",
       " 'hosia': 2,\n",
       " '22467': 1,\n",
       " 'similarly': 27,\n",
       " 'counsel': 4,\n",
       " 'offering': 1,\n",
       " 'huque': 3,\n",
       " 'tain': 1,\n",
       " 'mikhail': 4,\n",
       " 'kamaluddin': 2,\n",
       " 'cape-town': 1,\n",
       " 'cost': 150,\n",
       " 'romania': 198,\n",
       " 'radio': 604,\n",
       " 'appreciate': 4,\n",
       " 'advise': 31,\n",
       " 'inhabitant': 1,\n",
       " 'coronary': 1,\n",
       " 'cooperate': 13,\n",
       " 'commissions': 56,\n",
       " '30-day': 1,\n",
       " '10386': 1,\n",
       " 'co-operate': 49,\n",
       " 'nistor': 2,\n",
       " 'rabi': 1,\n",
       " 'laihsiensheng': 1,\n",
       " 'passage': 7,\n",
       " 'son': 1,\n",
       " 'seeking': 1,\n",
       " '9847': 1,\n",
       " 'hagen': 1,\n",
       " 'antiquities': 5,\n",
       " 'commercial': 71,\n",
       " 'increasingly': 1,\n",
       " 'oliveira': 1,\n",
       " 'encouragement': 26,\n",
       " 'ivanovsky': 1,\n",
       " 'wire': 2,\n",
       " '.the': 1,\n",
       " 'affair': 51,\n",
       " 'describe': 19,\n",
       " '9th': 4,\n",
       " '5025': 1,\n",
       " 'mass': 38,\n",
       " 'panama': 55,\n",
       " 'devise': 5,\n",
       " 'enemy': 1,\n",
       " 'metropolitan': 9,\n",
       " 'azurdia': 1,\n",
       " '230': 1,\n",
       " 'domingo': 9,\n",
       " 'mineshaft': 1,\n",
       " 'representatives': 4,\n",
       " 'pablo': 2,\n",
       " 'chonjo': 3,\n",
       " 'receipt': 55,\n",
       " 'comm1ss1on': 1,\n",
       " 'panamanian': 1,\n",
       " 'pedagogic': 6,\n",
       " 'site': 23,\n",
       " 'moshe': 2,\n",
       " 'dong': 4,\n",
       " 'boliviano-mexicana': 1,\n",
       " 'jos6': 3,\n",
       " 'ex': 19,\n",
       " 'brazzaville': 8,\n",
       " 'rich': 2,\n",
       " 'usual': 4,\n",
       " '1949': 18,\n",
       " 'film-projector': 2,\n",
       " 'experience': 232,\n",
       " 'tape': 63,\n",
       " 'moussa': 2,\n",
       " 'pertinent': 6,\n",
       " 'singing': 1,\n",
       " 'roumania': 11,\n",
       " 'long': 21,\n",
       " 'six-months': 1,\n",
       " 'mendez': 5,\n",
       " 'moyen': 1,\n",
       " 'immediate': 7,\n",
       " 'transparency': 6,\n",
       " 'binding': 4,\n",
       " 'flat': 3,\n",
       " 'hospitality': 1,\n",
       " 'pure': 2,\n",
       " 'pansa': 2,\n",
       " 'gymnasier': 1,\n",
       " 'chingshihleiming': 1,\n",
       " 'fldelski': 1,\n",
       " 'fruitful': 17,\n",
       " '1-23472': 1,\n",
       " 'head': 63,\n",
       " 'super': 2,\n",
       " 'chiehchitingchi-': 1,\n",
       " 'authoritative': 25,\n",
       " 'observer': 3,\n",
       " 'closed': 1,\n",
       " 'facilitate-and': 1,\n",
       " 'tongmu': 2,\n",
       " 'ratification': 730,\n",
       " 'seals': 1,\n",
       " 'man-month': 13,\n",
       " 'subcommission': 4,\n",
       " 'comparative': 1,\n",
       " 'e': 368,\n",
       " 'constitute': 61,\n",
       " 'interpret': 3,\n",
       " 'maharraka': 1,\n",
       " 'bechuanaland': 1,\n",
       " 'carias': 1,\n",
       " 'staying': 1,\n",
       " 'applied': 14,\n",
       " 'child': 64,\n",
       " 'fayat': 1,\n",
       " '953': 1,\n",
       " '8377': 2,\n",
       " 'rumanian': 73,\n",
       " 'metaxas': 2,\n",
       " 'presidium': 4,\n",
       " 'danish-french': 1,\n",
       " '1948': 58,\n",
       " 'awareness': 6,\n",
       " 'duly': 140,\n",
       " 'kuwaiti': 2,\n",
       " 'muhiddin': 1,\n",
       " 'naradhip': 2,\n",
       " 'auspex': 4,\n",
       " 'bank': 11,\n",
       " 'burden': 2,\n",
       " '9442': 1,\n",
       " 'album': 7,\n",
       " 'kratochvil': 1,\n",
       " 'assistant': 44,\n",
       " 'fortune': 2,\n",
       " 'conversion': 1,\n",
       " 'reappointment': 2,\n",
       " 'come': 463,\n",
       " 'autonomy': 1,\n",
       " 'islamic': 52,\n",
       " 'finnish': 20,\n",
       " 'yeuiryujip': 1,\n",
       " 'ou': 1,\n",
       " 'virtue': 6,\n",
       " 'castro': 4,\n",
       " 'harmonize': 9,\n",
       " 'republique': 1,\n",
       " 'run': 3,\n",
       " 'tsaitzuhuishu': 1,\n",
       " 'fort-lamy': 1,\n",
       " 'pattern': 2,\n",
       " 'understand': 44,\n",
       " 'non-intervention': 6,\n",
       " 'appeal': 6,\n",
       " 'laws': 2,\n",
       " 'namhwagyongchuhesanbo': 1,\n",
       " '181': 2,\n",
       " 'senegalese': 1,\n",
       " 'ort': 1,\n",
       " 'confederation': 3,\n",
       " 'invalidity': 1,\n",
       " 'crop': 2,\n",
       " 'exceed': 40,\n",
       " '8078': 2,\n",
       " 'huiluchiureproduced': 1,\n",
       " 'frequent': 1,\n",
       " 'ideal': 15,\n",
       " 'map': 8,\n",
       " 'promptly': 3,\n",
       " 'adenauer': 5,\n",
       " 'tao-sh': 1,\n",
       " 'solely': 10,\n",
       " 'comedy': 1,\n",
       " 'acting': 30,\n",
       " 'anthropological': 10,\n",
       " 'bal': 1,\n",
       " 'pompiliu': 2,\n",
       " 'brazilian': 52,\n",
       " 'refuse': 4,\n",
       " 'importation': 73,\n",
       " 'shape': 1,\n",
       " 'deaf': 1,\n",
       " 'rachad': 2,\n",
       " 'marketing': 5,\n",
       " '-material': 1,\n",
       " 'alcide': 1,\n",
       " 'sojourn': 17,\n",
       " 'oiaiz': 1,\n",
       " 'tegucigalpa': 14,\n",
       " 'cinemato': 1,\n",
       " '100th': 1,\n",
       " 'jr': 4,\n",
       " 'control': 32,\n",
       " 'thereof': 97,\n",
       " 'hoang-minhgiam': 1,\n",
       " 'rector': 12,\n",
       " 'goda': 1,\n",
       " 'cial': 1,\n",
       " 'intent': 11,\n",
       " 'm': 78,\n",
       " 'sol': 1,\n",
       " 'respective': 1002,\n",
       " 'efon': 1,\n",
       " 'continuous': 13,\n",
       " 'charge': 132,\n",
       " 'uris': 1,\n",
       " 'promotional': 2,\n",
       " 'dispel': 2,\n",
       " 'tonghegongyugo': 1,\n",
       " 'criado': 2,\n",
       " 'benjelloun': 1,\n",
       " 'native': 3,\n",
       " 'oasis': 2,\n",
       " 'nationally': 3,\n",
       " '4547': 1,\n",
       " 'post-war': 3,\n",
       " 'wonsop': 1,\n",
       " 'influence': 2,\n",
       " 'discrepancy': 4,\n",
       " 'unable': 15,\n",
       " 'naegelen': 4,\n",
       " 'tossizza': 1,\n",
       " 'cf\\\\ft7p': 1,\n",
       " 'circular': 4,\n",
       " 'mirepresentation': 1,\n",
       " 'threefour': 1,\n",
       " 'device': 2,\n",
       " 'audits': 3,\n",
       " 'exhaust': 5,\n",
       " '41': 12,\n",
       " 'suggest': 29,\n",
       " 'completely': 2,\n",
       " 'prolong': 15,\n",
       " 'periods': 1,\n",
       " 'arosemena': 2,\n",
       " 'tighten': 1,\n",
       " 'employ': 37,\n",
       " 'charg6': 1,\n",
       " 'primarily': 12,\n",
       " 'domicile': 8,\n",
       " 'lecteur': 5,\n",
       " '3693': 1,\n",
       " 'servitude': 1,\n",
       " 'fu': 1,\n",
       " 'belles-lettre': 5,\n",
       " '425260': 1,\n",
       " 'three-legged': 1,\n",
       " 'avram': 1,\n",
       " 'soviet-bulgarian': 1,\n",
       " '3847': 1,\n",
       " 'anghel': 1,\n",
       " '770': 1,\n",
       " 'treasurer': 13,\n",
       " 'czechoslovak-bulgarian': 2,\n",
       " 'formance': 1,\n",
       " 'cioroiu': 2,\n",
       " 'sixty-four': 5,\n",
       " 'odaka': 1,\n",
       " 'official': 687,\n",
       " 'double': 1,\n",
       " 'moura': 2,\n",
       " 'expansion': 29,\n",
       " 'securite': 5,\n",
       " 'enterprises': 1,\n",
       " 'wilfully': 1,\n",
       " 'stolen': 1,\n",
       " 'notes': 35,\n",
       " 'dfaz': 1,\n",
       " 'sunday': 6,\n",
       " 'public': 342,\n",
       " 'sub-committee': 27,\n",
       " 'ought': 1,\n",
       " 'yifru': 3,\n",
       " 'extraordinario': 2,\n",
       " 'neerlandais': 1,\n",
       " 'worth': 1,\n",
       " 'ullens': 2,\n",
       " 'spyros': 3,\n",
       " 'reconstruct': 1,\n",
       " 'highness': 15,\n",
       " 'neighbour': 4,\n",
       " 'preserve': 24,\n",
       " 'aoustfn': 1,\n",
       " 'bi-national': 1,\n",
       " 'newscast': 3,\n",
       " 'roundtrip': 1,\n",
       " '-reciprocal': 1,\n",
       " 'sponsoring': 4,\n",
       " 'argentine': 142,\n",
       " 'georgetown': 2,\n",
       " 'writer': 101,\n",
       " 'minutes': 1,\n",
       " 'conclude': 670,\n",
       " 'import': 149,\n",
       " 're-opening': 3,\n",
       " 'three-six': 1,\n",
       " 'peculiar': 1,\n",
       " 'berryer': 3,\n",
       " 'knowledge': 315,\n",
       " 'college': 56,\n",
       " 'co-signatorie': 1,\n",
       " 'i-b': 1,\n",
       " '10737': 1,\n",
       " 'worker': 419,\n",
       " 'details': 3,\n",
       " 'ministers': 77,\n",
       " '1969-1971': 1,\n",
       " 'commissariat': 1,\n",
       " '145': 3,\n",
       " 'diff': 1,\n",
       " 'mu': 1,\n",
       " 'electrophone': 1,\n",
       " 'annual': 157,\n",
       " 'coach': 4,\n",
       " 'barbados': 1,\n",
       " 'sub-committees': 2,\n",
       " 'h': 105,\n",
       " 'munich': 2,\n",
       " 'addis': 18,\n",
       " 'cornee': 1,\n",
       " 'cheque': 2,\n",
       " 'then1': 1,\n",
       " 'adversely': 3,\n",
       " 'thai': 1,\n",
       " 'insecticide': 1,\n",
       " 'bonjung': 1,\n",
       " '528': 1,\n",
       " 'legion': 1,\n",
       " '2--4': 1,\n",
       " 'guides': 1,\n",
       " 'privilege': 51,\n",
       " 'carry-out': 1,\n",
       " 'decorate': 1,\n",
       " 'hold': 341,\n",
       " '9341': 1,\n",
       " 'legislation': 238,\n",
       " 'convention1': 5,\n",
       " 'dalibor': 2,\n",
       " 'animate': 3,\n",
       " 'urge': 1,\n",
       " 'ordibehesht': 2,\n",
       " 'o': 23,\n",
       " 'exempt': 81,\n",
       " 'academician': 3,\n",
       " 'domestic': 97,\n",
       " 'flowerin-circle': 1,\n",
       " 'komitas': 1,\n",
       " 'pouch': 1,\n",
       " 'prearrange': 1,\n",
       " 'proletarian': 2,\n",
       " 'kabul': 13,\n",
       " 'elapse': 3,\n",
       " 'material': 539,\n",
       " '/6070/269': 1,\n",
       " 'yugoslavia-one': 1,\n",
       " 'tax': 178,\n",
       " 'ortiz': 14,\n",
       " 'trujillo': 1,\n",
       " 'multilateral': 15,\n",
       " 'seniority': 3,\n",
       " 'anglo-brazilian': 1,\n",
       " 'unique': 1,\n",
       " 'eric': 3,\n",
       " 'peda': 1,\n",
       " 'low': 6,\n",
       " 'focus': 2,\n",
       " 'nobusuke': 6,\n",
       " 'bequest': 4,\n",
       " 'urban': 9,\n",
       " '107': 1,\n",
       " 'non-commercially': 2,\n",
       " 'bureau': 4,\n",
       " 'lajti': 2,\n",
       " 'monetary': 4,\n",
       " '11907': 1,\n",
       " 'cestmir': 2,\n",
       " 'serbo-croatian': 1,\n",
       " 'alberdi': 1,\n",
       " 'agreement1': 35,\n",
       " 'francai': 2,\n",
       " 'gainer': 1,\n",
       " 'israeli': 1,\n",
       " 'bengali': 1,\n",
       " 'advanced-training': 1,\n",
       " 'smoothing-iron': 2,\n",
       " 'pompeu': 1,\n",
       " 'yunming': 1,\n",
       " 'teaching': 429,\n",
       " 'shihhua': 1,\n",
       " 'senior': 19,\n",
       " 'oriental': 1,\n",
       " 'solution': 35,\n",
       " 'zuazo': 1,\n",
       " '67-210': 4,\n",
       " '1933': 1,\n",
       " 'appliance': 4,\n",
       " 'three-year': 3,\n",
       " 'repertoire': 4,\n",
       " '1796-1820': 26,\n",
       " 'aires': 73,\n",
       " 'waller': 1,\n",
       " 'transmit': 44,\n",
       " 'stem': 1,\n",
       " 'tsoshihpoichuchieh': 1,\n",
       " 'january': 165,\n",
       " 'templ': 1,\n",
       " 'kyongse': 1,\n",
       " '970': 2,\n",
       " 'house': 20,\n",
       " 'force': 1650,\n",
       " 'paint': 1,\n",
       " 'facilitation': 7,\n",
       " 'recognise': 174,\n",
       " 'bulajk': 1,\n",
       " 'relax': 2,\n",
       " '5644': 2,\n",
       " 'mihajlo': 1,\n",
       " 'cyranigewicz': 1,\n",
       " 'famous': 1,\n",
       " 'bravo': 6,\n",
       " 'com': 19,\n",
       " 'khub': 2,\n",
       " 'two-three': 2,\n",
       " 'treaty2': 1,\n",
       " 'freezer': 3,\n",
       " '8921': 2,\n",
       " 'periodic': 39,\n",
       " '3698': 1,\n",
       " 'sir': 150,\n",
       " 'bator': 6,\n",
       " '1962/63': 2,\n",
       " 'stitution': 1,\n",
       " 'speedily': 1,\n",
       " 'pofujet': 1,\n",
       " '15th': 11,\n",
       " 'sessions': 1,\n",
       " 'traduction2': 1,\n",
       " 'spiritually': 1,\n",
       " 'tobias': 1,\n",
       " 'encourage': 1557,\n",
       " 'cam': 1,\n",
       " 'geographic': 6,\n",
       " 'running': 15,\n",
       " 'succeed': 3,\n",
       " 'difficulty': 22,\n",
       " 'dismantling': 3,\n",
       " '1945-1946': 4,\n",
       " 'better': 59,\n",
       " 'duty': 287,\n",
       " '319': 2,\n",
       " '177/1966': 1,\n",
       " 'rumania': 4,\n",
       " 'select': 75,\n",
       " 'technicalscientific': 1,\n",
       " 'fodeba': 1,\n",
       " 'familiar': 1,\n",
       " '59': 4,\n",
       " 'contemporary': 13,\n",
       " '143': 2,\n",
       " 'aptitude': 7,\n",
       " 'zaire': 7,\n",
       " 'un-officially': 1,\n",
       " 'traycho': 2,\n",
       " 'automobile': 7,\n",
       " 'profit': 2,\n",
       " 'affect': 54,\n",
       " 'sandstone': 1,\n",
       " 'calvo': 7,\n",
       " '358': 2,\n",
       " 'goverment': 1,\n",
       " 'abolition': 6,\n",
       " '87': 2,\n",
       " 'subversive': 1,\n",
       " 'bruce': 1,\n",
       " 'bowl-cover': 1,\n",
       " 'unofficial': 1,\n",
       " 'convince': 41,\n",
       " 'film-industry': 2,\n",
       " 'accrue': 11,\n",
       " '14140': 1,\n",
       " 'doctoral': 5,\n",
       " 'daufresne': 1,\n",
       " 'airfield': 1,\n",
       " 'yugoslav-polish': 1,\n",
       " 'moscow': 112,\n",
       " 'projection': 4,\n",
       " 'final': 43,\n",
       " 'finno-ugric': 3,\n",
       " 'cultural': 4080,\n",
       " '1963s': 1,\n",
       " 'shihhsuan': 1,\n",
       " 'carlo': 4,\n",
       " 'video': 1,\n",
       " '1860': 1,\n",
       " '6213': 2,\n",
       " 'vacation': 20,\n",
       " 'statute': 20,\n",
       " 'actually': 4,\n",
       " 'khandji': 1,\n",
       " 'overtime': 9,\n",
       " 'kinescope': 1,\n",
       " 'sculptor': 2,\n",
       " '1952': 18,\n",
       " 'maulana': 4,\n",
       " '8202': 2,\n",
       " 'forty-eighth': 1,\n",
       " 'elimination': 1,\n",
       " 'aix-marseilles': 1,\n",
       " 'gathering': 4,\n",
       " 'dejean': 2,\n",
       " 'inter': 45,\n",
       " 'signed\\\\': 3,\n",
       " '37/863/dg': 1,\n",
       " 'entirety': 2,\n",
       " 'czechoslovak': 246,\n",
       " 'eighthly': 1,\n",
       " 'i960': 4,\n",
       " 'obstacle': 1,\n",
       " 'adrien': 2,\n",
       " 'incised': 3,\n",
       " 'spaniard': 1,\n",
       " '12786': 1,\n",
       " 'spittoon': 1,\n",
       " 'integrity': 2,\n",
       " 'amplification': 3,\n",
       " '11606': 1,\n",
       " '32090': 1,\n",
       " 'extra-mural': 2,\n",
       " 'iyar': 1,\n",
       " '180th': 1,\n",
       " 'ideology': 4,\n",
       " '189': 1,\n",
       " 'apart': 2,\n",
       " 'budap': 1,\n",
       " 'quorum': 1,\n",
       " 'meeting-place': 1,\n",
       " '1862': 1,\n",
       " 'konrad': 3,\n",
       " '450': 1,\n",
       " '2708': 2,\n",
       " 'shih': 5,\n",
       " 'as-sallal': 1,\n",
       " 'yanez': 1,\n",
       " 'night': 1,\n",
       " 'ara': 2,\n",
       " 'plenipotentiary': 563,\n",
       " 'admin': 1,\n",
       " 'cent': 52,\n",
       " 'replace': 23,\n",
       " 'programs': 6,\n",
       " 'two-year': 76,\n",
       " '1961-63': 1,\n",
       " 'vary': 5,\n",
       " 'concurrent': 3,\n",
       " '300': 8,\n",
       " 'petrochemistry': 1,\n",
       " 'lyceum': 4,\n",
       " 'commonwealth': 13,\n",
       " 'till': 1,\n",
       " 'lucia': 1,\n",
       " 'participant': 41,\n",
       " 'dec': 2,\n",
       " 'equally': 376,\n",
       " 'litterateur': 1,\n",
       " 'footing': 3,\n",
       " 'possible': 830,\n",
       " 'magha': 1,\n",
       " 'mutual': 593,\n",
       " 'homad': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = get_current_corpus().textacy_corpus\n",
    "corpus.word_freqs(normalize='lemma', weighting='count', as_strings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> Corpus Statistics<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> List of Most Frequent Words<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "ADDITIONAL_STOPWORDS = []\n",
    "\n",
    "def compute_list_of_most_frequent_words(\n",
    "    corpus,\n",
    "    gui,\n",
    "    group_by_column='signed_year',\n",
    "    parties=None,\n",
    "    target='lemma',\n",
    "    weighting='count',\n",
    "    include_pos=None,\n",
    "    stop_words=None,\n",
    "    display_score=False\n",
    "):\n",
    "    stop_words = stop_words or set()\n",
    "    \n",
    "    def include(token):\n",
    "        flag = True\n",
    "        if not include_pos is None:\n",
    "             flag = flag and token.pos_ in include_pos\n",
    "        flag = flag and token.lemma_ not in stop_words\n",
    "        return flag\n",
    "    \n",
    "    gui.progress.max = len(corpus)\n",
    "    \n",
    "    df_freqs = pd.DataFrame({ 'treaty_id': [], 'signed_year': [], 'token': [], 'score': [] })\n",
    "    \n",
    "    parties_set = set(parties or [])\n",
    "    \n",
    "    docs = corpus if len(parties_set) == 0 \\\n",
    "        else ( x for x in corpus if len(set((x.metadata['party1'], x.metadata['party2'])) & parties_set) > 0 )\n",
    "                                                   \n",
    "    for doc in docs:\n",
    "        \n",
    "        doc_freqs = textacy_utility.textacy_doc_to_bow(doc, target=target, weighting=weighting, as_strings=True, include=include)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'treaty_id': doc.metadata['treaty_id'],\n",
    "            'signed_year': int(doc.metadata['signed_year']),\n",
    "            'token': list(doc_freqs.keys()),\n",
    "            'score': list(doc_freqs.values())\n",
    "        })\n",
    "        \n",
    "        df_freqs = df_freqs.append(df)\n",
    "        gui.progress.value = gui.progress.value + 1\n",
    "        \n",
    "    df_freqs['signed_year'] = df_freqs.signed_year.astype(int)\n",
    "    \n",
    "    for key, group in TREATY_TIME_GROUPINGS.items():\n",
    "        if key in df_freqs.columns:\n",
    "            continue\n",
    "        df_freqs[key] = (group['fx'])(df_freqs)\n",
    "        \n",
    "    df_freqs['term'] = df_freqs.token # if True else df_freqs.token\n",
    "    \n",
    "    df_freqs = df_freqs.groupby([group_by_column, 'term']).sum().reset_index()[[group_by_column, 'term', 'score']]\n",
    "    \n",
    "    if display_score is True:\n",
    "        df_freqs['term'] = df_freqs.term + '*' + (df_freqs.score.apply('{:,.3f}'.format) if weighting == 'freq' else df_freqs.score.astype(str))\n",
    "        \n",
    "    df_freqs['position'] = df_freqs.sort_values(by=[group_by_column, 'score'], ascending=False).groupby([group_by_column]).cumcount() + 1\n",
    "    \n",
    "    gui.progress.value = 0\n",
    "    \n",
    "    return df_freqs\n",
    "    \n",
    "def display_list_of_most_frequent_words(gui, df):\n",
    "    if gui.output_type.value == 'table':\n",
    "        display(df)\n",
    "    elif gui.output_type.value == 'rank':\n",
    "        group_by_column = gui.group_by_column.value\n",
    "        df = df[df.position <= gui.n_tokens.value]\n",
    "        df_unstacked_freqs = df[[group_by_column, 'position', 'term']].set_index([group_by_column, 'position']).unstack()\n",
    "        display(df_unstacked_freqs)\n",
    "    else:\n",
    "        filename = '../data/word_trend_data.xlsx'\n",
    "        df.to_excel(filename)\n",
    "        print('Excel written: ' + filename)\n",
    "        \n",
    "def word_frequency_gui(wti_index, corpus, compute_callback, display_callback):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    include_pos_tags = [ 'ADJ', 'VERB', 'NUM', 'ADV', 'NOUN', 'PROPN' ]\n",
    "    weighting_options = { 'Count': 'count', 'Frequency': 'freq' }\n",
    "    normalize_options = { '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }\n",
    "    #pos_tags = DF_TAGSET[DF_TAGSET.POS.isin(include_pos_tags)].groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    #pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    counter = collections.Counter(corpus.word_freqs(normalize='lemma', weighting='count', as_strings=True))\n",
    "    default_include_pos = ['NOUN', 'PROPN']\n",
    "    frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100, include_pos=default_include_pos) ]\n",
    "\n",
    "    group_by_options = { TREATY_TIME_GROUPINGS[k]['title']: k for k in TREATY_TIME_GROUPINGS }\n",
    "    output_type_options = [ ( 'List', 'table' ), ( 'Rank', 'rank' ), ( 'Excel', 'excel' ), ]\n",
    "    ngrams_options = { '-': None, '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=lw('90%')),\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('200px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('200px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=None, layout=lw('200px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=lw('200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=lw('200px')),\n",
    "        weighting=widgets.Dropdown(description='Weighting', options=weighting_options, value='freq', layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=default_include_pos, rows=7, layout=lw('150px')),\n",
    "        stop_words=widgets.SelectMultiple(description='STOP', options=frequent_words, value=list([]), rows=7, layout=lw('200px')),\n",
    "        group_by_column=widgets.Dropdown(description='Group by', value='signed_year', options=group_by_options, layout=lw('200px')),\n",
    "        output_type=widgets.Dropdown(description='Output', value='rank', options=output_type_options, layout=lw('200px')),\n",
    "        n_tokens=widgets.IntSlider(description='#tokens', value=25, min=3, max=500, layout=lw('250px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('120px')),\n",
    "        display_score=widgets.ToggleButton(description='Display score', icon='check', value=False, layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.weighting,\n",
    "                gui.group_by_column,\n",
    "                gui.output_type,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.parties,\n",
    "                gui.party_preset,\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            gui.stop_words,\n",
    "            widgets.VBox([\n",
    "                gui.n_tokens,\n",
    "                gui.display_score,\n",
    "                gui.compute,\n",
    "            ], layout=widgets.Layout(align_items='flex-end')),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "    \n",
    "    def pos_change_handler(*args):\n",
    "        with gui.output:\n",
    "            gui.compute.disabled = True\n",
    "            selected = set(gui.stop_words.value)\n",
    "            frequent_words = [\n",
    "                x[0] for x in textacy_utility.get_most_frequent_words(\n",
    "                    corpus,\n",
    "                    100,\n",
    "                    normalize=gui.normalize.value,\n",
    "                    include_pos=gui.include_pos.value,\n",
    "                    weighting=gui.weighting.value\n",
    "                )\n",
    "            ]\n",
    "            gui.stop_words.options = frequent_words\n",
    "            selected = selected & set(gui.stop_words.options)\n",
    "            gui.stop_words.value = list(selected)\n",
    "            gui.compute.disabled = False\n",
    "        \n",
    "    gui.include_pos.observe(pos_change_handler, 'value')    \n",
    "    gui.weighting.observe(pos_change_handler, 'value')    \n",
    "    \n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            try:\n",
    "                gui.compute.disabled = True\n",
    "                df_freqs = compute_callback(\n",
    "                    corpus=corpus,\n",
    "                    gui=gui,\n",
    "                    target=gui.normalize.value,\n",
    "                    group_by_column=gui.group_by_column.value,\n",
    "                    parties=gui.parties.value,\n",
    "                    weighting=gui.weighting.value,\n",
    "                    include_pos=gui.include_pos.value,\n",
    "                    stop_words=set(gui.stop_words.value),\n",
    "                    display_score=gui.display_score.value\n",
    "                )\n",
    "                display_callback(gui, df_freqs)\n",
    "            finally:\n",
    "                gui.compute.disabled = False\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "                \n",
    "try:\n",
    "    word_frequency_gui(\n",
    "        WTI_INDEX,\n",
    "        get_current_corpus().textacy_corpus,\n",
    "        compute_callback=compute_list_of_most_frequent_words,\n",
    "        display_callback=display_list_of_most_frequent_words\n",
    "    )\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> Corpus and Document Sizes<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import attrs\n",
    "%matplotlib inline\n",
    "\n",
    "def compute_corpus_statistics(\n",
    "    data_folder,\n",
    "    wti_index,\n",
    "    container,\n",
    "    gui,\n",
    "    group_by_column='signed_year',\n",
    "    parties=None,\n",
    "    target='lemma',\n",
    "    include_pos=None,\n",
    "    stop_words=None\n",
    "):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "\n",
    "    value_columns = list(textacy_utility.POS_NAMES) if (len(include_pos or [])) == 0 else list(include_pos)\n",
    "    \n",
    "    documents = textacy_utility.get_corpus_documents(corpus)\n",
    "\n",
    "    if len(parties or []) > 0:\n",
    "        documents = documents[documents.party1.isin(parties)|documents.party2.isin(parties)]\n",
    "\n",
    "    documents['signed_lustrum'] = (documents.signed_year - documents.signed_year.mod(5)).astype(int) \n",
    "    documents['signed_decade'] = (documents.signed_year - documents.signed_year.mod(10)).astype(int)\n",
    "    documents['total'] = documents[value_columns].apply(sum, axis=1)\n",
    "\n",
    "    #documents = documents.groupby(group_by_column).agg(sum) #.reset_index()\n",
    "    aggregates = { x: ['sum'] for x in value_columns }\n",
    "    aggregates['total'] = ['sum', 'mean', 'min', 'max', 'size' ]\n",
    "    #if group_by_column != 'treaty_id':\n",
    "    documents = documents.groupby(group_by_column).agg(aggregates)\n",
    "    documents.columns = [ ('Total, ' + x[1].lower()) if x[0] == 'total' else x[0] for x in documents.columns ]\n",
    "    columns = sorted(value_columns) + sorted([ x for x in documents.columns if x.startswith('Total')])\n",
    "    return documents[columns]\n",
    "        \n",
    "def corpus_statistics_gui(data_folder, wti_index, container, compute_callback, display_callback):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    include_pos_tags =  list(textacy_utility.POS_NAMES)\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    counter = collections.Counter(corpus.word_freqs(normalize='lemma', weighting='count', as_strings=True))\n",
    "    frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100) ]\n",
    "\n",
    "    group_by_options = { TREATY_TIME_GROUPINGS[k]['title']: k for k in TREATY_TIME_GROUPINGS }\n",
    "    # output_type_options = [ ( 'Table', 'table' ), ( 'Pivot', 'pivot' ), ( 'Excel', 'excel' ), ]\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('180px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('200px')),\n",
    "        target=widgets.Dropdown(description='Normalize', options={ '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }, value='lemma', layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list([]), rows=7, layout=widgets.Layout(width='180px')),\n",
    "        group_by_column=widgets.Dropdown(description='Group by', value='signed_year', options=group_by_options, layout=lw('200px')),\n",
    "        #output_type=widgets.Dropdown(description='Output', value='table', options=output_type_options, layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute', layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.group_by_column,\n",
    "                #gui.output_type,\n",
    "                gui.party_preset,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.parties,\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "\n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df_freqs = compute_callback(\n",
    "                data_folder=data_folder,\n",
    "                wti_index=wti_index,\n",
    "                container=container,\n",
    "                gui=gui,\n",
    "                target=gui.target.value,\n",
    "                group_by_column=gui.group_by_column.value,\n",
    "                parties=gui.parties.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "            )\n",
    "            display_callback(gui, df_freqs)\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "\n",
    "def plot_simple(xs, ys, **figopts):\n",
    "    source = bokeh.models.ColumnDataSource(dict(x=xs, y=ys))\n",
    "    figopts = utility.extend(dict(title='', toolbar_location=\"right\"), figopts)\n",
    "    p = bokeh.plotting.figure(**figopts)\n",
    "    glyph = p.line(source=source, x='x', y='y', line_color=\"#b3de69\")\n",
    "    return p\n",
    "\n",
    "def display_corpus_statistics(gui, df):\n",
    "    display(df)\n",
    "    #with gui.output:\n",
    "    #    plotopts=dict(plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "    #    p = plot_simple(X.index, X['Total, mean'], **plotopts)\n",
    "    #    bokeh.plotting.show(p)\n",
    "\n",
    "try:\n",
    "    gui = corpus_statistics_gui(DATA_FOLDER, WTI_INDEX, get_current_corpus(), compute_callback=compute_corpus_statistics, display_callback=display_corpus_statistics)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display Named Entities\n",
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus, wti_index):\n",
    "    \n",
    "    def display_document_entities(corpus, treaty_id):\n",
    "        \n",
    "        doc = textacy_utility.get_treaty_doc(corpus, treaty_id)\n",
    "        \n",
    "        displacy.render(doc.spacy_doc, style='ent', jupyter=True)\n",
    "\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "            \n",
    "    treaty_ids = widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_document_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        treaty_id=treaty_ids\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        treaty_ids,\n",
    "        widgets.VBox([itw.children[-1]], layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    corpus = get_current_corpus().textacy_corpus\n",
    "    display_document_entities_gui(corpus, WTI_INDEX)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Compute or Load a Topic Model<span style='color: red; float: right'>MANDATORY RUN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>MODEL</span> Compute a new Topic Model<span style='color: red; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import topic_model_gui\n",
    "try:\n",
    "    TM_GUI_MODEL = topic_model_gui.display_topic_model_gui(get_current_corpus().textacy_corpus, DF_TAGSET)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>MODEL</span> Store or Load a Topic Model<span style='color: red; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import topic_model\n",
    "import topic_model_utility\n",
    "\n",
    "def get_persisted_model_paths():\n",
    "    return sorted([ x for x in glob.glob(os.path.join(DATA_FOLDER, '*.pickle')) ])\n",
    "\n",
    "def load_model(gui, model_container, *args):\n",
    "    \n",
    "    gui.output.clear_output()\n",
    "    \n",
    "    with gui.output:\n",
    "       \n",
    "        if gui.stored_path.value is None:\n",
    "            print(\"Please specify which model to load.\")\n",
    "            return\n",
    "\n",
    "        model_container.model = topic_model.load_model(gui.stored_path.value)\n",
    "    \n",
    "        topics = topic_model_utility.get_lda_topics(model_container.model.tm_model, n_tokens=20)\n",
    "        \n",
    "        display(topics)\n",
    "        \n",
    "        print('Model was loaded!')\n",
    "        \n",
    "def store_model(gui, model_container, *args):\n",
    "    \n",
    "    gui.output.clear_output()\n",
    "    \n",
    "    with gui.output:\n",
    "        if gui.identifier.value == '':\n",
    "            print(\"Please specify a unique identifier for the model.\")\n",
    "            return\n",
    "\n",
    "        if gui.identifier.value != utility.filename_whitelist(gui.identifier.value):\n",
    "            print(\"Please use ONLY valid filename characters in identifier.\")\n",
    "            return\n",
    "\n",
    "        filename = os.path.join(DATA_FOLDER, 'topic_model.pickle')\n",
    "        filename = utility.path_add_date(filename)\n",
    "        filename = utility.path_add_suffix(filename, gui.identifier.value)\n",
    "\n",
    "        topic_model.store_model(model_container.model, filename)\n",
    "\n",
    "        gui.stored_path.options = get_persisted_model_paths()\n",
    "        gui.stored_path.value = filename if filename in gui.stored_path.options else None\n",
    "        \n",
    "        print('Model stored in file {}'.format(filename))\n",
    "    \n",
    "def display_persist_topic_model_gui(model_container):\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        stored_path=widgets.Dropdown(description='Path', options=get_persisted_model_paths(), layout=widgets.Layout(width='40%')),\n",
    "        load=widgets.Button(description='Load', button_style='Success', layout=widgets.Layout(width='80px')),\n",
    "        store=widgets.Button(description='Store', button_style='Success', layout=widgets.Layout(width='80px')),\n",
    "        identifier=widgets.Text(description='Identifier', layout=widgets.Layout(width='300px')),\n",
    "        output=widgets.Output()\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        widgets.HBox([gui.stored_path, gui.load, gui.store, gui.identifier ]),\n",
    "        widgets.HBox([\n",
    "            widgets.Label(value=\"\", layout=widgets.Layout(width='40%')),\n",
    "            widgets.Label(value=\"Stored models will be named ./data/topic_model_yyyymmdd_$identifier$.pickle\", layout=widgets.Layout(width='40%')),\n",
    "        ]),\n",
    "        widgets.VBox([gui.output])\n",
    "    ])\n",
    "    \n",
    "    fx = lambda *args: load_model(gui, model_container, *args)\n",
    "    gui.load.on_click(fx)\n",
    "\n",
    "    fy = lambda *args: store_model(gui, model_container, *args)\n",
    "    gui.store.on_click(fy)\n",
    "    \n",
    "    display(boxes)\n",
    "\n",
    "if 'TM_GUI_MODEL' not in globals():\n",
    "    TM_GUI_MODEL = types.SimpleNamespace(\n",
    "        model=None\n",
    "    )\n",
    "    \n",
    "display_persist_topic_model_gui(TM_GUI_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Wordcloud<span style='color: red; float: right'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_wordcloud_gui(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    model = tm_data.tm_model\n",
    "    output_options = output_options or []\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(\n",
    "    tm_data,\n",
    "    topic_id=0,\n",
    "    n_words=100,\n",
    "    output_format='Wordcloud',\n",
    "    widget_container=None\n",
    "):\n",
    "    container = tm_data.compiled_data\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = container.topic_token_weights.loc[(container.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = topic_model_utility.get_topic_title(container.topic_token_weights, topic_id, n_words=n_words)\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = topic_model_utility.get_topic_tokens(container.topic_token_weights, topic_id=topic_id, n_words=n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    display_wordcloud_gui(display_wordcloud, tm_data, 'tx02', ['Wordcloud', 'Table'])\n",
    "except TopicModelNotComputed as ex:\n",
    "    logger.info(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Chart<span style='color: red; float: right'>TRY IT</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# Display topic's word distribution\n",
    "import numpy as np\n",
    "\n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(tokens)\n",
    "\n",
    "    p = bokeh.plotting.figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = bokeh.models.ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bokeh.models.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def display_topic_tokens(tm_data, topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = topic_model_utility.get_topic_tokens(container.topic_token_weights, topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        p = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        bokeh.plotting.show(p)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(tokens)\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "        \n",
    "    # Added code for missing method: widget_container.reset()\n",
    "    if 'progress' in widget_container.__dict__.keys():\n",
    "        widget_container.progress.value = 0\n",
    "    \n",
    "    \n",
    "def display_topic_distribution_widgets(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0),\n",
    "        word_count=widgets.IntSlider(description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2]),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "display_topic_distribution_widgets(display_topic_tokens, get_current_model(), 'wc01', ['Chart', 'Table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Trend Over Time or Documents<span style='color: red; float: right'>RUN</span>\n",
    "- Displays topic's share over documents.\n",
    "\n",
    "- BUGG? Values > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "import math\n",
    "\n",
    "def plot_topic_trend(df, category_column, value_column, x_label=None, y_label=None, **figopts):\n",
    "    \n",
    "    xs = df[category_column].astype(np.str)\n",
    "    ys = df[value_column]\n",
    "    \n",
    "    figopts = utility.extend(dict(title='', toolbar_location=\"right\"), figopts)\n",
    "    \n",
    "    p = bokeh.plotting.figure(**figopts)\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=ys, width=0.5, fill_color=\"#b3de69\")\n",
    "    \n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or category_column.title().replace('_', ' ')).title()\n",
    "    p.yaxis[0].axis_label = (y_label or value_column.title().replace('_', ' ')).title()\n",
    "    p.y_range.start = 0.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    \n",
    "    return p\n",
    "\n",
    "def display_topic_trend(\n",
    "    topic_id,\n",
    "    year,\n",
    "    year_aggregate,\n",
    "    gui,\n",
    "    output_format='Chart',\n",
    "    document_topic_weights=None,\n",
    "    topic_token_weights=None,\n",
    "    threshold=0.01\n",
    "):\n",
    "    figopts = dict(plot_width=1000, plot_height=700, title='', toolbar_location=\"right\")\n",
    "\n",
    "    tokens = topic_model_utility.get_topic_title(topic_token_weights, topic_id, n_words=200)\n",
    "    gui.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    \n",
    "    pivot_column = 'signed_year' if year is None else None\n",
    "    value_column = year_aggregate if year is None else 'weight'\n",
    "\n",
    "    df = document_topic_weights[(document_topic_weights.topic_id == topic_id)]\n",
    "    \n",
    "    if year is not None:\n",
    "        df = df[(df.signed_year == year)]\n",
    "        \n",
    "    df = df[(df.weight > threshold)].reset_index()\n",
    "    \n",
    "    if pivot_column is not None:\n",
    "        df = df.groupby([pivot_column, 'topic_id']).agg([np.mean, np.max])['weight'].reset_index()\n",
    "        df.columns = [pivot_column, 'topic_id', 'mean', 'max']\n",
    "        category_column = pivot_column\n",
    "        min_year = document_topic_weights.signed_year.min()\n",
    "        max_year = document_topic_weights.signed_year.max()\n",
    "        figopts['x_range'] = list(map(str, range(min_year, max_year+1))) # utility.complete_value_range(df[category_column].unique(), str)\n",
    "    else:\n",
    "        df['treaty'] = df.treaty_id + ' ' + df.party1 + ' ' + df.party2\n",
    "        category_column = 'treaty'\n",
    "        figopts['x_range'] = df['treaty'].unique()\n",
    "        \n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        p = plot_topic_trend(df, category_column, value_column, **figopts)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "def create_topic_trend_widgets(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    model_data = tm_data.compiled_data\n",
    "    document_topic_weights = tm_data.compiled_data.document_topic_weights\n",
    "    topic_token_weights = tm_data.compiled_data.topic_token_weights\n",
    "\n",
    "    year_options = [ ('all years', None) ] + [ (x,x) for x in range(model_data.year_period[0], model_data.year_period[1] + 1)]\n",
    "    \n",
    "    element_id = 'topic_share_plot'\n",
    "    gui = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=element_id,\n",
    "        text=widgets_config.text(dom_id=element_id),\n",
    "        year=widgets.Dropdown(description='Year', options=year_options, value=None),\n",
    "        year_aggregate=widgets.Dropdown(description='Aggregate', options=['mean', 'max'], value='max'),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=widgets.Dropdown(description='Format', options=['Chart', 'Table'], value='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "    )\n",
    "    \n",
    "    gui.prev_topic_id = gui.create_prev_id_button('topic_id', model.num_topics)\n",
    "    gui.next_topic_id = gui.create_next_id_button('topic_id', model.num_topics)\n",
    "    \n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=gui.topic_id,\n",
    "        year=gui.year,\n",
    "        year_aggregate=gui.year_aggregate,\n",
    "        gui=widgets.fixed(gui),\n",
    "        output_format=gui.output_format,\n",
    "        document_topic_weights=widgets.fixed(model_data.document_topic_weights),\n",
    "        topic_token_weights=widgets.fixed(model_data.topic_token_weights),\n",
    "        threshold=gui.threshold\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.text,\n",
    "        widgets.HBox([gui.prev_topic_id, gui.next_topic_id, gui.year, gui.year_aggregate, gui.output_format]),\n",
    "        widgets.HBox([gui.topic_id, gui.threshold, gui.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "create_topic_trend_widgets(tm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic to Document Network<span style='color: red; float: right'>TRY IT</span>\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "def plot_document_topic_network(network, layout, scale=1.0, titles=None):\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = bokeh.plotting.figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=tools)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(renderers=[r_topics], tooltips=None, callback=widgets_utility.wf.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(x='x', y='y', text='name', level='overlay', x_offset=0, y_offset=0, text_font_size='8pt')\n",
    "    \n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "        \n",
    "def display_document_topic_network(layout_algorithm, tm_data, threshold=0.10, parties=None, period=None, ignores=None, scale=1.0, output_format='network', tick=utility.noop):\n",
    "\n",
    "    tick(1)\n",
    "    \n",
    "    container = tm_data.compiled_data\n",
    "    \n",
    "    titles = topic_model_utility.get_topic_titles(container.topic_token_weights)\n",
    "\n",
    "    df = container.document_topic_weights[container.document_topic_weights.weight > threshold].reset_index()\n",
    "\n",
    "    if len(parties or []) > 0:\n",
    "        df = df[df.party1.isin(parties) | df.party2.isin(parties)]\n",
    "\n",
    "    if len(period or []) == 2:\n",
    "        df = df[(df.signed_year>=period[0]) & (df.signed_year<=period[1])]\n",
    "        \n",
    "    if len(ignores or []) > 0:\n",
    "        df = df[~df.topic_id.isin(ignores)]\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print('No data')\n",
    "        return\n",
    "    \n",
    "    df['title'] = df.treaty_id + ' ' + df.party1 + ' ' + df.party2\n",
    "\n",
    "    network = NetworkUtility.create_bipartite_network(df, 'title', 'topic_id')\n",
    "    tick()\n",
    "\n",
    "    if output_format == 'network':\n",
    "        args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "        layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "        tick()\n",
    "        p = plot_document_topic_network(network, layout, scale=scale, titles=titles)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "    elif output_format == 'table':\n",
    "        display(df)\n",
    "\n",
    "    tick(0)\n",
    "        \n",
    "def document_topic_network_gui(wti_index, tm_data):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    text_id = 'nx_id1'\n",
    "    layout_options = [ 'Circular', 'Kamada-Kawai', 'Fruchterman-Reingold']\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    year_min, year_max = tm_data.compiled_data.year_period\n",
    "    \n",
    "    n_topics = tm_data.tm_model.num_topics\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        text=widgets_utility.wf.create_text_widget(text_id),\n",
    "        period=widgets.IntRangeSlider(description='Time', min=year_min, max=year_max, step=1, value=(year_min, year_max), continues_update=False),\n",
    "        scale=widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False),\n",
    "        output_format=widgets_utility.dropdown('Output', { 'Network': 'network', 'Table': 'table' }, 'network', layout=lw('200px')),\n",
    "        layout=widgets_utility.dropdown('Layout', layout_options, 'Fruchterman-Reingold', layout=lw('250px')),\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('180px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('180px')),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"99%\")),\n",
    "        ignores=widgets.SelectMultiple(description='Ignore', options=[('', None)] + [ ('Topic #'+str(i), i) for i in range(0, n_topics) ], value=[], rows=8, layout=lw('180px')),\n",
    "    )\n",
    "    \n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "        \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "    \n",
    "    iw = widgets.interactive(\n",
    "        display_document_topic_network,\n",
    "        layout_algorithm=gui.layout,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=gui.threshold,\n",
    "        parties=gui.parties,\n",
    "        period=gui.period,\n",
    "        ignores=gui.ignores,\n",
    "        scale=gui.scale,\n",
    "        output_format=gui.output_format,\n",
    "        tick=widgets.fixed(tick)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.text,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([gui.layout, gui.threshold, gui.scale, gui.period]), \n",
    "            widgets.VBox([gui.parties, gui.party_preset]), \n",
    "            widgets.VBox([gui.ignores]), \n",
    "            widgets.VBox([gui.output_format, gui.progress]),\n",
    "        ]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    document_topic_network_gui(WTI_INDEX, tm_data)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Topic Trends Overview<span style='color: red; float: right'>TRY IT</span>\n",
    "\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanford’s Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "import bokeh.transform\n",
    "\n",
    "def isint(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def get_topic_weight_by_year_or_document(document_topic_weights, key='mean', year=None):\n",
    "    pivot_column = 'year' if year is None else 'document_id'\n",
    "    #if df[(df.year == year)]\n",
    "    df = self.get_document_topic_weights(year) \\\n",
    "        .groupby([pivot_column,'topic_id']) \\\n",
    "        .agg(config.AGGREGATES[key])[['weight']].reset_index()\n",
    "    return df, pivot_column\n",
    "    \n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = ['#ffffff', '#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b']\n",
    "    mapper = bokeh.models.LinearColorMapper(palette=colors, low=0.0, high=1.0) # low=df.weight.min(), high=max_weight)\n",
    "    color_transform = bokeh.transform.transform('weight', mapper)\n",
    "    color_bar = bokeh.models.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bokeh.models.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bokeh.models.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def compute_int_range_categories(values):\n",
    "    categories = values.unique()\n",
    "    if all(map(utility.isint, categories)):\n",
    "        categories = sorted(list(map(int, categories)))\n",
    "        return list(map(str, categories))\n",
    "    else:\n",
    "        return sorted(list(categories))\n",
    "\n",
    "HEATMAP_FIGOPTS = dict(title=\"Topic heatmap\", toolbar_location=\"right\",  x_axis_location=\"above\", plot_width=1000)\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, titles, text_id, **figopts):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "\n",
    "    x_range = compute_int_range_categories(df[xs])\n",
    "    y_range = compute_int_range_categories(df[ys])\n",
    "    \n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = bokeh.models.ColumnDataSource(df)\n",
    "\n",
    "    if x_range is not None:\n",
    "        figopts['x_range'] = x_range\n",
    "\n",
    "    if y_range is not None:\n",
    "        figopts['y_range'] = y_range\n",
    "        figopts['plot_height'] = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = bokeh.plotting.figure(**figopts)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"8pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(tooltips=None, callback=widgets_utility.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def display_doc_topic_heatmap(model_data, key='max', flip_axis=False, glyph='Circle', year=None, year_aggregate=None, output_format=None):\n",
    "    try:\n",
    "\n",
    "        titles = topic_model_utility.get_topic_titles(model_data.topic_token_weights, n_words=100)\n",
    "        \n",
    "        df = model_data.document_topic_weights.copy()\n",
    "\n",
    "        if year is not None:\n",
    "            df = df[(df.signed_year == year)]\n",
    "\n",
    "        if year is None:\n",
    "            \n",
    "            ''' Display aggregate value grouped by year  '''\n",
    "            df = df.groupby(['signed_year', 'topic_id']).agg([np.mean, np.max])['weight'].reset_index()\n",
    "            df.columns = ['signed_year', 'topic_id', 'mean', 'max']\n",
    "            df['weight'] = df[year_aggregate]\n",
    "            df['signed_year'] = df.signed_year.astype(str)\n",
    "            category_column = 'signed_year'\n",
    "            \n",
    "        else:\n",
    "            ''' Display individual treaties for selected year  '''\n",
    "            df['treaty'] = df.treaty_id + ' ' + df.party1 + ' ' + df.party2\n",
    "            df = df[['treaty', 'treaty_id', 'topic_id', 'weight']]\n",
    "            category_column = 'treaty'  \n",
    "        \n",
    "        df['document_id'] = df.index.astype(str)\n",
    "        df['topic_id'] = df.topic_id.astype(str)\n",
    "         \n",
    "        if output_format.lower() == 'heatmap':\n",
    "            \n",
    "            p = plot_topic_relevance_by_year(\n",
    "                df,\n",
    "                xs=category_column,\n",
    "                ys='topic_id',\n",
    "                flip_axis=flip_axis,\n",
    "                titles=titles,\n",
    "                text_id='topic_relevance',\n",
    "                **HEATMAP_FIGOPTS)\n",
    "\n",
    "            bokeh.plotting.show(p)\n",
    "            \n",
    "        else:\n",
    "            display(df)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        logger.error(ex)\n",
    "        \n",
    "def doc_topic_heatmap_gui(model_data):\n",
    "\n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    text_id = 'topic_relevance'\n",
    "    \n",
    "    def text_widget(element_id=None, default_value='', style='', line_height='20px'):\n",
    "        value = \"<span class='{}' style='line-height: {};{}'>{}</span>\".format(element_id, line_height, style, default_value) if element_id is not None else ''\n",
    "        return widgets.HTML(value=value, placeholder='', description='', layout=widgets.Layout(height='150px'))\n",
    "    \n",
    "    year_options = [ ('all years', None) ] + [ (x,x) for x in range(model_data.year_period[0], model_data.year_period[1] + 1)]\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        text_id=text_id,\n",
    "        text=text_widget(text_id),\n",
    "        flip_axis=widgets.ToggleButton(value=True, description='Flip', icon='', layout=lw(\"80px\")),\n",
    "        year=widgets.Dropdown(description='Year', options=year_options, value=None, layout=lw(\"160px\")),\n",
    "        year_aggregate=widgets.Dropdown(description='Aggregate', options=['mean', 'max'], value='max', layout=lw(\"160px\")),\n",
    "        output_format=widgets.Dropdown(description='Output', options=['Heatmap', 'Table'], value='Heatmap', layout=lw(\"180px\"))\n",
    "    )\n",
    "    \n",
    "    iw = widgets.interactive(\n",
    "        display_doc_topic_heatmap,\n",
    "        model_data=widgets.fixed(model_data),\n",
    "        flip_axis=gui.flip_axis,\n",
    "        year=gui.year,\n",
    "        year_aggregate=gui.year_aggregate,\n",
    "        output_format=gui.output_format\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.year, gui.year_aggregate, gui.output_format, gui.flip_axis ]),\n",
    "        widgets.HBox([iw.children[-1]]), gui.text\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "try:\n",
    "    doc_topic_heatmap_gui(get_current_model().compiled_data)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Topic Cooccurrence<span style='color: red; float: right'>TRY IT</span>\n",
    "\n",
    "Computes weighted graph of topics co-occurring in the same document. Topics are defined as co-occurring if they both exists  in the same document both having weights above threshold. Weight are number of co-occurrences (binary yes or no). Node size reflects topic proportions over the entire corpus (normalized document) length, and are computed in accordance to how node sizes are computed in LDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize topic co-occurrence\n",
    "\n",
    "import common.plot_utility as plot_utility\n",
    "import common.network_utility as network_utility\n",
    "import bokeh.plotting # import figure, show, output_notebook, output_file\n",
    "\n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "def get_topic_titles(topic_token_weights, topic_id=None, n_words=100):\n",
    "    df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id==topic_id)]\n",
    "    df = df_temp\\\n",
    "            .sort_values('weight', ascending=False)\\\n",
    "            .groupby('topic_id')\\\n",
    "            .apply(lambda x: ' '.join(x.token[:n_words].str.title()))\n",
    "    return df\n",
    "\n",
    "# FIXME: add doc token length to df_documents\n",
    "def get_topic_proportions(corpus_documents, document_topic_weights):\n",
    "    topic_proportion = topic_model.compute_topic_proportions(document_topic_weights, corpus_documents)\n",
    "    return topic_proportion\n",
    "    \n",
    "def display_topic_co_occurrence_network(\n",
    "    tm_data,\n",
    "    parties=None,\n",
    "    period=None,\n",
    "    ignores=None,\n",
    "    threshold=0.10,\n",
    "    layout='Fruchterman-Reingold',\n",
    "    scale=1.0,\n",
    "    output_format='table'\n",
    "):\n",
    "    try:\n",
    "        \n",
    "        model_data = tm_data.compiled_data\n",
    "        \n",
    "        titles = topic_model_utility.get_topic_titles(model_data.topic_token_weights)\n",
    "        df = model_data.document_topic_weights\n",
    "        df['document_id'] = df.index\n",
    "        \n",
    "        node_sizes = topic_model.compute_topic_proportions(df, model_data.documents)\n",
    "\n",
    "        if ignores is not None:\n",
    "            df = df[~df.topic_id.isin(ignores)]\n",
    "            \n",
    "        if len(parties or []) > 0:\n",
    "            df = df[df.party1.isin(parties) | df.party2.isin(parties)]\n",
    "            \n",
    "        if period is not None:\n",
    "            df = df[df.signed_year.between(period[0], period[1], inclusive=True)]\n",
    "            \n",
    "        df = df.loc[(df.weight >= threshold)]\n",
    "        df = pd.merge(df, df, how='inner', left_on='document_id', right_on='document_id')\n",
    "        df = df.loc[(df.topic_id_x < df.topic_id_y)]\n",
    "        df = df.groupby([df.topic_id_x, df.topic_id_y]).size().reset_index()\n",
    "        df.columns = ['source', 'target', 'weight']\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print('No data. Please change selections.')\n",
    "            return\n",
    "        \n",
    "        if output_format == 'table':\n",
    "            display(df)\n",
    "        else:\n",
    "            network = network_utility.NetworkUtility.create_network(df, source_field='source', target_field='target', weight='weight')\n",
    "            p = plot_utility.PlotNetworkUtility.plot_network(\n",
    "                network=network,\n",
    "                layout_algorithm=layout,\n",
    "                scale=scale,\n",
    "                threshold=0.0,\n",
    "                node_description=titles,\n",
    "                node_proportions=node_sizes,\n",
    "                weight_scale=10.0,\n",
    "                normalize_weights=True,\n",
    "                element_id='cooc_id',\n",
    "                figsize=(900,500)\n",
    "            )\n",
    "            bokeh.plotting.show(p)\n",
    "\n",
    "    except Exception as x:\n",
    "        raise\n",
    "        print(\"No data: please adjust filters\")\n",
    "\n",
    "def topic_coocurrence_network_gui(wti_index, tm_data):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    n_topics = tm_data.tm_model.num_topics\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    text_id = 'cooc_id'\n",
    "    layout_options = [ 'Circular', 'Kamada-Kawai', 'Fruchterman-Reingold']\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    year_min, year_max = tm_data.compiled_data.year_period\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        n_topics=n_topics,\n",
    "        text=widgets_utility.wf.create_text_widget(text_id),\n",
    "        period=widgets.IntRangeSlider(description='Time', min=year_min, max=year_max, step=1, value=(year_min, year_max), continues_update=False),\n",
    "        scale=widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.20, continues_update=False),\n",
    "        output_format=widgets_utility.dropdown('Output', { 'Network': 'network', 'Table': 'table' }, 'network', layout=lw('200px')),\n",
    "        layout=widgets_utility.dropdown('Layout', layout_options, 'Fruchterman-Reingold', layout=lw('250px')),\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('180px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('180px')),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"99%\")),\n",
    "        ignores=widgets.SelectMultiple(description='Ignore', options=[('', None)] + [ ('Topic #'+str(i), i) for i in range(0, n_topics) ], value=[], rows=8, layout=lw('180px')),\n",
    "    )\n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "        \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "     \n",
    "    iw = widgets.interactive(\n",
    "        display_topic_co_occurrence_network,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        parties=gui.parties,\n",
    "        period=gui.period,\n",
    "        ignores=gui.ignores,\n",
    "        threshold=gui.threshold,\n",
    "        layout=gui.layout,\n",
    "        scale=gui.scale,\n",
    "        output_format=gui.output_format\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        gui.text,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([gui.layout, gui.threshold, gui.scale, gui.period]), \n",
    "            widgets.VBox([gui.parties, gui.party_preset]), \n",
    "            widgets.VBox([gui.ignores]), \n",
    "            widgets.VBox([gui.output_format, gui.progress]),\n",
    "        ]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "    \n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    topic_coocurrence_network_gui(WTI_INDEX, tm_data)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alignment.alignment import Needleman, Hirschberg\n",
    "\n",
    "corpus = get_current_corpus().textacy_corpus\n",
    "\n",
    "SIMILARITY_SKIP_POS = set([spacy.symbols.SPACE, spacy.symbols.NUM ])\n",
    "SIMILARITY_INCLUDE_POS = set([spacy.symbols.NOUN])\n",
    "\n",
    "def similarity_prepare_token(x):\n",
    "    \n",
    "    if x.pos == spacy.symbols.NOUN:\n",
    "        return x.lemma_\n",
    "    \n",
    "    return x.pos_\n",
    "\n",
    "def similarity_prepare_doc(doc, include_pos=None, min_length=2):\n",
    "    tokens = doc\n",
    "    tokens = ( x for x in tokens if x.pos not in SIMILARITY_SKIP_POS )\n",
    "    tokens = ( x for x in tokens if x.is_alpha and not x.is_stop and len(x) >= min_length  )\n",
    "    if include_pos is not None: \n",
    "        tokens = ( x for x in tokens if x.pos in include_pos  )\n",
    "        \n",
    "    return [ similarity_prepare_token(x) for x in tokens ]\n",
    "\n",
    "seqa = similarity_prepare_doc(corpus[56], include_pos=[spacy.symbols.NOUN], min_length=4)\n",
    "seqb = similarity_prepare_doc(corpus[62], include_pos=[spacy.symbols.NOUN], min_length=4)\n",
    "\n",
    "# Align using Needleman-Wunsch algorithm.\n",
    "n = Needleman()\n",
    "a,b = n.align(seqa, seqb)\n",
    "\n",
    "#l = min(len(a), len(b))\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Doc #1 ' + corpus[56].metadata['treaty_id']: a[:l],\n",
    "        'Doc #2 ' + corpus[62].metadata['treaty_id']: b[:l]\n",
    "        #corpus[0].metadata['treaty_id']: [ corpus.spacy_vocab[int(x)].lower if x != '|' else 0 for x in a[:l] ],\n",
    "        #corpus[1].metadata['treaty_id']: [ corpus.spacy_vocab[int(x)].lower if x != '|' else 0 for x in b[:l] ]\n",
    "    }\n",
    ")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.spacy_vocab [0].lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = Hirschberg()\n",
    "scores = []\n",
    "for i in range(1, len(corpus)-1):\n",
    "    for j in range(i+1, len(corpus)):\n",
    "        seqa = similarity_prepare_doc(corpus[i], include_pos=[spacy.symbols.NOUN], min_length=4)\n",
    "        seqb = similarity_prepare_doc(corpus[j], include_pos=[spacy.symbols.NOUN], min_length=4)\n",
    "        score = n.score(seqa, seqb)\n",
    "        #print('Treaty #1: {} Treaty #2 {} Score: {}'.format(corpus[i].metadata['treaty_id'], corpus[j].metadata['treaty_id'], score))\n",
    "        scores.append( {\n",
    "            'treaty_1': corpus[i].metadata['treaty_id'],\n",
    "            'treaty_2': corpus[j].metadata['treaty_id'],\n",
    "            'score': score\n",
    "        })\n",
    "df = pd.DataFrame(scores)\n",
    "df.to_excel('hirschberg_scores_lemma_noun.xlsx')\n",
    "#display(df.sort_values(['score'], ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
