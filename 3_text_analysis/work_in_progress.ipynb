{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> CO-occurrence Matrix<span style='color: red; float: right'>WORK IN PROGRESS</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#corpus = get_current_corpus().textacy_corpus\n",
    "\n",
    "nlp = textacy.load_spacy('en')\n",
    "text = \"The Horse Raced Past the Barn Fell\"\n",
    "\n",
    "corpus = textacy.Corpus(nlp)\n",
    "corpus.add_text(text)\n",
    "    \n",
    "term_args = dict(\n",
    "    args=dict(\n",
    "        #ngrams=gui.ngrams.value,\n",
    "        #named_entities=gui.named_entities.value,\n",
    "        normalize='lemma',\n",
    "        as_strings=True\n",
    "    ),\n",
    "    kwargs=dict(\n",
    "        filter_nums=True,\n",
    "        drop_determiners=True,\n",
    "        min_freq=1,\n",
    "        include_pos=['NOUN'],\n",
    "        filter_stops=True,\n",
    "        filter_punct=True\n",
    "    ),\n",
    "    extra_stop_words=None\n",
    ")\n",
    "terms_iter = lambda: ( textacy_utility.textacy_filter_terms(doc, term_args) for doc in corpus )\n",
    "\n",
    "#vocab = corpus.spacy_vocab\n",
    "#id2token = { x.lower: x.lower_ for x in vocab }\n",
    "#dictionary.token2id = { x.lower_: x.lower for x in vocab }\n",
    "dictionary = gensim.corpora.Dictionary(terms_iter())\n",
    "_ = dictionary[0]\n",
    "X = gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator(dictionary.id2token, dictionary)\n",
    "X.accumulate(terms_iter(), window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-14 14:59:45,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-12-14 14:59:45,962 : INFO : built Dictionary(6 unique tokens: ['of', 'the', 'concept', 'basic', 'word']...) from 1 documents (total 7 corpus positions)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6716205aeebf46ceb814bf47e445955f",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "nlp = textacy.load_spacy('en')\n",
    "text = \"The basic concept of the word association\".lower()\n",
    "\n",
    "corpus = textacy.Corpus(nlp)\n",
    "corpus.add_text(text)\n",
    "fx = lambda: ((x.orth_ for x in doc) for doc in corpus)\n",
    "dictionary = gensim.corpora.Dictionary(fx())\n",
    "_ = dictionary[0]\n",
    "X = gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator(dictionary.id2token, dictionary)\n",
    "X.accumulate(fx(), window_size=2)\n",
    "coo = X._co_occurrences.tocoo()\n",
    "df = pd.DataFrame({'word_id1': coo.row, 'word_id2': coo.col, 'count': coo.data})\n",
    "#df = df[df.word_id1 > df.word_id2]\n",
    "\n",
    "df['word1'] = df.word_id1.apply(lambda x: dictionary[x])\n",
    "df['word2'] = df.word_id2.apply(lambda x: dictionary[x])\n",
    "df['count1'] = df.word_id1.apply(lambda x: X._occurrences[x])\n",
    "df['count2'] = df.word_id2.apply(lambda x: X._occurrences[x])\n",
    "\n",
    "df = df[['word_id1', 'word_id2', 'word1', 'word2', 'count', 'count1', 'count2']].sort_values(['word1'], ascending=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def corpus_subset(corpus, px):\n",
    "    iter1, iter2 = itertools.tee(corpus.get(px))\n",
    "    docs = (d.spacy_doc for d in iter1)\n",
    "    metadatas = (d.metadata for d in iter2)\n",
    "    sub_corpus = textacy.Corpus(lang=corpus.spacy_lang, docs=docs, metadatas=metadatas)\n",
    "    return sub_corpus\n",
    "\n",
    "def fpx(year):\n",
    "    return lambda x: int(x.metadata['signed_year']) == year\n",
    "\n",
    "#data = corpus_subset(corpus, fpx(1947)).word_freqs(weighting='freq', as_strings=True)\n",
    "#df = pd.DataFrame({'key': list(data.keys()),  'weight': list(data.values()) })\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from glove import Corpus\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# See http://www.foldl.me/2014/glove-python/\n",
    "#def compute_GloVe_df(sentences, window=2, dictionary=None):\n",
    "#    \n",
    "#    corpus = Corpus(dictionary=dictionary)\n",
    "#    corpus.fit(sentences, window=window)\n",
    "\n",
    "#    dm = corpus.matrix.todense()\n",
    "#    inverse_dictionary = { i: w for w, i in corpus.dictionary.items() }\n",
    "#    id2token = [ inverse_dictionary[i] for i in range(0,max(inverse_dictionary.keys())+1)]\n",
    "\n",
    "#    df = pd.DataFrame(dm.T, columns=id2token).assign(word=id2token).set_index('word')\n",
    "#    return df\n",
    "\n",
    "#https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.nonzero.html\n",
    "    \n",
    "def _coo_to_sparse_series(A, dense_index=False):\n",
    "    \"\"\" Convert a scipy.sparse.coo_matrix to a SparseSeries.\n",
    "    Use the defaults given in the SparseSeries constructor. \"\"\"\n",
    "    # A = scipy.sparse.triu(A)\n",
    "    s = pd.Series(A.data, pd.MultiIndex.from_arrays((A.row, A.col)))\n",
    "    s = s.sort_index()\n",
    "    s = s.to_sparse()\n",
    "    return s\n",
    "\n",
    "term_args = dict(\n",
    "    args=dict(\n",
    "        ngrams=1,\n",
    "        named_entities=False,\n",
    "        normalize='lemma',\n",
    "        as_strings=True\n",
    "    ),\n",
    "    kwargs=dict(\n",
    "        filter_stops=True,\n",
    "        filter_punct=True,\n",
    "        filter_nums=True,\n",
    "        min_freq=1,\n",
    "        drop_determiners=True,\n",
    "        include_pos=('NOUN', 'PROPN', )\n",
    "    )\n",
    ")\n",
    "\n",
    "stream = (textacy_filter_terms(doc, term_args) for doc in CORPUS)\n",
    "#stream = (' '.join(list(textacy_filter_terms(doc, term_args))) for doc in CORPUS)\n",
    "\n",
    "glove_co_matrix = Corpus() #dictionary=None)\n",
    "\n",
    "docs = (list(textacy_filter_terms(doc, term_args)) for doc in CORPUS)\n",
    "glove_co_matrix.fit(docs, window=5)\n",
    "\n",
    "dictionary = glove_co_matrix.dictionary\n",
    "co_series = _coo_to_sparse_series(glove_co_matrix.matrix)\n",
    "co_df = co_series.to_dense().reset_index().rename(columns={ 'level_0': 'token_id_x', 'level_1': 'token_id_y', 0: 'weight'})\n",
    "id2token = pd.DataFrame({ 'token_id': list(dictionary.values()), 'token': list(dictionary.keys()) }).set_index('token_id')\n",
    "\n",
    "#https://stackoverflow.com/questions/34181494/populate-a-pandas-sparsedataframe-from-a-scipy-sparse-coo-matrix\n",
    "\n",
    "df = pd.merge(co_df, id2token, left_on='token_id_x', right_index=True, how='inner').rename(columns={'token': 'token_x'})\n",
    "df = pd.merge(df,    id2token, left_on='token_id_y', right_index=True, how='inner').rename(columns={'token': 'token_y'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'row': glove_co_matrix.matrix.row, 'col': glove_co_matrix.matrix.col}).groupby(['row', 'col']).size().nlargest()\n",
    "#glove_co_matrix.matrix.tocoo()\n",
    "#df.nlargest(100, ['weight'])\n",
    "#df.groupby(['token_id_x', 'token_id_y']).size().nlargest()\n",
    "#co_series.reset_index().groupby(['level_0', 'level_1']).size().nlargest()\n",
    "dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS AND WORK IN PROGRESS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_current_corpus().textacy_corpus\n",
    "term_args = {\n",
    "    'mask_gpe': True,\n",
    "    'kwargs': {'filter_stops': True, 'filter_punct': True, 'include_pos': ('NOUN', 'PROPN'), 'min_freq': 2},\n",
    "    'extra_stop_words': set(),\n",
    "    'args': {'normalize': 'lemma', 'named_entities': False, 'ngrams': [1], 'as_strings': True},\n",
    "    'min_freq': 2,\n",
    "    'max_doc_freq': 0.8\n",
    "}\n",
    "fx_terms = lambda: textacy_utility.extract_corpus_terms(corpus, term_args)\n",
    "\n",
    "for x in fx_terms():\n",
    "    print(len([y for y in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([ {\n",
    "    'n_topics': int(model.tm_model.num_topics),\n",
    "    'perplexity_score': model.perplexity_score,\n",
    "    'coherence_score': model.coherence_score\n",
    "  } for model in models ])\n",
    "df['n_topics'] = df.n_topics.astype(int)\n",
    "df = df.set_index('n_topics')\n",
    "df['coherence_score'].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame Corpus from spaCy Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Corpus vs WTI Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_documents = corpus.documents.set_index(['treaty_id', 'language'])\n",
    "treaty_text_languages = wti_index.get_treaty_text_languages().set_index(['treaty_id', 'language'])\n",
    "\n",
    "treaties_in_corpus_not_in_wti = corpus_documents.index.difference(treaty_text_languages.index).get_values()\n",
    "treaties_in_wti_not_in_corpus = treaty_text_languages.index.difference(corpus_documents.index).get_values()\n",
    "\n",
    "print(  'Found in corpus, but not in WTI: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_corpus_not_in_wti ]))\n",
    "\n",
    "print(  'Found in WTI, but not in corpus: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_wti_not_in_corpus ]))\n",
    "\n",
    "#corpus_documents.loc[corpus_text_not_in_wti]\n",
    "#treaty_text_languages.loc[wti_not_in_corpus]\n",
    "\n",
    "#wti_not_in_corpus\n",
    "\n",
    "# Duplicates:\n",
    "#corpus_documents.index.get_duplicates()\n",
    "#treaty_text_languages.corpus_documents.index.get_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Basic Corpus Statistics\n",
    "See https://www.nltk.org/book/ch01.html\n",
    "\n",
    "* Size of treaties over time\n",
    "* Unique word, unique words per word class\n",
    "* Lexical diversity\n",
    "* Frequency distribution\n",
    "* Average word length, sentence length\n",
    "\n",
    "\n",
    "```python\n",
    " \n",
    "len(texts) / count(docs)\n",
    "0.06230453042623537\n",
    "len(set(text3)) / len(text3)\n",
    "0.06230453042623537\n",
    "def lexical_diversity(text): [1]\n",
    "    return len(set(text)) / len(text) [2]\n",
    "\n",
    "def percentage(count, total): [3]\n",
    "    return 100 * count / total\n",
    "#### Most common words\n",
    "fdist1 = FreqDist(text1)\n",
    "fdist1.most_common(50)\n",
    "#### Word length frequencies\n",
    "fdist = FreqDist(len(w) for w in text1)  [2]\n",
    "print(fdist)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code \n",
    "\n",
    "corpus = None\n",
    "def display_token_toplist_interact(source_folder):\n",
    "    global corpus\n",
    "    progress_widget = None\n",
    "    \n",
    "    def display_token_toplist(source_folder, language, statistics='', remove_stopwords=False):\n",
    "        global corpus\n",
    "\n",
    "        try:\n",
    "\n",
    "            progress_widget.value = 1\n",
    "\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0]).load_mm_corpus()\n",
    "\n",
    "            progress_widget.value = 2\n",
    "            service = MmCorpusStatisticsService(corpus, dictionary=corpus.dictionary, language=language)\n",
    "\n",
    "            print(\"Corpus consists of {} documents, {} words in total and a vocabulary size of {} tokens.\"\\\n",
    "                      .format(len(corpus), corpus.dictionary.num_pos, len(corpus.dictionary)))\n",
    "\n",
    "            progress_widget.value = 3\n",
    "            if statistics == 'word_freqs':\n",
    "                display(service.compute_word_frequencies(remove_stopwords))\n",
    "            elif statistics == 'documents':\n",
    "                display(service.compute_document_stats())\n",
    "            elif statistics == 'word_count':\n",
    "                display(service.compute_word_stats())\n",
    "            else:\n",
    "                print('Unknown: ' + statistics)\n",
    "\n",
    "        except Exception as ex:\n",
    "            logger.error(ex)\n",
    "\n",
    "        progress_widget.value = 5\n",
    "        progress_widget.value = 0\n",
    "        return corpus\n",
    "    \n",
    "    language_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    statistics_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'Word freqs': 'word_freqs',\n",
    "            'Documents': 'documents',\n",
    "            'Word count': 'word_count'\n",
    "        },\n",
    "        value='word_count',\n",
    "        description='Statistics:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    remove_stopwords_widget=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist'\n",
    "    )\n",
    "    \n",
    "    progress_widget=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    "\n",
    "    wi = widgets.interactive(\n",
    "        display_token_toplist,\n",
    "        source_folder=source_folder,\n",
    "        language=language_widget,\n",
    "        statistics=statistics_widget,\n",
    "        remove_stopwords=remove_stopwords_widget\n",
    "    )\n",
    "\n",
    "    boxes = widgets.HBox(\n",
    "        [\n",
    "            language_widget, statistics_widget, remove_stopwords_widget, progress_widget\n",
    "        ]\n",
    "    )\n",
    "    display(widgets.VBox([boxes, wi.children[-1]]))\n",
    "    wi.update()\n",
    "\n",
    "display_token_toplist_interact('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Treaty Keyword Extraction (using TF-IDF weighing)\n",
    "- [ML Wiki.org](http://mlwiki.org/index.php/TF-IDF)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- Spärck Jones, K. (1972). \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\".\n",
    "- Manning, C.D.; Raghavan, P.; Schutze, H. (2008). \"Scoring, term weighting, and the vector space model\". ([PDF](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf))\n",
    "- https://markroxor.github.io/blog/tfidf-pivoted_norm/\n",
    "$\\frac{tf-idf}{\\sqrt(rowSums( tf-idf^2 ) )}$\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/pivoted-normalized-document-length-1.html\n",
    "\n",
    "Neural Network Methods in Natural Language Processing, Yoav Goldberg:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from scipy.sparse import csr_matrix\n",
    "%timeit\n",
    "\n",
    "def get_top_tfidf_words(data, n_top=5):\n",
    "    top_list = data.groupby(['treaty_id'])\\\n",
    "        .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "        .reset_index(level=0, drop=True)\n",
    "    return top_list\n",
    "\n",
    "def compute_tfidf_scores(corpus, dictionary, smartirs='ntc'):\n",
    "    #model = gensim.models.logentropy_model.LogEntropyModel(corpus, normalize=True)\n",
    "    model = gensim.models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary, normalize=True) #, smartirs=smartirs)\n",
    "    rows, cols, scores = [], [], []\n",
    "    for r, document in enumerate(corpus): \n",
    "        vector = model[document]\n",
    "        c, v = zip(*vector)\n",
    "        rows += (len(c) * [ int(r) ])\n",
    "        cols += c\n",
    "        scores += v\n",
    "        \n",
    "    return csr_matrix((scores, (rows, cols)))\n",
    "    \n",
    "if True: #'tfidf_cache' not in globals():\n",
    "    tfidf_cache = {\n",
    "    }\n",
    "    \n",
    "def display_tfidf_scores(source_folder, language, period, n_top=5, threshold=0.001):\n",
    "    \n",
    "    global state, tfw, tfidf_cache\n",
    "    \n",
    "    try:\n",
    "        treaties = state.treaties\n",
    "\n",
    "        tfw.progress.value = 0\n",
    "        tfw.progress.value += 1\n",
    "        if language[0] not in tfidf_cache.keys():\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0])\\\n",
    "                .load_mm_corpus(normalize_by_D=True)\n",
    "            document_names = corpus.document_names\n",
    "            dictionary = corpus.dictionary\n",
    "            _ = dictionary[0]\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            A = compute_tfidf_scores(corpus, dictionary)\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            scores = pd.DataFrame(\n",
    "                [ (i, j, dictionary.id2token[j], A[i, j]) for i, j in zip(*A.nonzero())],\n",
    "                columns=['document_id', 'token_id', 'token', 'score']\n",
    "            )\n",
    "            tfw.progress.value += 1\n",
    "            scores = scores.merge(document_names, how='inner', left_on='document_id', right_index=True)\\\n",
    "                .drop(['document_id', 'token_id', 'document_name'], axis=1)\n",
    "\n",
    "            scores = scores[['treaty_id', 'token', 'score']]\\\n",
    "                .sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "            tfidf_cache[language[0]] = scores\n",
    "\n",
    "        scores = tfidf_cache[language[0]]\n",
    "        if threshold > 0:\n",
    "            scores = scores.loc[scores.score >= threshold]\n",
    "\n",
    "        tfw.progress.value += 1\n",
    "\n",
    "        #scores = get_top_tfidf_words(scores, n_top=5)\n",
    "        #scores = scores.groupby(['treaty_id']).sum() \n",
    "\n",
    "        scores = scores.groupby(['treaty_id'])\\\n",
    "            .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            .set_index('treaty_id')\n",
    "\n",
    "        if period is not None:\n",
    "            periods = state.treaties[period]\n",
    "            scores = scores.merge(periods.to_frame(), left_index=True, right_index=True, how='inner')\\\n",
    "                .groupby([period, 'token']).score.agg([np.mean])\\\n",
    "                .reset_index().rename(columns={0:'score'}) #.sort_values('token')\n",
    "\n",
    "        #['token'].apply(' '.join)\n",
    "\n",
    "        display(scores)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    tfw.progress.value = 0\n",
    "\n",
    "#if 'tfidf_scores' not in globals():\n",
    "#    tfidf_scores = compute_document_tfidf(corpus, corpus.dictionary, state.treaties)\n",
    "#    tfidf_scores = tfidf_scores.sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "tfw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    n_top=widgets.IntSlider(\n",
    "        value=5, min=1, max=25, step=1,\n",
    "        description='Top #:',\n",
    "        continuous_update=False\n",
    "    ),\n",
    "    threshold=widgets.FloatSlider(\n",
    "        value=0.001, min=0.0, max=0.5, step=0.01,\n",
    "        description='Threshold:',\n",
    "        tooltip='Word having a TF-IDF score below this value is filtered out',\n",
    "        continuous_update=False,\n",
    "        readout_format='.3f',\n",
    "    ), \n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Output:', **drop_style\n",
    "    ),\n",
    "    progress=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itfw = widgets.interactive(\n",
    "    display_tfidf_scores,\n",
    "    source_folder='./data',\n",
    "    language=tfw.language,\n",
    "    n_top=tfw.n_top,\n",
    "    threshold=tfw.threshold,\n",
    "    period=tfw.period\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([tfw.language, tfw.period]),\n",
    "        widgets.VBox([tfw.n_top, tfw.threshold]),\n",
    "        widgets.VBox([tfw.progress, tfw.output])\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(widgets.VBox([boxes, itfw.children[-1]]))\n",
    "itfw.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "model = Cooccurrence(ngram_range=(1, 1))\n",
    "Xc = model.fit_transform(corpus)\n",
    "\n",
    "id2token = { i: x for (i, x) in enumerate(model.get_feature_names()) }\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Xxy = Xc.tocoo()\n",
    "word1 = [ id2token[x] for x in Xxy.row ]\n",
    "word2 = [ id2token[x] for x in Xxy.col ]\n",
    "\n",
    "df = pd.DataFrame({ 'word1': [ id2token[x] for x in Xxy.row ], 'word2': [ id2token[x] for x in Xxy.col ], 'count': Xxy.data })[['word1', 'word2', 'count']] #.set_index(['word1', 'word2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coo_to_sparse_series(A, dense_index=False):\n",
    "    \"\"\" Convert a scipy.sparse.coo_matrix to a SparseSeries.\n",
    "    Use the defaults given in the SparseSeries constructor. \"\"\"\n",
    "    s = Series(A.data, MultiIndex.from_arrays((A.row, A.col)))\n",
    "    s = s.sort_index()\n",
    "    s = s.to_sparse()  # TODO: specify kind?\n",
    "    # ...\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
