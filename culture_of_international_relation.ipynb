{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Culture of International Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DH Nordic 2018 task list\n",
    "\n",
    "<pre>\n",
    "<b>DONE GRAPH 1: A graph with the top five countries in terms of how many new cultural treaties they signed, per period.</b>\n",
    "<b>DONE GRAPH 2: France’s (cultural=yes) treaty totals by period.</b>\n",
    "NEW GRAPH 3: All (cult=yes) treaties, 1919-1944) in cytoscape.  \n",
    "NEW GRAPH 4: All (cult=yes) treaties, 1945-1955) in cytoscape.  \n",
    "<b>DONE GRAPH 5: 7CULT, 7SCI, and 7EDUC over time (by year)</b>\n",
    "<b>DONE GRAPH 6: 7CULT, 7SCI, and 7EDUC+4EDUC (integrated into one variable) over time (by year)</b>\n",
    "<b>DONE GRAPH 7: 7CULT, 7SCI, and 7EDUC+4EDUC over time (by period)</b>\n",
    "DRAFT GRAPH 8: Most frequent (meaningful) co-occurances in treaty headings.\n",
    "DRAFT GRAPH 9: plot selected co-occurences over time?\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Instructions on Jupyter Notebooks\n",
    "Please see [add link] for an introduction on what Jupyter notebooksare and how to use them. In short, a notebook is a document with embedded executable code presented in a simple and easy to use web interface. Most important things to note are:\n",
    "- Click on the menu Help -> User Interface Tour for an overview of the Jupyter Notebook App user interface.\n",
    "- The **code cells** contains the script code (Python in this case, but can be other languages are also suported) and are the sections marked by **In [x]** in the left margin. It is marked as **In []** if it hasn't been executed, and as **In [n]** when it has been executed(n is an integer). A cell marked as **In [\\*]** is either executing, or waiting to be executed (i.e. other cells are executing).\n",
    "- The **current cell** is highlighted with a blue (or green if in \"edit\" mode) border. You make a cell current by clicking on it,\n",
    "- Code cells aren't executed automatically. Instead you execute the current cell by either pressing **shift+enter** or the **play** button in the toolbar. The output (or result) of a cell's execution is presented directly below the cell prefixed by **Out[n]**.\n",
    "- The next cell will automatically be selected (made current) after a cell has been executed. Repeatadly pressing **shift+enter** or the play button hence executes the cells in sequence.\n",
    "- You can run the entire notebook in a single step by clicking on the menu Cell -> Run All. Note that this can take some time to finish. You can see how cells are executed in sequence via the indicator in the margin (i.e. \"In [\\*]\" changes to \"In [n]\" where n is an integer).\n",
    "- The cells can be edited if they are double-clicked, in which case the cell border turns green. Use the ESC key to escape edit mode (or click on any other cell).\n",
    "\n",
    "To restart the kernel (i.e. the computational engine assigned to your session), click on the menu Kernel -> Restart. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# YouTube video\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo(\"h9S4kN4l5Is\", width=400, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to update data from Google Drive\n",
    "The statistics computed on this page is dependent on a recent verison of the WTI treaties master list. This file is stored on Google Drive, and the script \"./google_drive.py\" can be used to download and update the data. Please note that the load script below reads CSV-files, with specific names, so a manual download of the master list must be followed by saving each sheet as an CSV. The script ./google_drive.py does this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef0d41b81a647a794844e276df28237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='File:', options={'WTI Master Index': {'file_id': '1V8KPeghLQ2iOMWkbPqff480zDSLa5YDX', 'destination': './data/Treaties_Master_List.xlsx', 'sheets': ['Treaties']}, 'Country & Continent': {'file_id': '19lEmVPu7hNmr1MaMpU0VvKL7muu-OKg9', 'destination': './data/country_continent.csv', 'sheets': []}, 'Curated Parties': {'file_id': '1k4dOPuqR7oi4K8SazoGN6R40jOBWOdWp', 'destination': './data/parties_curated.xlsx', 'sheets': ['parties', 'group', 'continent']}}, value=None), ToggleButton(value=False, description='Confirm', icon='check'))), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code: Update WTI master data from Google Drive\n",
    "%run ./google_drive\n",
    "%run ./widgets_utility\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "files_to_download = {\n",
    "    'WTI Master Index': {\n",
    "        'file_id': '1V8KPeghLQ2iOMWkbPqff480zDSLa5YDX',\n",
    "        'destination': './data/Treaties_Master_List.xlsx',\n",
    "        'sheets': [ 'Treaties' ]\n",
    "    },\n",
    "    'Curated Parties': {\n",
    "        'file_id': '1k4dOPuqR7oi4K8SazoGN6R40jOBWOdWp',\n",
    "        'destination': './data/parties_curated.xlsx',\n",
    "        'sheets': ['parties', 'group', 'continent']\n",
    "    },\n",
    "    'Country & Continent': {\n",
    "        'file_id': '19lEmVPu7hNmr1MaMpU0VvKL7muu-OKg9',\n",
    "        'destination': './data/country_continent.csv',\n",
    "        'sheets': [ ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def update_file(file, confirm):\n",
    "    global upw\n",
    "    if file is None:\n",
    "        return\n",
    "    if confirm is False:\n",
    "        print('Please confirm update by checking the CONFIRM button!')\n",
    "        return\n",
    "    upw.confirm.value = False\n",
    "    print('Updatating Google file with ID: {}'.format(file['file_id']))\n",
    "    process_file(file, overwrite=confirm)\n",
    "    \n",
    "upw = BaseWidgetUtility(\n",
    "    file=widgets.Dropdown(\n",
    "        options=files_to_download,\n",
    "        value=None,\n",
    "        description='File:',\n",
    "    ),\n",
    "    confirm=widgets.ToggleButton(\n",
    "        description='Confirm',\n",
    "        button_style='',\n",
    "        icon='check',\n",
    "        value=False\n",
    "    ),)\n",
    "iupw = widgets.interactive(update_file, file=upw.file, confirm=upw.confirm)\n",
    "display(widgets.VBox([widgets.HBox([upw.file, upw.confirm]), iupw.children[-1]]))\n",
    "# iupw.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".jupyter-widgets {\n",
    "    font-size: 9pt;\n",
    "}\n",
    ".widget-label {\n",
    "    font-size: 8pt;\n",
    "}\n",
    ".widget-dropdown > select {\n",
    "    font-size: 8pt;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>**Mandatory Prepare Step**</span>: Setup Notebook\n",
    "The following code cell must to be executed once for each user session.\n",
    "\n",
    "The step loads utility Python code stored in separate files, and imports dependencies to external libraries. The following external libraries are used:\n",
    "- [NLTK](https://www.nltk.org/): *Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc.*\n",
    "- [gensim](https://radimrehurek.com/gensim/index.html): [Google scholar](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vG_kV0AAAAJ&citation_for_view=9vG_kV0AAAAJ:NaGl4SEjCO4C)\n",
    "- pandas\n",
    "- networkx\n",
    "- bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"f204d9f4-95a7-4052-9d6c-3083dc996181\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"f204d9f4-95a7-4052-9d6c-3083dc996181\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"f204d9f4-95a7-4052-9d6c-3083dc996181\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f204d9f4-95a7-4052-9d6c-3083dc996181' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"f204d9f4-95a7-4052-9d6c-3083dc996181\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"f204d9f4-95a7-4052-9d6c-3083dc996181\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"f204d9f4-95a7-4052-9d6c-3083dc996181\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'f204d9f4-95a7-4052-9d6c-3083dc996181' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"f204d9f4-95a7-4052-9d6c-3083dc996181\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "%run ./file_utility\n",
    "%run ./network_utility\n",
    "%run ./widgets_utility\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import logging\n",
    "import fnmatch\n",
    "import datetime\n",
    "import wordcloud\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import bokeh.plotting as bp\n",
    "import bokeh.palettes\n",
    "import bokeh.models as bm\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import zipfile\n",
    "import nltk.tokenize\n",
    "import nltk.corpus\n",
    "import gensim.models\n",
    "\n",
    "from pivottablejs import pivot_ui\n",
    "from math import sqrt\n",
    "from bokeh.io import push_notebook\n",
    "from gensim.corpora.textcorpus import TextCorpus\n",
    "\n",
    "from IPython.display import display, HTML #, clear_output, IFrame\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.ERROR)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "warnings.filterwarnings('ignore')\n",
    "bp.output_notebook()\n",
    "\n",
    "%run utility\n",
    "#%autosave 120\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "matplotlib_plot_styles =[\n",
    "    'ggplot',\n",
    "    'bmh',\n",
    "    'seaborn-notebook',\n",
    "    'seaborn-whitegrid',\n",
    "    '_classic_test',\n",
    "    'seaborn',\n",
    "    'fivethirtyeight',\n",
    "    'seaborn-white',\n",
    "    'seaborn-dark',\n",
    "    'seaborn-talk',\n",
    "    'seaborn-colorblind',\n",
    "    'seaborn-ticks',\n",
    "    'seaborn-poster',\n",
    "    'seaborn-pastel',\n",
    "    'fast',\n",
    "    'seaborn-darkgrid',\n",
    "    'seaborn-bright',\n",
    "    'Solarize_Light2',\n",
    "    'seaborn-dark-palette',\n",
    "    'grayscale',\n",
    "    'seaborn-muted',\n",
    "    'dark_background',\n",
    "    'seaborn-deep',\n",
    "    'seaborn-paper',\n",
    "    'classic'\n",
    "]\n",
    "\n",
    "output_formats = {\n",
    "    'Plot vertical bar': 'plot_bar',\n",
    "    'Plot horisontal bar': 'plot_barh',\n",
    "    'Plot vertical bar, stacked': 'plot_bar_stacked',\n",
    "    'Plot horisontal bar, stacked': 'plot_barh_stacked',\n",
    "    'Plot line': 'plot_line',\n",
    "    'Plot stacked line': 'plot_line_stacked',\n",
    "    # 'Chart ': 'chart',\n",
    "    'Table': 'table',\n",
    "    'Pivot': 'pivot'\n",
    "}\n",
    "toggle_style = dict(icon='', layout=widgets.Layout(width='100px', left='0'))\n",
    "drop_style = dict(layout=widgets.Layout(width='260px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>**Mandatory Prepare Step**</span>: Load and Process Treaty Master Index\n",
    "The following code cell to be executed once for each user session. The code loads the WTI master index (and some related data files), and prepares the data for subsequent use.\n",
    "\n",
    "The treaty data is processed as follows:\n",
    "- All the treaty data are loaded.Extract year treaty was signed as seperate fields\n",
    "- Add new fields for specified signed period divisions\n",
    "- Fields 'group1' and 'group2' are ignored (many missing values). Instead group are fetched via party code from encoding found in the \"groups\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported: Treaties_Master_List_Treaties.csv\n",
      "Imported: country_continent.csv\n",
      "Imported: parties_curated_parties.csv\n",
      "Imported: parties_curated_continent.csv\n",
      "Imported: parties_curated_group.csv\n",
      "Number of treaties loaded: 61365\n",
      "Number of cultural treaties: 2231 (total), 1266 within periods\n",
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load and process treaties master index\n",
    "\n",
    "period_divisions = [\n",
    "    [ (1919, 1939), (1940, 1944), (1945, 1955), (1956, 1966), (1967, 1972) ],\n",
    "    [ (1919, 1944), (1945, 1955), (1956, 1966), (1967, 1972) ]\n",
    "]\n",
    "\n",
    "parties_of_interest = ['FRANCE', 'GERMU', 'ITALY', 'GERMAN', 'UK', 'GERME', 'GERMW', 'INDIA' ]\n",
    "\n",
    "class TreatyState:\n",
    "    \n",
    "    def __init__(self, data_folder='./data'):\n",
    "        self.data_folder = data_folder\n",
    "        self.treaties_skip_columns = [\n",
    "            'extra_entry', 'dbflag', 'dummy1', 'english', 'french', 'ispartyof4', 'other',\n",
    "            'regis', 'regisant', 'vol', 'page', 'force', 'group1', 'group2'\n",
    "        ]\n",
    "        self.treaties_columns = [\n",
    "            'sequence',\n",
    "            'treaty_id',\n",
    "            'is_cultural_yesno',\n",
    "            'english',\n",
    "            'french',\n",
    "            'other',\n",
    "            'source',\n",
    "            'vol',\n",
    "            'page',\n",
    "            'signed',\n",
    "            'force',\n",
    "            'regis',\n",
    "            'regisant',\n",
    "            'party1',\n",
    "            'group1',\n",
    "            'party2',\n",
    "            'group2',\n",
    "            'laterality',\n",
    "            'headnote',\n",
    "            'topic',\n",
    "            'topic1',\n",
    "            'topic2',\n",
    "            'title',\n",
    "            'extra_entry',\n",
    "            'dbflag',\n",
    "            'ispartyof4',\n",
    "            'dummy1'\n",
    "        ]\n",
    "        self.csv_files = [\n",
    "            ('Treaties_Master_List_Treaties.csv', 'treaties', None),\n",
    "            ('country_continent.csv', 'country_continent', None),\n",
    "            ('parties_curated_parties.csv', 'parties', None),\n",
    "            ('parties_curated_continent.csv', 'continent', None),\n",
    "            ('parties_curated_group.csv', 'group', None)\n",
    "        ]\n",
    "        self.data = self.read_data(data_folder)\n",
    "        self.treaty_headnote_corpus = None\n",
    "        \n",
    "    def read_data(self, data_folder):\n",
    "        data = {}\n",
    "        for (filename, key, dtype) in self.csv_files:\n",
    "            path = os.path.join(self.data_folder, filename)\n",
    "            data[key] = pd.read_csv(path, sep='\\t', low_memory=False)\n",
    "            print('Imported: {}'.format(filename))\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def process_treaties(self):\n",
    "\n",
    "        def get_period(division, year):\n",
    "            match = [ p for p in division if p[0] <= year <= p[1]]\n",
    "            return '{} to {}'.format(match[0][0], match[0][1]) if len(match) > 0 else 'other'\n",
    "    \n",
    "        treaties = self.data['treaties']\n",
    "        treaties.columns = self.treaties_columns\n",
    "        \n",
    "        treaties['vol'] = treaties.vol.fillna(0).astype('int', errors='ignore')\n",
    "        treaties['page'] = treaties.page.fillna(0).astype('int', errors='ignore')\n",
    "        treaties['signed'] = pd.to_datetime(treaties.signed, errors='coerce')\n",
    "        treaties['is_cultural_yesno'] = treaties.is_cultural_yesno.astype(str)\n",
    "        treaties['signed_year'] = treaties.signed.apply(lambda x: x.year)\n",
    "        treaties['signed_period'] = treaties.signed.apply(lambda x: get_period(period_divisions[0], x.year))\n",
    "        treaties['signed_period_alt'] = treaties.signed.apply(lambda x: get_period(period_divisions[1], x.year))\n",
    "        treaties['force'] = pd.to_datetime(treaties.force, errors='coerce')\n",
    "        treaties['sequence'] = treaties.sequence.astype('int', errors='ignore')\n",
    "        treaties['group1'] = treaties.group1.fillna(0).astype('int', errors='ignore')\n",
    "        treaties['group2'] = treaties.group2.fillna(0).astype('int', errors='ignore')\n",
    "        treaties['is_cultural'] = treaties.is_cultural_yesno.apply(lambda x: x.lower() == 'yes')\n",
    "        treaties['headnote'] = treaties.headnote.fillna('').astype(str).str.upper()\n",
    "\n",
    "        # Drop columns not used\n",
    "        treaties.drop(self.treaties_skip_columns, axis=1, inplace=True)\n",
    "        treaties = treaties.set_index(['treaty_id'])\n",
    "        return treaties\n",
    "\n",
    "    def get_stacked_treaties(self):\n",
    "        '''\n",
    "        Returns a bi-directional (duplicated) and processed version of the treaties master list.\n",
    "        Each treaty has two records where party1 and party2 are reversed:\n",
    "            Record #1: party=party1, party_other=party2, reversed=False\n",
    "            Record #2: party=party2, party_other=party1, reversed=True\n",
    "        Fields are also added for the party's and party_other's country code (2 chars), continent and WTI group.\n",
    "        The two rows are identical for all other fields.\n",
    "        '''\n",
    "        df1 = self.treaties\\\n",
    "                .rename(columns={\n",
    "                    'party1': 'party',\n",
    "                    'party2': 'party_other',\n",
    "                    'group1': 'party_group_no',\n",
    "                    'group2': 'party_other_group_no'\n",
    "                })\\\n",
    "                .assign(reversed=False)\n",
    "\n",
    "        df2 = self.treaties\\\n",
    "                .rename(columns={\n",
    "                    'party2': 'party',\n",
    "                    'party1': 'party_other',\n",
    "                    'group2': 'party_group_no',\n",
    "                    'group1': 'party_other_group_no'\n",
    "                })\\\n",
    "                .assign(reversed=True)\n",
    "        \n",
    "        treaties = df1.append(df2) #.set_index(['treaty_id'])\n",
    "        \n",
    "        # Add fields for party's country, continent and WTI group\n",
    "        parties = self.parties[['country_code', 'continent_code', 'group_name']]\n",
    "        \n",
    "        parties.columns = ['party_country', 'party_continent', 'party_group']\n",
    "        treaties = treaties.merge(parties, how='left', left_on='party', right_index=True)\n",
    "        \n",
    "        # Add fields for party_other's country, continent and WTI group\n",
    "        parties.columns = ['party_other_country', 'party_other_continent', 'party_other_group']\n",
    "        treaties = treaties.merge(parties,how='left', left_on='party_other', right_index=True)\n",
    "        \n",
    "        # Drop columns\n",
    "        treaties = treaties.drop(['sequence', 'is_cultural_yesno'], axis=1)\n",
    "        return treaties\n",
    "    \n",
    "    def get_continents(self):\n",
    "        \n",
    "        df = self.data['continent'].drop(['Unnamed: 0'], axis=1).set_index('country_code2')\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def get_groups(self):\n",
    "        \n",
    "        df = self.data['group']\\\n",
    "            .drop(['Unnamed: 0'], axis=1)\\\n",
    "            .rename(columns={'GroupNo': 'group_no','GroupName': 'group_name'})\\\n",
    "\n",
    "        df['group_no'] = df.group_no.astype(np.int32)\n",
    "        df['group_name'] = df.group_name.astype(str)\n",
    "        \n",
    "        df = df.set_index('group_no')\n",
    "\n",
    "        return df\n",
    "        \n",
    "    def get_parties(self):\n",
    "        \n",
    "        parties = self.data['parties']\\\n",
    "            .drop(['Unnamed: 0'], axis=1)\\\n",
    "            .dropna(subset=['PartyID'])\\\n",
    "            .rename(columns={\n",
    "                'PartyID': 'party',\n",
    "                'PartyName': 'party_name',\n",
    "                'ShortName': 'short_name',\n",
    "                'GroupNo': 'group_no',\n",
    "                'reversename': 'reverse_name'\n",
    "            })\\\n",
    "            .dropna(subset=['party'])\\\n",
    "            .set_index('party')\n",
    "            \n",
    "        parties['group_no'] = parties.group_no.astype(np.int32)\n",
    "        parties['party_name'] = parties.party_name.apply(lambda x: re.sub(r'\\(.*\\)', '', x))\n",
    "        parties['short_name'] = parties.short_name.apply(lambda x: re.sub(r'\\(.*\\)', '', x))\n",
    "\n",
    "        parties.loc[(parties.group_no==8), ['country', 'country_code', 'country_code3']] = ''\n",
    "\n",
    "        parties = pd.merge(parties, self.groups, how='left', left_on='group_no', right_index=True)\n",
    "        parties = pd.merge(parties, self.continents, how='left', left_on='country_code', right_index=True)\n",
    "        \n",
    "        return parties\n",
    "    \n",
    "    def get_party_name(self, party, party_name_column):\n",
    "        try:\n",
    "            if party in self.parties.index:\n",
    "                return self.parties.loc[party, party_name_column]\n",
    "            return party\n",
    "        except:\n",
    "            print('Warning: {} not in curated parties list'.format(party))\n",
    "            return party\n",
    "        \n",
    "    def process(self):\n",
    "        \n",
    "        self.groups = self.get_groups()\n",
    "        self.continents = self.get_continents()\n",
    "        self.parties = self.get_parties()\n",
    "        \n",
    "        self.treaties = self.process_treaties()\n",
    "        self.stacked_treaties = self.get_stacked_treaties()\n",
    "        \n",
    "        self.cultural_treaties = self.treaties[self.treaties.is_cultural]\n",
    "        self.cultural_treaties_of_interest = self.cultural_treaties[(self.cultural_treaties.signed_period != 'other')]\n",
    "        \n",
    "        print('Number of treaties loaded: {}'.format(len(self.treaties)))\n",
    "        print('Number of cultural treaties: {} (total), {} within periods'.format(\n",
    "            len(self.cultural_treaties),\n",
    "            len(self.cultural_treaties_of_interest)\n",
    "        ))\n",
    "        \n",
    "        self.tagged_headnotes = None\n",
    "        return self\n",
    "\n",
    "    def get_headnotes(self):\n",
    "        return self.treaties.headnote.fillna('').astype(str)\n",
    "    \n",
    "    def get_tagged_headnotes(self, tags=None):\n",
    "        if self.tagged_headnotes is None:\n",
    "            filename = os.path.join(self.data_folder, 'tagged_headnotes.csv')\n",
    "            self.tagged_headnotes = pd.read_csv(filename, sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "        if tags is None:\n",
    "            return self.tagged_headnotes\n",
    "        return self.tagged_headnotes.loc[(self.tagged_headnotes.pos.isin(tags))]\n",
    "\n",
    "try:\n",
    "    state = TreatyState().process()\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "print(\"Data loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Sanity Checks: Per field (pair) value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "\n",
    "treaty_fields = [\n",
    "    '', 'is_cultural_yesno', 'source', 'party1', 'party2', 'laterality',\n",
    "    'headnote', 'topic', 'topic1', 'topic2', 'title', 'signed_year', 'signed_period', 'signed_period_alt', 'is_cultural'\n",
    "]    \n",
    "\n",
    "def display_variable_stats(field1, field2, crosstab):\n",
    "    \n",
    "    columns = [ x for x in set([field1, field2 ]) if x != '' ]\n",
    "    \n",
    "    if len(columns) > 0:\n",
    "        df = state.treaties.groupby(columns).size().reset_index()\\\n",
    "            .rename(columns={0: 'Count'})\\\n",
    "            .sort_values(['Count'], ascending=False)\n",
    "            \n",
    "        if crosstab is True:\n",
    "            if len(columns) == 2:\n",
    "                display(pd.crosstab(df[field1], df[field1]))\n",
    "            else:\n",
    "                print('Both fields are needed for crosstab')\n",
    "        else:\n",
    "            display(df)\n",
    "        # df.set_index(columns).plot.bar(figsize=(16,8))\n",
    "\n",
    "sw = BaseWidgetUtility(\n",
    "    field1=wf.create_select_widget('Field 1:', treaty_fields, default=''),\n",
    "    field2=wf.create_select_widget('Field 2:', treaty_fields, default=''),\n",
    "    crosstab=widgets.ToggleButton(\n",
    "        description='Crosstab',\n",
    "        button_style='',\n",
    "        icon='check'\n",
    "    ),\n",
    ")\n",
    "\n",
    "isw = widgets.interactive(display_variable_stats, field1=sw.field1, field2=sw.field2, crosstab=sw.crosstab)\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([sw.field1, sw.field2, sw.crosstab]), isw.children[-1]]))\n",
    "\n",
    "isw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Treaty Quantities by Selected Parties \n",
    "```\n",
    "DONE GRAPH 1: A graph with the top five countries in terms of how many new cultural treaties they signed, per period.\n",
    "DONE GRAPH 2: France’s (cultural=yes) treaty totals by period.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#colors = hsv(np.linspace(0, 1.0, 16))\n",
    "colors = bokeh.palettes.Category20[20] #plt.get_cmap('jet')(np.linspace(0, 1.0, 16))\n",
    "def plot_treaties_per_period(data, output_format, plot_style, figsize=(12,6), xlabel='', ylabel=''):\n",
    "\n",
    "    matplotlib.style.use(plot_style)\n",
    "    stacked = 'stacked' in output_format\n",
    "    kind = output_format.split('_')[1]\n",
    "    ax = data.plot(kind=kind, stacked=stacked, figsize=figsize, color=colors)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "    # Put a legend to the right of the current axis\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "\n",
    "\n",
    "def bokeh_plot_signed_treaties_per_period(df, pivot_column, key='mean', n_topics=3, year=None, n_words=100):\n",
    "    \n",
    "    def generate_category_colors(n_items, palette=bokeh.palettes.Category20[20]):\n",
    "        ''' Repeat palette to get n_items colors '''\n",
    "        colors = (((n_items // len(palette)) + 1) * palette)[:n_items]\n",
    "        return colors\n",
    "    \n",
    "    categories = list(df.columns[1:])\n",
    "    colors = generate_category_colors(n_topics)\n",
    "    source = ColumnDataSource(df)\n",
    "    \n",
    "    p = bp.figure(plot_width=900, plot_height=800, title=state.basename, tools=TOOLS, toolbar_location=\"right\")\n",
    "    \n",
    "    p.xaxis[0].axis_label = key.title() + ' weight'\n",
    "    p.yaxis[0].axis_label = pivot_column.title()\n",
    "    \n",
    "    #legend = [ value(x) for x in categories ]\n",
    "    #p.hbar_stack(categories, y=pivot_column, source=source, color=colors, height=0.5, legend=legend)\n",
    "        \n",
    "    bottoms, tops = [], []\n",
    "    for i, category in enumerate(categories):\n",
    "        tops = tops + [category]\n",
    "        cr = p.hbar(y=pivot_column,\n",
    "                    left=expr(bm.expressions.Stack(fields=bottoms)),\n",
    "                    right=expr(bm.expressions.Stack(fields=tops)),\n",
    "                    color=colors[i],\n",
    "                    height=0.5,\n",
    "                    source=source,\n",
    "                    legend='Topic ' + str(category))\n",
    "        topic_id = int(category)\n",
    "        tooltip = 'ID {}: {}'.format(topic_id, state.get_topics_tokens_as_text(n_words=200, cache=True).iloc[topic_id])\n",
    "        p.add_tools(bm.HoverTool(tooltips=tooltip, renderers=[cr]))\n",
    "        bottoms = bottoms + [category]\n",
    "            \n",
    "    return p\n",
    "\n",
    "def get_top_parties(data, period, party_name, n_top=5):\n",
    "    xd = data.groupby([period, party_name]).size().rename('TopCount').reset_index()\n",
    "    top_list = xd.groupby([period]).apply(lambda x: x.nlargest(n_top, 'TopCount'))\\\n",
    "        .reset_index(level=0, drop=True)\\\n",
    "        .set_index([period, party_name])\n",
    "    return top_list\n",
    "\n",
    "def display_treaties_per_period(\n",
    "    period,\n",
    "    party_name,\n",
    "    parties_selection,\n",
    "    only_is_cultural=False,\n",
    "    normalize_values=False,\n",
    "    output_format='chart',\n",
    "    plot_style='classic',\n",
    "    top_n_parties=5\n",
    "):\n",
    "    try:\n",
    "        data = state.stacked_treaties.copy()\n",
    "\n",
    "        # if only_within_period_of_interest:\n",
    "        data = data.loc[(data.signed_period!='other')]\n",
    "\n",
    "        if only_is_cultural:\n",
    "            data = data.loc[(data.is_cultural==True)]\n",
    "\n",
    "        if isinstance(parties_selection, list):\n",
    "            data = data.loc[(data.party.isin(parties_selection))]\n",
    "\n",
    "        data = data.merge(state.parties, how='left', left_on='party', right_index=True)\n",
    "\n",
    "        n_top_list = get_top_parties(data, period, party_name, n_top=top_n_parties)\n",
    "\n",
    "        data = data.groupby([period, party_name])\\\n",
    "                .size()\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns={ period: 'Period', party_name: 'Party', 0: 'Count' })\n",
    "\n",
    "        if isinstance(parties_selection, str) and parties_selection.startswith('only_top_'):\n",
    "            join = 'inner' #if parties_selection == 'only_top_parties' else 'left'\n",
    "            data = data.merge(n_top_list, how=join, left_on=['Period', 'Party'], right_index=True)\n",
    "\n",
    "        pivot = pd.pivot_table(data, index=['Period'], values=[\"Count\"], columns=['Party'], fill_value=0)\n",
    "        pivot.columns = [ x[-1] for x in pivot.columns ]\n",
    "\n",
    "        if normalize_values is True:\n",
    "            pivot = pivot.div(0.01 * pivot.sum(1), axis=0)\n",
    "\n",
    "        if output_format.startswith('plot'):\n",
    "\n",
    "            label = 'Number of treaties' if not normalize_values else 'Share%'\n",
    "\n",
    "            ylabel = label if 'barh' not in output_format else ''\n",
    "            xlabel = label if 'barh' in output_format else ''\n",
    "\n",
    "            height = 10 if 'barh' in output_format and period == 'signed_year' else 6\n",
    "            plot_treaties_per_period(pivot, output_format, plot_style, figsize=(16, height), xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            display(data)\n",
    "            # display(HTML(data.to_html()))\n",
    "        else:\n",
    "            display(pivot)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "\n",
    "tw = BaseWidgetUtility(\n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    ),\n",
    "    party_name=widgets.Dropdown(\n",
    "        options={\n",
    "            'WTI Code': 'party',\n",
    "            'WTI Name': 'party_name',\n",
    "            'WTI Short': 'short_name',\n",
    "            'Country': 'party_country'\n",
    "        },\n",
    "        value='party',\n",
    "        description='Name:',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    ),\n",
    "    parties_selection=widgets.Dropdown(\n",
    "        options={\n",
    "            'Only top parties': 'only_top_parties',\n",
    "            'Only the five parties': parties_of_interest,\n",
    "            'France': ['FRANCE'],\n",
    "            'UK': ['UK'],\n",
    "            'Germany': [ 'GERMU', 'GERMAN', 'GERME', 'GERMW' ],\n",
    "            'India': [ 'INDIA' ]\n",
    "            # 'Only top + others': 'only_top_and_others',\n",
    "            # 'Only five + others': 'only_five_and_others',\n",
    "            # 'All (long list)': 'all_parties'\n",
    "        },\n",
    "        value='only_top_parties',\n",
    "        description='Parties:',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    ),\n",
    "\n",
    "    only_is_cultural=widgets.ToggleButton(\n",
    "        description='Only Cultural', value=True, **toggle_style\n",
    "    ),\n",
    "    normalize_values=widgets.ToggleButton(\n",
    "        description='Share%', **toggle_style\n",
    "    ),\n",
    "    output_format=widgets.Dropdown(\n",
    "        description='Output', options=output_formats, layout=widgets.Layout(width='200px')\n",
    "    ),\n",
    "    plot_style=widgets.Dropdown(\n",
    "        options=matplotlib_plot_styles, value='seaborn-pastel',\n",
    "        description='Style:', layout=widgets.Layout(width='200px')\n",
    "    ),\n",
    "    top_n_parties=widgets.IntSlider(\n",
    "        value=3, min=1, max=10, step=1,\n",
    "        description='Top #:',\n",
    "        continuous_update=True,\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    )\n",
    ")\n",
    "\n",
    "itw = widgets.interactive(\n",
    "    display_treaties_per_period,\n",
    "    period=tw.period,\n",
    "    party_name=tw.party_name,\n",
    "    parties_selection=tw.parties_selection,\n",
    "    only_is_cultural=tw.only_is_cultural,\n",
    "    normalize_values=tw.normalize_values,\n",
    "    output_format=tw.output_format,\n",
    "    plot_style=tw.plot_style,\n",
    "    top_n_parties=tw.top_n_parties\n",
    ")\n",
    "\n",
    "first_column_box = widgets.VBox([tw.period, tw.party_name,])\n",
    "second_column_box = widgets.VBox([ tw.parties_selection, tw.top_n_parties ])\n",
    "third_column_box = widgets.VBox([ tw.only_is_cultural, tw.normalize_values])\n",
    "fourth_column_box = widgets.VBox([ tw.output_format, tw.plot_style ])\n",
    "boxes = widgets.HBox([first_column_box, second_column_box, third_column_box, fourth_column_box ])\n",
    "display(widgets.VBox([boxes, itw.children[-1]]))\n",
    "itw.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Chart: Treaty Quantities by Selected Topics\n",
    "TODO Uses currently only topic1\n",
    "```\n",
    "DONE GRAPH 5: 7CULT, 7SCI, and 7EDUC over time (by year)\n",
    "DONE GRAPH 6: 7CULT, 7SCI, and 7EDUC+4EDUC (integrated into one variable) over time (by year)\n",
    "DONE GRAPH 7: 7CULT, 7SCI, and 7EDUC+4EDUC over time (by period)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "%matplotlib inline\n",
    "category_maps = {\n",
    "    '7CULT, 7SCIEN, and 7EDUC': {\n",
    "        '7CULT': '7CULT',\n",
    "        '7SCIEN': '7SCIEN',\n",
    "        '7EDUC': '7EDUC'\n",
    "    },\n",
    "    '7CULT, 7SCI, and 7EDUC+4EDUC': {\n",
    "        '7CULT': '7CULT',\n",
    "        '7SCIEN': '7SCIEN',\n",
    "        '7EDUC': '7EDUC+4EDUC',\n",
    "        '4EDUC': '7EDUC+4EDUC'\n",
    "    }\n",
    "}\n",
    "\n",
    "def plot_display_quantity_of_topics(pivot, kind, stacked, xlabel='', ylabel='', plot_style='classic', figsize=(12,10)):\n",
    "   \n",
    "    matplotlib.style.use(plot_style)\n",
    "    ax = pivot.plot(kind=kind, stacked=stacked, figsize=figsize)\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    # legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4)\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "    \n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)    \n",
    "    \n",
    "def display_quantity_of_topics(\n",
    "    period,\n",
    "    category_map_name,\n",
    "    recode_7cult=False,\n",
    "    normalize_values=False,\n",
    "    include_other=False,\n",
    "    output_format='chart',\n",
    "    plot_style='classic'\n",
    "):\n",
    "    global X\n",
    "    try:\n",
    "        data = state.treaties.copy()\n",
    "\n",
    "        category_map = category_maps[category_map_name]\n",
    "\n",
    "        if not include_other:\n",
    "            data = data.loc[(data.topic1.isin(category_map.keys()))]\n",
    "\n",
    "        data = data.loc[(data.signed_period!='other')]\n",
    "\n",
    "        if recode_7cult:\n",
    "            data.loc[(data.is_cultural==True), 'topic1'] = '7CULT'\n",
    "\n",
    "        data['category'] = data.topic1.apply(lambda x: category_map.get(x, 'OTHER'))\n",
    "\n",
    "        data = data\\\n",
    "                .groupby([period, 'category'])\\\n",
    "                .size()\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns={ period: 'Period', 'category': 'Category', 0: 'Count' })\n",
    "\n",
    "        pivot = pd.pivot_table(data, index=['Period'], values=[\"Count\"], columns=['Category'], fill_value=0)\n",
    "        pivot.columns = [ x[-1] for x in pivot.columns ]\n",
    "\n",
    "        if normalize_values is True:\n",
    "            pivot = pivot.div(0.01 * pivot.sum(1), axis=0)\n",
    "\n",
    "        if output_format.startswith('plot'):\n",
    "\n",
    "            label = 'Number of treaties' if not normalize_values else 'Share%'\n",
    "\n",
    "            ylabel = label if 'barh' not in output_format else ''\n",
    "            xlabel = label if 'barh' in output_format else ''\n",
    "\n",
    "            stacked = 'stacked' in output_format\n",
    "            kind = output_format.split('_')[1]\n",
    "            height = 10 if 'barh' in output_format and period == 'signed_year' else 6\n",
    "\n",
    "            plot_display_quantity_of_topics(\n",
    "                pivot, kind=kind, stacked=stacked, xlabel=xlabel, ylabel=ylabel, plot_style=plot_style, figsize=(16,height)\n",
    "            )\n",
    "\n",
    "        elif output_format == 'chart':\n",
    "            print('bokeh plot not implemented')\n",
    "            data.plot.line(figsize=(12,8))\n",
    "        elif output_format == 'table':\n",
    "            #display(data)\n",
    "            display(HTML(data.to_html()))\n",
    "        else:\n",
    "            display(pivot)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "tw = BaseWidgetUtility(\n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    category_map_name=widgets.Dropdown(\n",
    "        options=category_maps.keys(),\n",
    "        description='Category:', **drop_style\n",
    "    ),\n",
    "    recode_7cult=widgets.ToggleButton(\n",
    "        description='Recode 7CULT',\n",
    "        tooltip='Treat all treaties with cultural=yes as 7CULT',\n",
    "        value=False, **toggle_style\n",
    "    ),\n",
    "    normalize_values=widgets.ToggleButton(\n",
    "        description='Normalize%',\n",
    "        tooltip='Display shares per category instead of count', **toggle_style\n",
    "    ),\n",
    "    include_other=widgets.ToggleButton(\n",
    "        description='+Other', value=False,  **toggle_style\n",
    "    ),\n",
    "    output_format=widgets.Dropdown(\n",
    "        description='Output',\n",
    "        options=output_formats, **drop_style\n",
    "    ),\n",
    "    plot_style=widgets.Dropdown(\n",
    "        options=matplotlib_plot_styles,\n",
    "        value='seaborn-pastel',\n",
    "        description='Style:', **drop_style\n",
    "    ),\n",
    ")\n",
    "\n",
    "itw = widgets.interactive(\n",
    "    display_quantity_of_topics,\n",
    "    period=tw.period,\n",
    "    category_map_name=tw.category_map_name,\n",
    "    recode_7cult=tw.recode_7cult,\n",
    "    normalize_values=tw.normalize_values,\n",
    "    include_other=tw.include_other,\n",
    "    output_format=tw.output_format,\n",
    "    plot_style=tw.plot_style\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([ tw.period, tw.category_map_name]),\n",
    "        widgets.VBox([ tw.recode_7cult, tw.normalize_values]),\n",
    "        widgets.VBox([ tw.include_other]),\n",
    "        widgets.VBox([ tw.output_format, tw.plot_style ])\n",
    "    ]\n",
    ")\n",
    "display(widgets.VBox([boxes, itw.children[-1]]))\n",
    "itw.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Headnote Word and Cooccurence Toplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "toggle_style = dict(icon='', layout=widgets.Layout(width='140px', left='0'))\n",
    "\n",
    "class HeadnoteTokenServiceOLD():\n",
    "\n",
    "    def __init__(self, tokenizer, stopwords=None, lemmatizer=None, min_word_size=2):\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenizer,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_word_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)) \n",
    "        ]\n",
    "        \n",
    "        if stopwords is not None:\n",
    "            self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        if lemmatizer is not None:\n",
    "            self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "\n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def parse_headnotes(self, treaties):\n",
    "        \n",
    "        headnotes = treaties['headnote']\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        #tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compute_stacked(self, treaties):\n",
    "        \n",
    "        df = self.parse_headnotes(treaties)\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "    def compute_co_occurrence(self, treaties, pos_tags, only_cultural_treaties=False):\n",
    "\n",
    "        # Filter out tags based on treaties of interest\n",
    "        pos_tags = pos_tags.merge(treaties, how='inner', left_on='treaty_id', right_index=True)[[]]\n",
    "        \n",
    "        if only_cultural_treaties:\n",
    "            df_pos_tags = df_pos_tags[(df_pos_tags.is_cultural.str.contains('yes',na=False))]\n",
    "\n",
    "        # Self join of words within same treaty\n",
    "        df_co_occurrence = pd.merge(df_pos_tags, df_pos_tags, how='inner', left_on='treaty_id', right_on='treaty_id')\n",
    "        # Only consider a specific poir once\n",
    "        df_co_occurrence = df_co_occurrence[(df_co_occurrence.wid_x < df_co_occurrence.wid_y)]\n",
    "        # Reduce number of returned columns\n",
    "        df_co_occurrence = df_co_occurrence[['treaty_id', 'year_x', 'is_cultural_x', 'lemma_x', 'lemma_y' ]]\n",
    "        # Rename columns\n",
    "        df_co_occurrence.columns = ['treaty_id', 'year', 'is_cultural', 'lemma_x', 'lemma_y' ]\n",
    "\n",
    "        # Sort token pair so smallest always comes first\n",
    "        lemma_x = df_co_occurrence[['lemma_x', 'lemma_y']].min(axis=1)\n",
    "        lemma_y = df_co_occurrence[['lemma_x', 'lemma_y']].max(axis=1)\n",
    "        df_co_occurrence['lemma_x'] = lemma_x\n",
    "        df_co_occurrence['lemma_y'] = lemma_y\n",
    "\n",
    "        return df_co_occurrence\n",
    "\n",
    "class HeadnoteTokenCorpus():\n",
    "\n",
    "    def __init__(self, treaties, tokenize=None, stopwords=None, lemmatize=None, min_size=2):\n",
    "        \n",
    "        tokenize = tokenize or nltk.tokenize.word_tokenize\n",
    "        lemmatize = lemmatize or WordNetLemmatizer().lemmatize\n",
    "        stopwords = stopwords or nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenize,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)),\n",
    "            lambda ws: list(set(ws)) \n",
    "        ]\n",
    "        \n",
    "        #if stopwords is not None:\n",
    "        #    self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        #if lemmatizer is not None:\n",
    "        #    self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "        \n",
    "        treaty_tokens = self._compute_stacked(treaties)\n",
    "        vocabulary = treaty_tokens.token.unique()\n",
    "        lemmas = list(map(lemmatize, vocabulary))\n",
    "        lemma_map = { w: l for (w, l) in zip(*(vocabulary, lemmas)) if w != l }\n",
    "        stopwords_map = { s : True for s in stopwords }\n",
    "        treaty_tokens['lemma'] = treaty_tokens.token.apply(lambda x: lemma_map.get(x, x))\n",
    "        treaty_tokens['is_stopword'] = treaty_tokens.token.apply(lambda x: stopwords_map.get(x, False))\n",
    "\n",
    "        self.treaty_tokens = treaty_tokens.set_index(['treaty_id', 'sequence_id'])\n",
    "        \n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def _parse_headnotes(self, treaties):\n",
    "        \n",
    "        headnotes = treaties['headnote']\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _compute_stacked(self, treaties):\n",
    "        \n",
    "        df = self._parse_headnotes(treaties)\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "def compute_co_occurrance(treaties):\n",
    "    \n",
    "    treaty_tokens = state.treaty_headnote_corpus.treaty_tokens\n",
    "    i1 = treaties.index\n",
    "    # i2 = treaty_tokens.reset_index().set_index('treaty_id').index\n",
    "    i2 = treaty_tokens.index.get_level_values(0)\n",
    "    treaty_tokens = treaty_tokens[i2.isin(i1)]\n",
    "    \n",
    "    treaty_tokens = treaty_tokens.loc[treaty_tokens.is_stopword==False]\n",
    "    treaty_tokens = treaty_tokens.reset_index().drop(['is_stopword', 'sequence_id'], axis=1).set_index('treaty_id')\n",
    "\n",
    "    co_occurrance = treaty_tokens.merge(treaty_tokens, how='inner', left_index=True, right_index=True)\n",
    "    co_occurrance = co_occurrance.loc[(co_occurrance['token_x'] < co_occurrance['token_y'])]\n",
    "    #co_occurrance['token'] = co_occurrance.apply(lambda row: row[groupby_pair[0]] + ' - ' + row[groupby_pair[1]], axis=1)\n",
    "    co_occurrance['token'] = co_occurrance.apply(lambda row: ' - '.join([row['token_x'].upper(), row['token_y'].upper()]), axis=1)\n",
    "    co_occurrance['lemma'] = co_occurrance.apply(lambda row: ' - '.join([row['lemma_x'].upper(), row['lemma_y'].upper()]), axis=1)\n",
    "    co_occurrance = co_occurrance.assign(is_stopword=False, sequence_id=0)[['sequence_id', 'token', 'lemma', 'is_stopword']]\n",
    "    \n",
    "    return co_occurrance\n",
    "\n",
    "def create_bigram_transformer(documents):\n",
    "    import gensim.models.phrases\n",
    "    bigram = gensim.models.phrases.Phrases(map(nltk.tokenize.word_tokenize, documents))\n",
    "    return lambda ws: bigram[ws]\n",
    "\n",
    "def remove_snake_case(snake_str):\n",
    "    return ' '.join(x.title() for x in snake_str.split('_'))\n",
    "\n",
    "def get_top_partiesssss(data, period, party_name, n_top=5):\n",
    "    xd = data.groupby([period, party_name]).size().rename('TopCount').reset_index()\n",
    "    top_list = xd.groupby([period]).apply(lambda x: x.nlargest(n_top, 'TopCount'))\\\n",
    "        .reset_index(level=0, drop=True)\\\n",
    "        .set_index([period, party_name])\n",
    "    return top_list\n",
    "\n",
    "result=None\n",
    "def display_headnote_toplist(\n",
    "    period=None,\n",
    "    parties=None,\n",
    "    extra_groupbys=None,\n",
    "    only_is_cultural=True,\n",
    "    use_lemma=False,\n",
    "    compute_co_occurance=False,\n",
    "    remove_stopwords=True,\n",
    "    min_word_size=2,\n",
    "    n_min_count=1,\n",
    "    output_format='table',\n",
    "    n_top=50\n",
    "    # plot_style=tw.plot_style\n",
    "):\n",
    "    global ihnw, result\n",
    "    \n",
    "    try:\n",
    "        hnw.progress.value = 1    \n",
    "        treaties = state.treaties.loc[state.treaties.signed_period != 'other']\n",
    "\n",
    "        if state.treaty_headnote_corpus is None:\n",
    "            print('Preparing headnote corpus for first time use')\n",
    "            state.treaty_headnote_corpus = HeadnoteTokenCorpus(treaties=treaties)\n",
    "\n",
    "        if only_is_cultural:\n",
    "            treaties = treaties.loc[(state.treaties.is_cultural)]\n",
    "\n",
    "        if parties is not None:\n",
    "            ids = state.stacked_treaties.loc[(state.stacked_treaties.party.isin(parties))].index\n",
    "            treaties = treaties.loc[ids]\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        if compute_co_occurance:\n",
    "\n",
    "            treaty_tokens = compute_co_occurrance(treaties)\n",
    "\n",
    "        else:\n",
    "\n",
    "            treaty_tokens = state.treaty_headnote_corpus.treaty_tokens\n",
    "\n",
    "            if remove_stopwords is True:\n",
    "                treaty_tokens = treaty_tokens.loc[treaty_tokens.is_stopword==False]\n",
    "\n",
    "            treaty_tokens = treaty_tokens.reset_index().set_index('treaty_id')\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        treaty_tokens = treaty_tokens\\\n",
    "            .merge(treaties, how='inner', left_index=True, right_index=True)\\\n",
    "            .drop(['sequence', 'is_cultural_yesno', 'source', 'signed', 'headnote', 'is_cultural',\n",
    "                   'topic1', 'topic2', 'title'], axis=1)\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        token_or_lemma = 'token' if not use_lemma else 'lemma'\n",
    "\n",
    "        groupbys  = []\n",
    "        groupbys += [ period ] if not period is None else []\n",
    "        groupbys += (extra_groupbys or [])\n",
    "        groupbys += [ token_or_lemma ]\n",
    "\n",
    "        result = treaty_tokens.groupby(groupbys).size().reset_index().rename(columns={0: 'Count'})\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        ''' Filter out the n_top most frequent words from each group '''\n",
    "        result = result.groupby(groupbys[-1]).apply(lambda x: x.nlargest(n_top, 'Count'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            # .set_index(groupbys)\n",
    "\n",
    "        if min_word_size > 0:\n",
    "            result = result.loc[result[token_or_lemma].str.len() >= min_word_size]\n",
    "\n",
    "        if n_min_count > 1:\n",
    "            result = result.loc[result.Count >= n_min_count]\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        result = result.sort_values(groupbys[:-1] + ['Count'], ascending=len(groupbys[:-1])*[True] + [False])\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        if output_format == 'table':\n",
    "            result.columns = [ remove_snake_case(x) for x in result.columns ]\n",
    "            display(result)\n",
    "            # display(HTML(result.to_html()))\n",
    "        elif output_format == 'unstack':\n",
    "            result = result.set_index(groupbys).unstack(level=0).fillna(0).astype('int32')\n",
    "            result.columns = [ x[1] for x in result.columns ]\n",
    "            display(result)\n",
    "        elif output_format == 'unstack_plot':\n",
    "            result = result.set_index(list(reversed(groupbys))).unstack(level=0).fillna(0).astype('int32')\n",
    "            result.columns = [ x[1] for x in result.columns ]\n",
    "            result.plot(kind='bar', figsize=(16,8))\n",
    "\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    hnw.progress.value += 1\n",
    "    hnw.progress.value = 0\n",
    "\n",
    "hnw = BaseWidgetUtility(\n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    parties=widgets.Dropdown(\n",
    "        options={\n",
    "            '(all)': None,\n",
    "            'PartyOf5': parties_of_interest,\n",
    "            'France': [ 'FRANCE' ],\n",
    "            'Italy': [ 'ITALY' ],\n",
    "            'UK': [ 'UK' ],\n",
    "            'India': [ 'INDIA' ],\n",
    "            'Germany': [ 'GERMU', 'GERMAN', 'GERME', 'GERMW' ]\n",
    "        },\n",
    "        value=None,\n",
    "        description='Parties:', **drop_style\n",
    "    ),\n",
    "    use_lemma=widgets.ToggleButton(\n",
    "        description='Use lemma', value=False,\n",
    "        tooltip='Use WordNet lemma', **toggle_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords', **toggle_style\n",
    "    ),\n",
    "    extra_groupbys=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Topic': [ 'Topic' ],\n",
    "        },\n",
    "        value=None,\n",
    "        description='Groupbys:', **drop_style\n",
    "    ),\n",
    "    min_word_size=widgets.BoundedIntText(\n",
    "        value=2, min=0, max=5, step=1,\n",
    "        description='Min word:', layout=widgets.Layout(width='140px')\n",
    "    ),\n",
    "    only_is_cultural=widgets.ToggleButton(\n",
    "        description='Only Cultural', value=True,\n",
    "        tooltip='Display only \"is_cultural\" treaties', **toggle_style\n",
    "    ),\n",
    "    compute_co_occurance=widgets.ToggleButton(\n",
    "        description='Cooccurrence', value=True,\n",
    "        tooltip='Compute Cooccurrence', **toggle_style\n",
    "    ),\n",
    "    output_format=widgets.Dropdown(\n",
    "        description='Output', value='table',\n",
    "        options={\n",
    "            'Table': 'table',\n",
    "            'Unstack': 'unstack',\n",
    "            'Unstack plot': 'unstack_plot'\n",
    "        }, **drop_style\n",
    "    ),\n",
    "    plot_style=widgets.Dropdown(\n",
    "        options=matplotlib_plot_styles,\n",
    "        value='seaborn-pastel',\n",
    "        description='Style:', **drop_style\n",
    "    ),\n",
    "    n_top=widgets.IntSlider(\n",
    "        value=25, min=2, max=100, step=10,\n",
    "        description='Top/grp #:', # continuous_update=False,\n",
    "    ),\n",
    "    n_min_count=widgets.IntSlider(\n",
    "        value=2, min=1, max=10, step=1,\n",
    "        tooltip='Filter out words with count less than specified value',\n",
    "        description='Min count:', # continuous_update=False,\n",
    "    ),\n",
    "    progress=wf.create_int_progress_widget(min=0, max=10, step=1, value=0, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "ihnw = widgets.interactive(\n",
    "    display_headnote_toplist,\n",
    "    period=hnw.period,\n",
    "    parties=hnw.parties,\n",
    "    extra_groupbys=hnw.extra_groupbys,\n",
    "    only_is_cultural=hnw.only_is_cultural,\n",
    "    n_min_count=hnw.n_min_count,\n",
    "    n_top=hnw.n_top,\n",
    "    min_word_size=hnw.min_word_size,\n",
    "    use_lemma=hnw.use_lemma,\n",
    "    compute_co_occurance=hnw.compute_co_occurance,\n",
    "    remove_stopwords=hnw.remove_stopwords,\n",
    "    output_format=hnw.output_format,\n",
    "    # plot_style=tw.plot_style\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([ hnw.period, hnw.parties, hnw.min_word_size ]),\n",
    "        widgets.VBox([ hnw.extra_groupbys, hnw.n_top, hnw.n_min_count]),\n",
    "        widgets.VBox([ hnw.only_is_cultural, hnw.use_lemma, hnw.remove_stopwords, hnw.compute_co_occurance]),\n",
    "        widgets.VBox([ hnw.output_format, hnw.progress ])\n",
    "    ]\n",
    ")\n",
    "display(widgets.VBox([boxes, ihnw.children[-1]]))\n",
    "ihnw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:blue'>**Mandatory Step**</span>: Prepare Treaty Text Corpora\n",
    "\n",
    "This code cell is a mandatory step for subsequent text corpus statistics. \n",
    "\n",
    "This step processes the treaty text for from given compressed archive (ZIP-file), each language , and stores in an efficient Market-Matrix (MM) corpus format. The corpora is only stored if it is not previously stored, or the \"Force Update\" is specified. Note that an update MUST be forced whenever the treaty archive is updated - otherwise the text in the new archive is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "\n",
    "sort_chained = lambda x, f: list(x).sort(key=f) or x\n",
    "    \n",
    "def ls_sorted(path):\n",
    "    return sort_chained(list(filter(os.path.isfile, glob.glob(path))), os.path.getmtime)\n",
    "       \n",
    "class CompressedFileReader(object):\n",
    "\n",
    "    def __init__(self, archive_pattern, filename_pattern='*.txt'):\n",
    "        self.archive_pattern = archive_pattern\n",
    "        self.filename_pattern = filename_pattern\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for zip_path in glob.glob(self.archive_pattern):\n",
    "            with zipfile.ZipFile(zip_path) as zip_file:\n",
    "                filenames = [ name for name in zip_file.namelist() if fnmatch.fnmatch(name, self.filename_pattern) ]\n",
    "                for filename in filenames:\n",
    "                    try:\n",
    "                        with zip_file.open(filename, 'rU') as text_file:\n",
    "                            content = text_file.read()\n",
    "                            content = gensim.utils.to_unicode(content, 'utf8', errors='ignore')\n",
    "                            content = content.replace('-\\r\\n', '').replace('-\\n', '')\n",
    "                            yield os.path.basename(filename), content\n",
    "                    except:\n",
    "                        print('Unicode error: {}'.format(filename))\n",
    "                        raise\n",
    "                        \n",
    "class TreatyCorpus(TextCorpus):\n",
    "\n",
    "    def __init__(self, content_iterator, dictionary=None, metadata=False, character_filters=None,\n",
    "                 tokenizer=None, token_filters=None, bigram_transform=False\n",
    "    ):\n",
    "        self.content_iterator = content_iterator\n",
    "        \n",
    "        token_filters = [\n",
    "           (lambda tokens: [ x.lower() for x in tokens ]),\n",
    "           (lambda tokens: [ x for x in tokens if any(map(lambda x: x.isalpha(), x)) ])\n",
    "        ] + (token_filters or [])\n",
    "        \n",
    "        #if bigram_transform is True:\n",
    "        #    train_corpus = TreatyCorpus(content_iterator, token_filters=[ x.lower() for x in tokens ])\n",
    "        #    phrases = gensim.models.phrases.Phrases(train_corpus)\n",
    "        #    bigram = gensim.models.phrases.Phraser(phrases)\n",
    "        #    token_filters.append(\n",
    "        #        lambda tokens: bigram[tokens]\n",
    "        #    )           \n",
    "        \n",
    "        super(TreatyCorpus, self).__init__(\n",
    "            input=True,\n",
    "            dictionary=dictionary,\n",
    "            metadata=metadata,\n",
    "            character_filters=character_filters,\n",
    "            tokenizer=tokenizer,\n",
    "            token_filters=token_filters\n",
    "        )\n",
    "        \n",
    "    def getstream(self):\n",
    "        \"\"\"Generate documents from the underlying plain text collection (of one or more files).\n",
    "        Yields\n",
    "        ------\n",
    "        str\n",
    "            Document read from plain-text file.\n",
    "        Notes\n",
    "        -----\n",
    "        After generator end - initialize self.length attribute.\n",
    "        \"\"\"\n",
    "        filenames = []\n",
    "        num_texts = 0\n",
    "        for filename, content in self.content_iterator:\n",
    "            yield content\n",
    "            filenames.append(filename)\n",
    "        self.length = num_texts\n",
    "        self.filenames = filenames\n",
    "        self.document_names = self._compile_document_names()\n",
    "                 \n",
    "    def get_texts(self):\n",
    "        '''\n",
    "        This is mandatory method from gensim.corpora.TextCorpus. Returns stream of documents.\n",
    "        '''\n",
    "        for document in self.getstream():\n",
    "            yield self.preprocess_text(document)\n",
    "            \n",
    "    def preprocess_text(self, text):\n",
    "            \"\"\"Apply `self.character_filters`, `self.tokenizer`, `self.token_filters` to a single text document.\n",
    "            Parameters\n",
    "            ---------\n",
    "            text : str\n",
    "                Document read from plain-text file.\n",
    "            Return\n",
    "            ------\n",
    "            list of str\n",
    "                List of tokens extracted from `text`.\n",
    "            \"\"\"\n",
    "            for character_filter in self.character_filters:\n",
    "                text = character_filter(text)\n",
    "\n",
    "            tokens = self.tokenizer(text)\n",
    "            for token_filter in self.token_filters:\n",
    "                tokens = token_filter(tokens)\n",
    "\n",
    "            return tokens\n",
    "        \n",
    "    def _compile_document_names(self):\n",
    "        \n",
    "        document_names = pd.DataFrame(dict(\n",
    "            document_name=self.filenames,\n",
    "            treaty_id=[ x.split('_')[0] for x in self.filenames ]\n",
    "        )).reset_index().rename(columns={'index': 'document_id'})\n",
    "        \n",
    "        document_names = document_names.set_index('document_id')   \n",
    "        dupes = document_names.groupby('treaty_id').size().loc[lambda x: x > 1]\n",
    "        \n",
    "        if len(dupes) > 0:\n",
    "            logger.critical('Warning! Duplicate treaties found in corpus: {}'.format(' '.join(list(dupes.index))))\n",
    "            \n",
    "        return document_names\n",
    "\n",
    "class MmCorpusStatisticsService():\n",
    "    \n",
    "    def __init__(self, corpus, dictionary, language):\n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        self.stopwords = nltk.corpus.stopwords.words(language[1])\n",
    "        _ = dictionary[0]\n",
    "        \n",
    "    def get_total_token_frequencies(self):\n",
    "        dictionary = self.corpus.dictionary\n",
    "        freqencies = np.zeros(len(dictionary.id2token))\n",
    "        document_stats = []\n",
    "        for document in corpus:\n",
    "            for i, f in document:\n",
    "                freqencies[i] += f\n",
    "        return freqencies\n",
    "\n",
    "    def get_document_token_frequencies(self):\n",
    "        from itertools import chain\n",
    "        '''\n",
    "        Returns a DataFrame with per document token frequencies i.e. \"melts\" doc-term matrix\n",
    "        '''\n",
    "        data = ((document_id, x[0], x[1]) for document_id, values in enumerate(self.corpus) for x in values )\n",
    "        pd = pd.DataFrame(list(zip(*data)), columns=['document_id', 'token_id', 'count'])\n",
    "        pd = pd.merge(self.corpus.document_names, left_on='document_id', right_index=True)\n",
    "\n",
    "        return pd\n",
    "\n",
    "    def compute_word_frequencies(self, remove_stopwords):\n",
    "        id2token = self.dictionary.id2token\n",
    "        term_freqencies = np.zeros(len(id2token))\n",
    "        document_stats = []\n",
    "        for document in self.corpus:\n",
    "            for i, f in document:\n",
    "                term_freqencies[i] += f\n",
    "        stopwords = set(self.stopwords).intersection(set(id2token.values()))\n",
    "        df = pd.DataFrame({\n",
    "            'token_id': list(id2token.keys()),\n",
    "            'token': list(id2token.values()),\n",
    "            'frequency': term_freqencies,\n",
    "            'dfs':  list(self.dictionary.dfs.values())\n",
    "        })\n",
    "        df['is_stopword'] = df.token.apply(lambda x: x in stopwords)\n",
    "        if remove_stopwords is True:\n",
    "            df = df.loc[(df.is_stopword==False)]\n",
    "        df['frequency'] = df.frequency.astype(np.int64)\n",
    "        df = df[['token_id', 'token', 'frequency', 'dfs', 'is_stopword']].sort_values('frequency', ascending=False)\n",
    "        return df.set_index('token_id')\n",
    "\n",
    "    def compute_document_stats(self):\n",
    "        id2token = self.dictionary.id2token\n",
    "        stopwords = set(self.stopwords).intersection(set(id2token.values()))\n",
    "        df = pd.DataFrame({\n",
    "            'document_id': self.corpus.index,\n",
    "            'document_name': self.corpus.document_names.document_name,\n",
    "            'treaty_id': self.corpus.document_names.treaty_id,\n",
    "            'size': [ sum(list(zip(*document))[1]) for document in self.corpus],\n",
    "            'stopwords': [ sum([ v for (i,v) in document if id2token[i] in self.stopwords]) for document in self.corpus],\n",
    "        }).set_index('document_name')\n",
    "        df[['size', 'stopwords']] = df[['size', 'stopwords']].astype('int')\n",
    "        return df\n",
    "\n",
    "    def compute_word_stats(self):\n",
    "        df = self.compute_document_stats()[['size', 'stopwords']]\n",
    "        df_agg = df.agg(['count', 'mean', 'std', 'min', 'median', 'max', 'sum']).reset_index()\n",
    "        legend_map = {\n",
    "            'count': 'Documents',\n",
    "            'mean': 'Mean words',\n",
    "            'std': 'Std',\n",
    "            'min': 'Min',\n",
    "            'median': 'Median',\n",
    "            'max': 'Max',\n",
    "            'sum': 'Sum words'\n",
    "        }\n",
    "        df_agg['index'] = df_agg['index'].apply(lambda x: legend_map[x]).astype('str')\n",
    "        df_agg = df_agg.set_index('index')\n",
    "        df_agg[df_agg.columns] = df_agg[df_agg.columns].astype('int')\n",
    "        return df_agg.reset_index()\n",
    "    \n",
    "#@staticmethod\n",
    "\n",
    "class ExtMmCorpus(gensim.corpora.MmCorpus):\n",
    "    \"\"\"Extension of MmCorpus that allow TF normalization based on document length.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def norm_tf_by_D(doc):\n",
    "        D = sum([x[1] for x in doc])\n",
    "        return doc if D == 0 else map(lambda tf: (tf[0], tf[1]/D), doc)\n",
    "\n",
    "    def __init__(self, fname):\n",
    "        gensim.corpora.MmCorpus.__init__(self, fname)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in gensim.corpora.MmCorpus.__iter__(self):\n",
    "            yield self.norm_tf_by_D(doc)\n",
    "\n",
    "    def __getitem__(self, docno):\n",
    "        return self.norm_tf_by_D(gensim.corpora.MmCorpus.__getitem__(self, docno))\n",
    "\n",
    "class TreatyCorpusSaveLoad():\n",
    "\n",
    "    def __init__(self, source_folder, lang):\n",
    "        \n",
    "        self.mm_filename = os.path.join(source_folder, 'corpus_{}.mm'.format(lang))\n",
    "        self.dict_filename = os.path.join(source_folder, 'corpus_{}.dict.gz'.format(lang))\n",
    "        self.document_index = os.path.join(source_folder, 'corpus_{}_documents.csv'.format(lang))\n",
    "        \n",
    "    def store_as_mm_corpus(self, treaty_corpus):\n",
    "        \n",
    "        gensim.corpora.MmCorpus.serialize(self.mm_filename, treaty_corpus, id2word=treaty_corpus.dictionary.id2token)\n",
    "        treaty_corpus.dictionary.save(self.dict_filename)\n",
    "        treaty_corpus.document_names.to_csv(self.document_index, sep='\\t')\n",
    "\n",
    "    def load_mm_corpus(self, normalize_by_D=False):\n",
    "    \n",
    "        corpus_type = ExtMmCorpus if normalize_by_D else gensim.corpora.MmCorpus\n",
    "        corpus = corpus_type(self.mm_filename)\n",
    "        corpus.dictionary = gensim.corpora.Dictionary.load(self.dict_filename)\n",
    "        corpus.document_names = pd.read_csv(self.document_index, sep='\\t').set_index('document_id')  \n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    def exists(self):\n",
    "        return os.path.isfile(self.mm_filename) and \\\n",
    "            os.path.isfile(self.dict_filename) and \\\n",
    "            os.path.isfile(self.document_index)\n",
    "\n",
    "def store_mm_corpora(source_path, force, languages):\n",
    "    \n",
    "    try:\n",
    "        print('Current archive:{}'.format(source_path))\n",
    "        tokenizer = nltk.tokenize.word_tokenize\n",
    "        source_folder = os.path.split(source_path)[0]\n",
    "        for language in languages.split(','):\n",
    "            loader = TreatyCorpusSaveLoad(source_folder, language)\n",
    "            if not loader.exists() or force:\n",
    "                print('Processing: {}'.format(language))\n",
    "                stream = CompressedFileReader(source_path, filename_pattern='*_{}*.txt'.format(language))\n",
    "                treaty_corpus = TreatyCorpus(stream, tokenizer=tokenizer)        \n",
    "                loader.store_as_mm_corpus(treaty_corpus)\n",
    "        print('Corpus is up-to-date!')\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "current_archives = (ls_sorted('./data/*.zip') or [])\n",
    "\n",
    "cuw = BaseWidgetUtility(\n",
    "    source_path=widgets.Dropdown(\n",
    "        options=current_archives,\n",
    "        value=current_archives[-1] if len(current_archives) else None,\n",
    "        description='Corpus:' #, **drop_style\n",
    "    ),\n",
    "    force_corpus_update=widgets.ToggleButton(\n",
    "        description='Force Update',\n",
    "        tooltip='Force refresh saved corpus cache (a performance feature). Use when ZIP-archive has been updated.',\n",
    "        value=False #, **toggle_style\n",
    "    )\n",
    ")\n",
    "\n",
    "icuw = widgets.interactive(\n",
    "    store_mm_corpora,\n",
    "    source_path=cuw.source_path,\n",
    "    force=cuw.force_corpus_update,\n",
    "    languages='en,it,fr,de'\n",
    ")\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([cuw.source_path, cuw.force_corpus_update]), icuw.children[-1]]))\n",
    "\n",
    "icuw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Basic Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code \n",
    "\n",
    "corpus = None\n",
    "def display_token_toplist(source_folder, language, statistics='', remove_stopwords=False):\n",
    "    global tlw, corpus\n",
    "    try:\n",
    "        \n",
    "        tlw.progress.value = 1\n",
    "\n",
    "        corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0]).load_mm_corpus()\n",
    "\n",
    "        tlw.progress.value = 2\n",
    "        service = MmCorpusStatisticsService(corpus, dictionary=corpus.dictionary, language=language)\n",
    "\n",
    "        print(\"Corpus consists of {} documents, {} words in total and a vocabulary size of {} tokens.\"\\\n",
    "                  .format(len(corpus), corpus.dictionary.num_pos, len(corpus.dictionary)))\n",
    "\n",
    "        tlw.progress.value = 3\n",
    "        if statistics == 'word_freqs':\n",
    "            display(service.compute_word_frequencies(remove_stopwords))\n",
    "        elif statistics == 'documents':\n",
    "            display(service.compute_document_stats())\n",
    "        elif statistics == 'word_count':\n",
    "            display(service.compute_word_stats())\n",
    "        else:\n",
    "            print('Unknown: ' + statistics)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    tlw.progress.value = 5\n",
    "    tlw.progress.value = 0\n",
    "    \n",
    "tlw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    statistics=widgets.Dropdown(\n",
    "        options={\n",
    "            'Word freqs': 'word_freqs',\n",
    "            'Documents': 'documents',\n",
    "            'Word count': 'word_count'\n",
    "        },\n",
    "        value='word_count',\n",
    "        description='Statistics:', **drop_style\n",
    "    ),    \n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    progress=wf.create_int_progress_widget(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itlw = widgets.interactive(\n",
    "    display_token_toplist,\n",
    "    source_folder='./data',\n",
    "    language=tlw.language,\n",
    "    statistics=tlw.statistics,\n",
    "    remove_stopwords=tlw.remove_stopwords\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        tlw.language, tlw.statistics, tlw.remove_stopwords, tlw.progress\n",
    "    ]\n",
    ")\n",
    "display(widgets.VBox([boxes, itlw.children[-1]]))\n",
    "itlw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Treaty Keyword Extraction (using TF-IDF weighing)\n",
    "- [ML Wiki.org](http://mlwiki.org/index.php/TF-IDF)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- Spärck Jones, K. (1972). \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\".\n",
    "- Manning, C.D.; Raghavan, P.; Schutze, H. (2008). \"Scoring, term weighting, and the vector space model\". ([PDF](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf))\n",
    "- https://markroxor.github.io/blog/tfidf-pivoted_norm/\n",
    "$\\frac{tf-idf}{\\sqrt(rowSums( tf-idf^2 ) )}$\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/pivoted-normalized-document-length-1.html\n",
    "\n",
    "Neural Network Methods in Natural Language Processing, Yoav Goldberg:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from scipy.sparse import csr_matrix\n",
    "%timeit\n",
    "\n",
    "    \n",
    "def get_top_tfidf_words(data, n_top=5):\n",
    "    top_list = data.groupby(['treaty_id'])\\\n",
    "        .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "        .reset_index(level=0, drop=True)\n",
    "    return top_list\n",
    "\n",
    "def compute_tfidf_scores(corpus, dictionary, smartirs='ntc'):\n",
    "    #model = gensim.models.logentropy_model.LogEntropyModel(corpus, normalize=True)\n",
    "    model = gensim.models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary, normalize=True) #, smartirs=smartirs)\n",
    "    rows, cols, scores = [], [], []\n",
    "    for r, document in enumerate(corpus): \n",
    "        vector = model[document]\n",
    "        c, v = zip(*vector)\n",
    "        rows += (len(c) * [ int(r) ])\n",
    "        cols += c\n",
    "        scores += v\n",
    "        \n",
    "    return csr_matrix((scores, (rows, cols)))\n",
    "    \n",
    "if True: #'tfidf_cache' not in globals():\n",
    "    tfidf_cache = {\n",
    "    }\n",
    "    \n",
    "def display_tfidf_scores(source_folder, language, period, n_top=5, threshold=0.001):\n",
    "    \n",
    "    global state, tfw, tfidf_cache\n",
    "    \n",
    "    try:\n",
    "        treaties = state.treaties\n",
    "\n",
    "        tfw.progress.value = 0\n",
    "        tfw.progress.value += 1\n",
    "        if language[0] not in tfidf_cache.keys():\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0])\\\n",
    "                .load_mm_corpus(normalize_by_D=True)\n",
    "            document_names = corpus.document_names\n",
    "            dictionary = corpus.dictionary\n",
    "            _ = dictionary[0]\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            A = compute_tfidf_scores(corpus, dictionary)\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            scores = pd.DataFrame(\n",
    "                [ (i, j, dictionary.id2token[j], A[i, j]) for i, j in zip(*A.nonzero())],\n",
    "                columns=['document_id', 'token_id', 'token', 'score']\n",
    "            )\n",
    "            tfw.progress.value += 1\n",
    "            scores = scores.merge(document_names, how='inner', left_on='document_id', right_index=True)\\\n",
    "                .drop(['document_id', 'token_id', 'document_name'], axis=1)\n",
    "\n",
    "            scores = scores[['treaty_id', 'token', 'score']]\\\n",
    "                .sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "            tfidf_cache[language[0]] = scores\n",
    "\n",
    "        scores = tfidf_cache[language[0]]\n",
    "        if threshold > 0:\n",
    "            scores = scores.loc[scores.score >= threshold]\n",
    "\n",
    "        tfw.progress.value += 1\n",
    "\n",
    "        #scores = get_top_tfidf_words(scores, n_top=5)\n",
    "        #scores = scores.groupby(['treaty_id']).sum() \n",
    "\n",
    "        scores = scores.groupby(['treaty_id'])\\\n",
    "            .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            .set_index('treaty_id')\n",
    "\n",
    "        if period is not None:\n",
    "            periods = state.treaties[period]\n",
    "            scores = scores.merge(periods.to_frame(), left_index=True, right_index=True, how='inner')\\\n",
    "                .groupby([period, 'token']).score.agg([np.mean])\\\n",
    "                .reset_index().rename(columns={0:'score'}) #.sort_values('token')\n",
    "\n",
    "        #['token'].apply(' '.join)\n",
    "\n",
    "        display(scores)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    tfw.progress.value = 0\n",
    "\n",
    "#if 'tfidf_scores' not in globals():\n",
    "#    tfidf_scores = compute_document_tfidf(corpus, corpus.dictionary, state.treaties)\n",
    "#    tfidf_scores = tfidf_scores.sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "tfw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    n_top=widgets.IntSlider(\n",
    "        value=5, min=1, max=25, step=1,\n",
    "        description='Top #:',\n",
    "        continuous_update=False\n",
    "    ),\n",
    "    threshold=widgets.FloatSlider(\n",
    "        value=0.001, min=0.0, max=0.5, step=0.01,\n",
    "        description='Threshold:',\n",
    "        tooltip='Word having a TF-IDF score below this value is filtered out',\n",
    "        continuous_update=False,\n",
    "        readout_format='.3f',\n",
    "    ), \n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Output:', **drop_style\n",
    "    ),\n",
    "    progress=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itfw = widgets.interactive(\n",
    "    display_tfidf_scores,\n",
    "    source_folder='./data',\n",
    "    language=tfw.language,\n",
    "    n_top=tfw.n_top,\n",
    "    threshold=tfw.threshold,\n",
    "    period=tfw.period\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([tfw.language, tfw.period]),\n",
    "        widgets.VBox([tfw.n_top, tfw.threshold]),\n",
    "        widgets.VBox([tfw.progress, tfw.output])\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(widgets.VBox([boxes, itfw.children[-1]]))\n",
    "itfw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Network Visualization of Signed Treaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": [
     63,
     184
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37ade9342b34e11837338047eabb178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(HBox(children=(Dropdown(description='Parties:', layout=Layout(width='220px'), options={'PartyOf5': ['FRANCE', 'GERMU', 'ITALY', 'GERMAN', 'UK', 'GERME', 'GERMW', 'INDIA'], 'UK': ['UK'], 'France': ['FRANCE'], 'Italy': ['ITALY'], 'India': ['INDIA'], '(all)': None, 'Germany': ['GERMU', 'GERMAN', 'GERME', 'GERMW']}, value=None), ToggleButton(value=True, description='Only Cultural', layout=Layout(width='100px'), tooltip='Display only \"is_cultural\" treaties'))), Dropdown(description='Period:', index=2, layout=Layout(width='200px'), options={'1940 to 1944': (1940, 1944), '1967 to 1972': (1967, 1972), '1919 to 1939': (1919, 1939), '1919 to 1944': (1919, 1944), '1956 to 1966': (1956, 1966), '1945 to 1955': (1945, 1955)}, value=(1919, 1939)), IntProgress(value=0, layout=Layout(width='99%'), max=4))), VBox(children=(Dropdown(description='Layout', layout=Layout(width='220px'), options={'Graphviz (fdp)': 'graphviz_fdp', 'Graph-Tool (fruchterman_reingold)': 'graphtool_fruchterman_reingold', 'Graphviz (circo)': 'graphviz_circo', 'Graphviz (dot)': 'graphviz_dot', 'Graphviz (neato)': 'graphviz_neato', 'Graph-Tool (sfdp)': 'graphtool_sfdp', 'Graph-Tool (arf)': 'graphtool_arf', 'Graphviz (sfdp)': 'graphviz_sfdp', 'Circular': 'circular_layout', 'Kamada-Kawai': 'kamada_kawai_layout', 'Fruchterman-Reingold': 'spring_layout', 'Shell': 'shell_layout', 'Eigenvectors of Laplacian': 'spectral_layout'}, value='graphviz_fdp'), Dropdown(description='Name:', index=3, layout=Layout(width='220px'), options={'WTI Name': 'party_name', 'WTI Code': 'party', 'Country': 'party_country', 'WTI Short': 'short_name', 'CC': 'country_code'}, value='short_name'), Dropdown(description='Output:', index=2, layout=Layout(width='220px'), options={'Graphviz': 'network_graphviz', 'List': 'table', 'Bokeh': 'network_bokeh'}, value='network_bokeh'), Dropdown(description='Color:', layout=Layout(width='220px'), options={'BuPu': 'BuPu', 'PRGn': 'PRGn', 'PuRd': 'PuRd', 'PuBu': 'PuBu', 'YlGnBu': 'YlGnBu', 'Greys': 'Greys', 'Set1': 'Set1', 'BuGn': 'BuGn', 'RdYlBu': 'RdYlBu', 'RdYlGn': 'RdYlGn', 'Accent': 'Accent', 'Colorblind': 'Colorblind', 'Viridis': 'Viridis', 'Dark2': 'Dark2', 'Category10': 'Category10', 'Plasma': 'Plasma', 'Pastel1': 'Pastel1', 'BrBG': 'BrBG', 'Set3': 'Set3', 'Oranges': 'Oranges', 'Reds': 'Reds', 'YlGn': 'YlGn', 'Pastel2': 'Pastel2', 'Category20': 'Category20', 'Purples': 'Purples', 'Inferno': 'Inferno', 'PuBuGn': 'PuBuGn', 'Category20b': 'Category20b', 'YlOrRd': 'YlOrRd', 'Category20c': 'Category20c', 'RdBu': 'RdBu', 'Magma': 'Magma', 'PiYG': 'PiYG', 'PuOr': 'PuOr', 'RdPu': 'RdPu', 'RdGy': 'RdGy', 'Spectral': 'Spectral', 'Paired': 'Paired', 'Greens': 'Greens', 'Set2': 'Set2', 'Blues': 'Blues', 'GnBu': 'GnBu', 'OrRd': 'OrRd', 'YlOrBr': 'YlOrBr'}, value='BuPu'))), HBox(children=(FloatSlider(value=0.1, continuous_update=False, description='K', layout=Layout(height='160px', width='30px'), max=1.0, min=0.01, orientation='vertical', step=0.01), IntSlider(value=1, continuous_update=False, description='C', layout=Layout(height='160px', width='30px'), orientation='vertical'), FloatSlider(value=1.1, continuous_update=False, description='p', layout=Layout(height='160px', width='30px'), max=2.0, min=0.01, orientation='vertical', step=0.01), IntRangeSlider(value=(20, 40), continuous_update=False, description='Node size', layout=Layout(height='160px', width='70px'), min=5, orientation='vertical'))), VBox(children=(ToggleButton(value=False, description='Refresh', layout=Layout(width='100px'), tooltip='Update plot'),)))), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize treaties\n",
    "import bokeh.palettes as pals\n",
    "\n",
    "periods_division = [\n",
    "    (1919, 1939), (1940, 1944), (1919, 1944), (1945, 1955), (1956, 1966), (1967, 1972)\n",
    "]\n",
    "%run ./network_utility\n",
    "%run ./plot_utility\n",
    "def display_party_network(\n",
    "    parties,\n",
    "    period,\n",
    "    only_is_cultural=True,\n",
    "    layout_algorithm='',\n",
    "    C=1.0,\n",
    "    K=0.10,\n",
    "    p1=0.10,\n",
    "    output='network_bokeh',\n",
    "    party_name='party',\n",
    "    node_size_range=[40,60],\n",
    "    refresh=False,\n",
    "    palette_name=None\n",
    "):\n",
    "    global state, zn\n",
    "    \n",
    "    figsize=(900, 900)\n",
    "    palette_id = max(pals.all_palettes[palette_name].keys())\n",
    "    palette = pals.RdYlBu[11] if palette_name is None else pals.all_palettes[palette_name][palette_id]\n",
    "    \n",
    "    zn.refresh.value = False\n",
    "    zn.progress.value = 1\n",
    "    \n",
    "    data = state.stacked_treaties.copy()\n",
    "    \n",
    "    data = data.loc[(data.signed_period!='other')]\n",
    "\n",
    "    if only_is_cultural:\n",
    "        data = data.loc[(data.is_cultural==True)]\n",
    "        \n",
    "    if isinstance(parties, list):\n",
    "        data = data.loc[(data.party.isin(parties))]\n",
    "    else:\n",
    "        data = data.loc[(data.reversed==False)]\n",
    "        \n",
    "    data = data.loc[(data.signed_period != period)]\n",
    "    data = data.loc[(data.signed_year.between(period[0], period[1]))]\n",
    "    data = data.sort_values('signed')\n",
    "    zn.progress.value = 2\n",
    "    data = data.groupby(['party', 'party_other']).size().reset_index().rename(columns={0: 'weight'})\n",
    "    data = data[[ 'party', 'party_other', 'weight']]\n",
    "\n",
    "    if party_name != 'party':\n",
    "        for column in ['party', 'party_other']:\n",
    "            data[column] = data[column].apply(lambda x: state.get_party_name(x, party_name))\n",
    "\n",
    "    edges_data = [ tuple(x) for x in data.values ]\n",
    "\n",
    "    #network = NetworkUtility.create_network_from_xyw_list(edges_data)\n",
    "    \n",
    "    G = nx.Graph(K=K)\n",
    "    G.add_weighted_edges_from(edges_data)\n",
    "\n",
    "    zn.progress.value = 3\n",
    "        \n",
    "    if output == 'network_graphviz':\n",
    "        import graphviz, pydotplus\n",
    "        def apply_styles(graph, styles):\n",
    "            graph.graph_attr.update(\n",
    "                ('graph' in styles and styles['graph']) or {}\n",
    "            )\n",
    "            graph.node_attr.update(\n",
    "                ('nodes' in styles and styles['nodes']) or {}\n",
    "            )\n",
    "            graph.edge_attr.update(\n",
    "                ('edges' in styles and styles['edges']) or {}\n",
    "            )\n",
    "            return graph\n",
    "        styles = {\n",
    "            'graph': {\n",
    "                'label': 'Graph',\n",
    "                'fontsize': '16',\n",
    "                'fontcolor': 'white',\n",
    "                'bgcolor': '#333333',\n",
    "                'rankdir': 'BT',\n",
    "            },\n",
    "            'nodes': {\n",
    "                'fontname': 'Helvetica',\n",
    "                'shape': 'hexagon',\n",
    "                'fontcolor': 'white',\n",
    "                'color': 'white',\n",
    "                'style': 'filled',\n",
    "                'fillcolor': '#006699',\n",
    "            },\n",
    "            'edges': {\n",
    "                'style': 'dashed',\n",
    "                'color': 'white',\n",
    "                'arrowhead': 'open',\n",
    "                'fontname': 'Courier',\n",
    "                'fontsize': '12',\n",
    "                'fontcolor': 'white',\n",
    "            }\n",
    "        }\n",
    "        P=nx.nx_pydot.to_pydot(G)\n",
    "        P.format = 'svg'\n",
    "        #if root is not None :\n",
    "        #    P.set(\"root\",make_str(root))\n",
    "        D=P.create_dot(prog='circo')\n",
    "        if D==\"\":\n",
    "            return\n",
    "        Q=pydotplus.graph_from_dot_data(D)\n",
    "        #Q = apply_styles(Q, styles)\n",
    "        from IPython.display import Image\n",
    "        I = Image(Q.create_png())\n",
    "        display(I)\n",
    "        \n",
    "    elif output == 'network_bokeh':\n",
    "        args = PlotNetworkUtility.layout_args(layout_algorithm, network=G, scale=1.0, k=K)\n",
    "        if layout_algorithm.startswith('graphtool'):\n",
    "            global layout_gt, G_gt\n",
    "            import graph_tool.draw as gt_draw\n",
    "            import graph_tool.all as gt\n",
    "\n",
    "            G_gt = GraphToolUtility.nx2gt(G)\n",
    "            G_gt.set_directed(False)\n",
    "            weights = G_gt.edge_properties['weight']\n",
    "            N = len(G)\n",
    "            if layout_algorithm.endswith('sfdp'):\n",
    "                layout_gt = gt_draw.sfdp_layout(\n",
    "                    G_gt, eweight=weights, K=K, C=C, p=p1\n",
    "                )\n",
    "            elif layout_algorithm.endswith('arf'):\n",
    "                layout_gt = gt_draw.arf_layout(G_gt, weight=weights, d=K, a=C)\n",
    "            elif layout_algorithm.endswith('fruchterman_reingold'):\n",
    "                layout_gt = gt_draw.fruchterman_reingold_layout(G_gt, weight=weights, a=(2.0*N*K), r=2.0*C)\n",
    "                \n",
    "            if False:  # graph-tool plot\n",
    "                v_text = G_gt.vertex_properties['id']\n",
    "                v_degrees_p = G_gt.degree_property_map('out')\n",
    "                v_degrees_p.a = np.sqrt(v_degrees_p.a)+2\n",
    "                v_size_p = gt.prop_to_size(v_degrees_p, node_size_range[0], node_size_range[1])\n",
    "                e_size_p = gt.prop_to_size(weights, 1.0, 4.0)\n",
    "                #state = gt.minimize_blockmodel_dl(G_gt)\n",
    "                #state.draw(\n",
    "                #c = gt.all.closeness(G_gt)\n",
    "\n",
    "                v_blocks = gt.minimize_blockmodel_dl(G_gt).get_blocks()\n",
    "                print(list(v_blocks))\n",
    "                plot_color = G_gt.new_vertex_property('vector<double>')\n",
    "                G_gt.vertex_properties['plot_color'] = plot_color\n",
    "                for v_i, v in enumerate(G_gt.vertices()):\n",
    "                    scolor = palette[v_blocks[v_i]]\n",
    "                    plot_color[v] = tuple(int(scolor[i:i+2], 16) for i in (1, 3, 5)) + (1,)\n",
    "\n",
    "                gt_draw.graph_draw(\n",
    "                    G_gt,\n",
    "                    #vorder=c,\n",
    "                    pos=layout_gt,\n",
    "                    output_size=(1000, 1000),\n",
    "                    vertex_text=v_text,\n",
    "                    vertex_color=[1,1,1,0],\n",
    "                    vertex_fill_color=plot_color,\n",
    "                    vertex_size=v_degrees_p,\n",
    "                    edge_pen_width=e_size_p\n",
    "                )\n",
    "                return\n",
    "            \n",
    "            layout = { G_gt.vertex_properties['id'][i]: layout_gt[i] for i in G_gt.vertices() }\n",
    "\n",
    "        elif layout_algorithm.startswith('graphviz'):\n",
    "            def norm_layout(layout):\n",
    "                max_xy = max([ max(x,y) for x,y in layout.values()])\n",
    "                layout = { n: (layout[n][0]/max_xy, layout[n][1]/max_xy) for n in layout.keys() }\n",
    "                return layout\n",
    "            \n",
    "            G.graph['K'] = K\n",
    "            G.graph['overlap'] = False\n",
    "            engine = layout_algorithm.split('_')[1]\n",
    "            args={} # \"-Goverlap=scalexy -Gepsilon=5 -GK{}\".format(k).replace(',','.')\n",
    "            layout = nx.nx_pydot.pydot_layout(G, prog=engine, args=args)\n",
    "            layout = norm_layout(layout)\n",
    "            \n",
    "        else:\n",
    "            layout = (get_layout_function(layout_algorithm))(G, **args)\n",
    "            \n",
    "        zn.progress.value = 4\n",
    "        p = PlotNetworkUtility.plot_network(\n",
    "            network=G,\n",
    "            layout=layout,\n",
    "            scale=1.0,\n",
    "            text_opts=dict(\n",
    "                x_offset=0, #y_offset=5,\n",
    "                level='overlay',\n",
    "                text_align='center',\n",
    "                text_baseline='middle',\n",
    "                render_mode='canvas',\n",
    "                text_font=\"Tahoma\",\n",
    "                text_font_size=\"9pt\",\n",
    "                text_color='black'\n",
    "            ),\n",
    "            node_opts= dict(\n",
    "                color=None, #'green',\n",
    "                level='overlay',\n",
    "                alpha=1.0\n",
    "            ),\n",
    "            line_opts=dict(\n",
    "                color='green',\n",
    "                alpha=0.5\n",
    "            ),\n",
    "            figsize=figsize,\n",
    "            node_size_source=list(G.degree()),\n",
    "            node_size_range=node_size_range,\n",
    "            palette=palette,  # Spectral[9]\n",
    "            x_axis_type=None,\n",
    "            y_axis_type=None,\n",
    "            background_fill_color='white'\n",
    "        )\n",
    "        zn.progress.value = 6\n",
    "        bp.show(p)\n",
    "        \n",
    "    elif output == 'table':\n",
    "        display(data)\n",
    "    else:\n",
    "        display(pivot_ui(data))\n",
    "        \n",
    "    zn.progress.value = 0\n",
    "\n",
    "zn = BaseWidgetUtility(\n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '{} to {}'.format(x[0], x[1]): x for x in list(set(period_divisions[0] + period_divisions[1]))\n",
    "        },\n",
    "        value=period_divisions[0][0],\n",
    "        description='Period:', layout=widgets.Layout(width='200px')\n",
    "    ),\n",
    "    parties=widgets.Dropdown(\n",
    "        description='Parties:',\n",
    "        options={\n",
    "            '(all)': None,\n",
    "            'PartyOf5': parties_of_interest,\n",
    "            'France': [ 'FRANCE' ],\n",
    "            'Italy': [ 'ITALY' ],\n",
    "            'UK': [ 'UK' ],\n",
    "            'India': [ 'INDIA' ],\n",
    "            'Germany': [ 'GERMU', 'GERMAN', 'GERME', 'GERMW' ]\n",
    "        },\n",
    "        value=None,\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    party_name=widgets.Dropdown(\n",
    "        description='Name:',\n",
    "        options={\n",
    "            'WTI Code': 'party',\n",
    "            'WTI Name': 'party_name',\n",
    "            'WTI Short': 'short_name',\n",
    "            'CC': 'country_code',\n",
    "            'Country': 'party_country'\n",
    "        },\n",
    "        value='short_name',\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    palette=widgets.Dropdown(\n",
    "        description='Color:',\n",
    "        options={\n",
    "            palette_name: palette_name\n",
    "                    for palette_name in bokeh.palettes.all_palettes.keys()\n",
    "                        if any([ len(x) > 7 for x in bokeh.palettes.all_palettes[palette_name].values()])\n",
    "        },\n",
    "        #value='short_name',\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    C=widgets.IntSlider(\n",
    "        description='C', min=0, max=100, step=1, value=1,\n",
    "        continuous_update=False, orientation='vertical', layout=widgets.Layout(width='30px', height='160px')\n",
    "    ),\n",
    "    K=widgets.FloatSlider(\n",
    "        description='K', min=0.01, max=1.0, step=0.01, value=0.10,\n",
    "        continuous_update=False, orientation='vertical', layout=widgets.Layout(width='30px', height='160px')\n",
    "    ),\n",
    "    p=widgets.FloatSlider(\n",
    "        description='p', min=0.01, max=2.0, step=0.01, value=1.10,\n",
    "        continuous_update=False, orientation='vertical', layout=widgets.Layout(width='30px', height='160px')\n",
    "    ),\n",
    "    node_size_range=widgets.IntRangeSlider(\n",
    "        description='Node size',\n",
    "        value=[20, 40], min=5, max=100, step=1,\n",
    "        continuous_update=False, orientation='vertical', layout=widgets.Layout(width='70px', height='160px')\n",
    "    ),\n",
    "    only_is_cultural=widgets.ToggleButton(\n",
    "        description='Only Cultural', value=True,\n",
    "        tooltip='Display only \"is_cultural\" treaties', layout=widgets.Layout(width='100px')\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        description='Output:',\n",
    "        options={\n",
    "            'Bokeh': 'network_bokeh',\n",
    "            'Graphviz': 'network_graphviz',\n",
    "            'List': 'table'\n",
    "        },\n",
    "        value='network_bokeh',\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    layout=widgets.Dropdown(\n",
    "        description='Layout',\n",
    "        options={\n",
    "            x[0]: x[1] for x in list(zip(layout_function_name.values(), layout_function_name.keys())) +\n",
    "                    [('Graphviz ({})'.format(x), 'graphviz_{}'.format(x))\n",
    "                     for x in  ['neato', 'dot', 'circo', 'fdp', 'sfdp']\n",
    "                    ] + # 'wc', 'gvcolor', 'ccomps', 'sccmap', 'twopi', 'gvpr', 'nop', 'tred' , 'acyclic'\n",
    "                    [\n",
    "                        ('Graph-Tool ({})'.format(x), 'graphtool_{}'.format(x))\n",
    "                             for x in  ['sfdp', 'arf', 'fruchterman_reingold']\n",
    "                    ]\n",
    "        }, \n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    progress=wf.create_int_progress_widget(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"99%\")),\n",
    "    refresh=widgets.ToggleButton(\n",
    "        description='Refresh', value=False,\n",
    "        tooltip='Update plot', layout=widgets.Layout(width='100px')\n",
    "    ),\n",
    ") \n",
    "#search_text = widgets.Text(description = 'Search') \n",
    "#search_result = widgets.Select(description = 'Select table')\n",
    "\n",
    "#def search_action(sender):\n",
    "#    phrase = search_text.value\n",
    "#    df = search(phrase) # A function that returns the results in a pandas df\n",
    "#    titles = df['title'].tolist()\n",
    "#    with search_result.hold_trait_notifications():\n",
    "#        search_result.options = titles\n",
    "        \n",
    "wn = widgets.interactive(\n",
    "    display_party_network,\n",
    "    parties=zn.parties,\n",
    "    period=zn.period,\n",
    "    only_is_cultural=zn.only_is_cultural,\n",
    "    layout_algorithm=zn.layout,\n",
    "    C=zn.C,\n",
    "    K=zn.K,\n",
    "    p1=zn.p,\n",
    "    output=zn.output,\n",
    "    party_name=zn.party_name,\n",
    "    node_size_range=zn.node_size_range,\n",
    "    refresh=zn.refresh,\n",
    "    palette_name=zn.palette\n",
    ")\n",
    "boxes = widgets.HBox([\n",
    "    widgets.VBox([widgets.HBox([zn.parties, zn.only_is_cultural]), zn.period, zn.progress]),\n",
    "    widgets.VBox([zn.layout, zn.party_name, zn.output, zn.palette]),\n",
    "    widgets.HBox([zn.K, zn.C, zn.p, zn.node_size_range]),\n",
    "    widgets.VBox([zn.refresh]),\n",
    "])\n",
    "\n",
    "display(widgets.VBox([boxes, wn.children[-1]]))\n",
    "\n",
    "wn.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red'>IGNORE EVERYTHING BELOW</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Headnote Co-Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class CoOccurrance():\n",
    "\n",
    "    def __init__(self, tokenizer, stopwords=None, lemmatizer=None, min_word_size=2):\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenizer,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_word_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)) \n",
    "        ]\n",
    "        \n",
    "        if stopwords is not None:\n",
    "            self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        if lemmatizer is not None:\n",
    "            self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "\n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def compute(self, headnotes):\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "    def compute_co_occurrence(self, treaties, pos_tags, only_cultural_treaties=False):\n",
    "\n",
    "        # Filter out tags based on treaties of interest\n",
    "        pos_tags = pos_tags.merge(treaties, how='inner', left_on='treaty_id', right_index=True)[[]]\n",
    "        \n",
    "        if only_cultural_treaties:\n",
    "            df_pos_tags = df_pos_tags[(df_pos_tags.is_cultural.str.contains('yes',na=False))]\n",
    "\n",
    "        # Self join of words within same treaty\n",
    "        df_co_occurrence = pd.merge(df_pos_tags, df_pos_tags, how='inner', left_on='treaty_id', right_on='treaty_id')\n",
    "        # Only consider a specific poir once\n",
    "        df_co_occurrence = df_co_occurrence[(df_co_occurrence.wid_x < df_co_occurrence.wid_y)]\n",
    "        # Reduce number of returned columns\n",
    "        df_co_occurrence = df_co_occurrence[['treaty_id', 'year_x', 'is_cultural_x', 'lemma_x', 'lemma_y' ]]\n",
    "        # Rename columns\n",
    "        df_co_occurrence.columns = ['treaty_id', 'year', 'is_cultural', 'lemma_x', 'lemma_y' ]\n",
    "\n",
    "        # Sort token pair so smallest always comes first\n",
    "        lemma_x = df_co_occurrence[['lemma_x', 'lemma_y']].min(axis=1)\n",
    "        lemma_y = df_co_occurrence[['lemma_x', 'lemma_y']].max(axis=1)\n",
    "        df_co_occurrence['lemma_x'] = lemma_x\n",
    "        df_co_occurrence['lemma_y'] = lemma_y\n",
    "\n",
    "        return df_co_occurrence\n",
    "    \n",
    "def create_bigram_transformer(documents):\n",
    "    import gensim.models.phrases\n",
    "    bigram = gensim.models.phrases.Phrases(map(nltk.tokenize.word_tokenize, documents))\n",
    "    return lambda ws: bigram[ws]\n",
    "\n",
    "treaties = state.treaties.loc[(state.treaties.is_cultural)]\n",
    "headnotes = treaties['headnote']\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = nltk.tokenize.word_tokenize\n",
    "lemmatizer = WordNetLemmatizer().lemmatize\n",
    "df = CoOccurrance(tokenizer=tokenizer, stopwords=stopwords, lemmatizer=lemmatizer, min_word_size=2).compute(headnotes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.parties.loc['ADF', 'party_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_document_token_frequencies(corpus, treaties):\n",
    "    from itertools import chain\n",
    "    '''\n",
    "    Returns a DataFrame with per document token frequencies i.e. \"melts\" doc-term matrix\n",
    "    '''\n",
    "    data = ((document_id, x[0], x[1]) for document_id, values in enumerate(corpus) for x in values )\n",
    "    df = pd.DataFrame(list(data), columns=['document_id', 'token_id', 'count'])\n",
    "    df = df.merge(corpus.document_names, left_on='document_id', right_index=True)[['treaty_id', 'token_id', 'count']]\n",
    "    df = df.merge(treaties[['signed_year', 'signed_period', 'signed_period_alt', 'is_cultural']], \n",
    "                  left_on='treaty_id', right_index=True)\n",
    "    _ = corpus.dictionary[0]\n",
    "    id2token = corpus.dictionary.id2token\n",
    "    df['token'] = df.token_id.apply(lambda x: id2token[x])\n",
    "    print(len(df))\n",
    "    df = df.loc[(df.token.str.len() > 2)]\n",
    "    print(len(df))\n",
    "    return df\n",
    "\n",
    "df = get_document_token_frequencies(corpus, state.treaties)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#pip install -U spacy\n",
    "#python -m spacy download en\n",
    "#python -m spacy download de\n",
    "#python -m spacy download fr\n",
    "#python -m spacy download it\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class SpacyNltkTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = nltk.tokenize.word_tokenize(text)\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "    \n",
    "nlp = spacy.load('en')\n",
    "nlp.tokenizer = SpacyNltkTokenizer(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "nlp.pipeline\n",
    "\n",
    "text = 'Donald Trump lives in New York.'\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        ent.merge(ent.root.tag_, ent.text, ent.label_)\n",
    "        \n",
    "for ent in doc.ents:\n",
    "    if len(ent.orth_.split()) > 1:\n",
    "      start = text.index(ent.orth_)\n",
    "      end = start+len(ent.orth_)\n",
    "      print(ent.orth_ + ' start: ' + str(start) + ' ' + 'end: ' + str(end) + ' ' + 'entity: ' + ent.label_)\n",
    "      doc.merge(start, end, '', '', ent.label_)\n",
    "      for token in doc:    \n",
    "          print(token.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# doc = nlp(u'') #u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "#special_case = [{ORTH: u'gim', LEMMA: u'give', POS: u'VERB'}, {ORTH: u'me'}]\n",
    "#nlp.tokenizer.add_special_case(u'gimme', special_case)\n",
    "\n",
    "#nlp.tokenizer = nltk.tokenize.word_tokenize\n",
    "# nlp.tokenizer = my_tokenizer_factory(nlp.vocab)\n",
    "\n",
    "treaty_headnotes = pd.DataFrame(state.treaties.head(10)['headnote'])\n",
    "texts = ( (index, [ (t.text, t.lemma_, t.tag_, t.is_alpha, t.is_stop) for t in nlp(row[0]) ]) \n",
    "         for index, row in treaty_headnotes.iterrows() )\n",
    "\n",
    "result = [x for x in texts]\n",
    "# print(result)\n",
    "\n",
    "#tagged_documents = (index, row[0], [ x for x in nlp(row[0])]) for index, row in treaty_headnotes.iterrows() )\n",
    "#print(next(tagged_documents))\n",
    "#tokens = ( (id,) + doc for (id, doc) in (x[0],x[1]) for x in treaty_headnotes.iteritems())\n",
    "#print(next(tokens))\n",
    "\n",
    "#    for treaty, doc in tagged_documents:\n",
    "#    tokens = [ ]\n",
    "#    for token in doc:\n",
    "#        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "print(next(wordnet.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from numpy import exp\n",
    "from scipy.special import factorial\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "poisson_pmf = lambda y, mu: mu**y / factorial(y) * exp(-mu)\n",
    "y_values = range(0, 25)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for mu in [1, 5, 10]:\n",
    "    distribution = []\n",
    "    for y_i in y_values:\n",
    "        distribution.append(poisson_pmf(y_i, mu))\n",
    "    ax.plot(y_values, distribution, label=('$\\mu$=' + str(mu)),\n",
    "            alpha=0.5, marker='o', markersize=8)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('$y$', fontsize=14)\n",
    "ax.set_ylabel('$f(y \\mid \\mu)$', fontsize=14)\n",
    "ax.axis(xmin=0, ymin=0)\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from gensim import corpora, models, similarities\n",
    "tweets=[\n",
    "['human', 'interface', 'computer', 'human'],\n",
    " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    " ['eps', 'user', 'interface', 'system'],\n",
    " ['system', 'human', 'system', 'eps'],\n",
    " ['user', 'response', 'time'],\n",
    " ['trees'],\n",
    " ['graph', 'trees'],\n",
    " ['graph', 'minors', 'trees'],\n",
    " ['graph', 'minors', 'survey']] \n",
    "\n",
    "# create dictionary (index of each element)\n",
    "dictionary = corpora.Dictionary(tweets)\n",
    "_ = dictionary[0]\n",
    "raw_corpus = [dictionary.doc2bow(t) for t in tweets]\n",
    "tfidf = models.TfidfModel(raw_corpus, smartirs='ntc') # step 1 -- initialize a model\n",
    "document = tfidf[raw_corpus[0]]\n",
    "\n",
    "x = [ { dictionary.id2token[id]: score for (id, score) in document } for document in tfidf[raw_corpus]]\n",
    "y = [ sum([x[1] for x in tfidf[d]]) for d in raw_corpus]\n",
    "z = [ [x[1] for x in tfidf[d]] for d in raw_corpus]\n",
    "y = [ gensim.matutils.unitvec(np.array([x[1] for x in tfidf[d]]), norm='l1') for d in raw_corpus]\n",
    "\n",
    "w = np.array([0.40824829046386296, 0.8164965809277259, 0.40824829046386296])\n",
    "gensim.matutils.unitvec(w, norm='l1')\n",
    "print(y)\n",
    "\n",
    "#https://stackoverflow.com/questions/42269313/interpreting-the-sum-of-tf-idf-scores-of-words-across-documents\n",
    "#The interpretation of TF-IDF in corpus is the highest TF-IDF in corpus for a given term.\n",
    "corpus_tfidf = tfidf[raw_corpus]\n",
    "\n",
    "toplist = {}\n",
    "for doc in corpus_tfidf:\n",
    "    for token_id, score in doc:\n",
    "        if token_id not in toplist:\n",
    "            toplist[token_id] = 0\n",
    "\n",
    "        if score > toplist[token_id]:\n",
    "            toplist[token_id] = score\n",
    "\n",
    "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(\"%2s: %-13s %s\" % (i, dictionary[item[0]], item[1]))\n",
    "    if i == 6: break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    " \n",
    "'''\n",
    " \n",
    "This script just show the basic workflow to compute TF-IDF similarity matrix with Gensim \n",
    " \n",
    " \n",
    "OUTPUT :\n",
    " \n",
    "clemsos@miner $ python gensim_workflow.py \n",
    " \n",
    " \n",
    "How to use Gensim to compute TF-IDF similarity step by step\n",
    "----------\n",
    "Let's start with a raw corpus :<type 'list'>\n",
    " \n",
    "STEP 1 : Index and vectorize\n",
    "----------\n",
    "We create a dictionary, an index of all unique values: <class 'gensim.corpora.dictionary.Dictionary'>\n",
    "Then convert convert tokenized documents to vectors: <type 'list'>\n",
    "Save the vectorized corpus as a .mm file\n",
    " \n",
    "STEP 2 : Transform and compute similarity between corpuses\n",
    "----------\n",
    "We load our dictionary : <class 'gensim.corpora.dictionary.Dictionary'>\n",
    "We load our vector corpus : <class 'gensim.corpora.mmcorpus.MmCorpus'> \n",
    "We initialize our TF-IDF transformation tool : <class 'gensim.models.tfidfmodel.TfidfModel'>\n",
    "We convert our vectors corpus to TF-IDF space : <class 'gensim.interfaces.TransformedCorpus'>\n",
    " \n",
    "STEP 3 : Create similarity matrix of all files\n",
    "----------\n",
    "We compute similarities from the TF-IDF corpus : <class 'gensim.similarities.docsim.MatrixSimilarity'>\n",
    "We get a similarity matrix for all documents in the corpus <type 'numpy.ndarray'>\n",
    " \n",
    "Done in 0.011s\n",
    " \n",
    "'''\n",
    "from gensim import corpora, models, similarities\n",
    "from time import time\n",
    " \n",
    "t0=time()\n",
    " \n",
    "# keywords have been extracted and stopwords removed.\n",
    " \n",
    "tweets=[['human', 'interface', 'computer'],\n",
    " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    " ['eps', 'user', 'interface', 'system'],\n",
    " ['system', 'human', 'system', 'eps'],\n",
    " ['user', 'response', 'time'],\n",
    " ['trees'],\n",
    " ['graph', 'trees'],\n",
    " ['graph', 'minors', 'trees'],\n",
    " ['graph', 'minors', 'survey']] \n",
    " \n",
    "print \"How to use Gensim to compute TF-IDF similarity step by step\"\n",
    "print '-'*10\n",
    "print \"Let's start with a raw corpus :%s\"%type(tweets)\n",
    "print\n",
    "# STEP 1 : Compile corpus and dictionary\n",
    "print \"STEP 1 : Index and vectorize\"\n",
    "print '-'*10\n",
    " \n",
    "# create dictionary (index of each element)\n",
    "dictionary = corpora.Dictionary(tweets)\n",
    "dictionary.save('/tmp/tweets.dict') # store the dictionary, for future reference\n",
    "print \"We create a dictionary, an index of all unique values: %s\"%type(dictionary)\n",
    " \n",
    "# compile corpus (vectors number of times each elements appears)\n",
    "raw_corpus = [dictionary.doc2bow(t) for t in tweets]\n",
    "print \"Then convert convert tokenized documents to vectors: %s\"% type(raw_corpus)\n",
    "corpora.MmCorpus.serialize('/tmp/tweets.mm', raw_corpus) # store to disk\n",
    "print \"Save the vectorized corpus as a .mm file\"\n",
    "print\n",
    " \n",
    "# STEP 2 : similarity between corpuses\n",
    "print \"STEP 2 : Transform and compute similarity between corpuses\"\n",
    "print '-'*10\n",
    "dictionary = corpora.Dictionary.load('/tmp/tweets.dict')\n",
    "print \"We load our dictionary : %s\"% type(dictionary)\n",
    " \n",
    "corpus = corpora.MmCorpus('/tmp/tweets.mm')\n",
    "print \"We load our vector corpus : %s \"% type(corpus) \n",
    " \n",
    "# Transform Text with TF-IDF\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "print \"We initialize our TF-IDF transformation tool : %s\"%type(tfidf)\n",
    " \n",
    "# corpus tf-idf\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "print \"We convert our vectors corpus to TF-IDF space : %s\"%type(corpus_tfidf)\n",
    "print\n",
    " \n",
    "# STEP 3 : Create similarity matrix of all files\n",
    "print \"STEP 3 : Create similarity matrix of all files\"\n",
    "print '-'*10\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "print \"We compute similarities from the TF-IDF corpus : %s\"%type(index)\n",
    "index.save('/tmp/deerwester.index')\n",
    "index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')\n",
    " \n",
    "sims = index[corpus_tfidf]\n",
    "print \"We get a similarity matrix for all documents in the corpus %s\"% type(sims)\n",
    "print\n",
    "print \"Done in %.3fs\"%(time()-t0)\n",
    " \n",
    "# print sims\n",
    "# print list(enumerate(sims))\n",
    "# sims = sorted(enumerate(sims), key=lambda item: item[1])\n",
    "# print sims # print sorted (document number, similarity score) 2-tuples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
