{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Part-of-speech tagging of Treaty Headnote\n",
    "This script PoS-tags the treaty headnotes for the entire WTI master data. It uses Stanford's CoreNLP package and the Stanford POS tagger to compute the PoS-tags, and the resulting data is stored in a tab seperated file \"./data/tagged_headnotes.csv\".\n",
    "\n",
    "Note that it will take some time, perhaps an hour, to execute the script. The script needs only to be run once, though, unless changes are made to any headnote or errors are found in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoreNLP Dependency Parsing. Start CoreNLP server (~roger/applications/stanford.../start-server.bash)\n",
    "from nltk.tag.stanford import CoreNLPPOSTagger\n",
    "if 'pos_tagger' not in globals():\n",
    "    pos_tagger = nltk.tag.stanford.CoreNLPPOSTagger(url='http://127.0.0.1:9000', encoding='utf8')\n",
    "\n",
    "def pos_tag_texts(pos_tagger, source_texts, done_texts):\n",
    "    pending_keys = set(source_texts.index) - set(done_texts.keys())\n",
    "    counter = 0\n",
    "    for key in pending_keys:\n",
    "        counter += 1\n",
    "        text = source_texts.loc[key] or ''\n",
    "        try:\n",
    "            done_text = pos_tagger.tag(text.lower().split())\n",
    "        except:\n",
    "            print('Failed: {}[{}]'.format(key, text))\n",
    "            done_text = []\n",
    "        done_texts[key] = done_text\n",
    "        if counter % 1000 == 0:\n",
    "            print('Done: {} out of {}'.format(counter, len(pending_keys)))\n",
    "    '''\n",
    "    Add key and sequence number to each key's tuples\n",
    "    '''  \n",
    "    keyed_texts = {\n",
    "        key: [ (key, i, w, p) for i, (w, p) in enumerate(done_texts[key]) ] for key in done_texts.keys()\n",
    "    }\n",
    "    '''\n",
    "    Flatten list to list of tuples\n",
    "    '''  \n",
    "    keyed_texts_list = [ item for sublist in list(keyed_texts.values()) for item in sublist ]\n",
    "\n",
    "    return keyed_texts_list\n",
    "    \n",
    "tagged_treaties = {}\n",
    "headnotes = state.get_headnotes()\n",
    "tagged_headnotes = pos_tag_texts(pos_tagger, headnotes, tagged_treaties)\n",
    "\n",
    "df_tagged_headnotes = pd.DataFrame(tagged_headnotes, columns=['treaty_id', 'sequence_id', 'word', 'pos'])\n",
    "df_tagged_headnotes.to_csv('./data/tagged_headnotes.csv', sep='\\t')\n",
    "\n",
    "# Failed: 134271[] Failed: 124126[] Failed: 133760[]\n",
    "\n",
    "# pos_tagger.session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
