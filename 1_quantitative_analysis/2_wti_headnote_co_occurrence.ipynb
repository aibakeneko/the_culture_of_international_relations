{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Culture of International Relations\n",
    "\n",
    "#### About this project\n",
    "Cultural treaties are the bi-lateral and multilateral agreements among states that promote and regulate cooperation and exchange in the fields of life generally call cultural or intellectual. Although it was only invented in the early twentieth century, this treaty type came to be the fourth most common bilateral treaty in the period 1900-1980 (Poast et al., 2010). In this project, we seek to use several (mostly European) statesâ€™ cultural treaties as a historical source with which to explore the emergence of a global concept of culture in the twentieth century. Specifically, the project will investigate the hypothesis that the culture concept, in contrast to earlier ideas of civilization, played a key role in the consolidation of the post-World War II international order.\n",
    "\n",
    "The central questions that interest me here can be divided into two groups: \n",
    "- First, what is the story of the cultural treaty, as a specific tool of international relations, in the twentieth century? What was the historical curve of cultural treaty-making? For example, in which political or ideological constellations do we find (the most) use of cultural treaties? Among which countries, in which historical periods? What networks of relations were thereby created, reinforced, or challenged? \n",
    "- Second, what is the \"culture\" addressed in these treaties? That is, what do the two signatories seem to mean by \"culture\" in these documents, and what does that tell us about the role that concept played in the international system? How can quantitative work on this dataset advance research questions about the history of concepts?\n",
    "\n",
    "In this notebook, we deal with these treaties in three ways:\n",
    "1) quantitative analysis of \"metadata\" about all bilateral cultural treaties signed betweeen 1919 and 1972, as found in the World Treaty Index or WTI (Poast et al., 2010).\n",
    "    For more on how exactly we define a \"cultural treaty\" here, and on other principles of selection, see... [add this, using text now in \"WTI quality assurance\"].\n",
    "2) network analysis of the system of international relationships created by these treaties (using data from WTI, as above).\n",
    "3) Text analysis of the complete texts of selected treaties. \n",
    "\n",
    "After some set-up sections, the discussion of the material begins at \"Part 1,\" below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Instructions on Jupyter Notebooks\n",
    "Please see [this tutorial](https://www.youtube.com/watch?v=h9S4kN4l5Is) for an introduction on what Jupyter notebooks are and how to use them. There are lots of other Jupyter tutorials on YouTube (and elsewhere) as well. In short, a notebook is a document with embedded executable code presented in a simple and easy to use web interface. Most important things to note are:\n",
    "- Click on the menu Help -> User Interface Tour for an overview of the Jupyter Notebook App user interface.\n",
    "- The **code cells** contains the script code (Python in this case, but can be other languages are also suported) and are the sections marked by **In [x]** in the left margin. It is marked as **In []** if it hasn't been executed, and as **In [n]** when it has been executed(n is an integer). A cell marked as **In [\\*]** is either executing, or waiting to be executed (i.e. other cells are executing).\n",
    "- The **current cell** is highlighted with a blue (or green if in \"edit\" mode) border. You make a cell current by clicking on it,\n",
    "- Code cells aren't executed automatically. Instead you execute the current cell by either pressing **shift+enter** or the **play** button in the toolbar. The output (or result) of a cell's execution is presented directly below the cell prefixed by **Out[n]**.\n",
    "- The next cell will automatically be selected (made current) after a cell has been executed. Repeatadly pressing **shift+enter** or the play button hence executes the cells in sequence.\n",
    "- You can run the entire notebook in a single step by clicking on the menu Cell -> Run All. Note that this can take some time to finish. You can see how cells are executed in sequence via the indicator in the margin (i.e. \"In [\\*]\" changes to \"In [n]\" where n is an integer).\n",
    "- The cells can be edited if they are double-clicked, in which case the cell border turns green. Use the ESC key to escape edit mode (or click on any other cell).\n",
    "\n",
    "To restart the kernel (i.e. the computational engine assigned to your session), click on the menu Kernel -> Restart. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".jupyter-widgets, .widget-label, .widget-dropdown > select { font-size: 8pt; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>**Mandatory Prepare Step**</span>: Setup Notebook and Load and Process Treaty Master Index\n",
    "The following code cell to be executed once for each user session. The step loads utility Python code stored in separate files, and imports dependencies to external libraries. The code also loads the WTI master index (and some related data files), and prepares the data for subsequent use.\n",
    "\n",
    "The treaty data is processed as follows:\n",
    "- All the treaty data are loaded.Extract year treaty was signed as seperate fields\n",
    "- Add new fields for specified signed period divisions\n",
    "- Fields 'group1' and 'group2' are ignored (many missing values). Instead group are fetched via party code from encoding found in the \"groups\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"39fd83d2-b799-4de0-81cf-86e4efabaca9\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"39fd83d2-b799-4de0-81cf-86e4efabaca9\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"39fd83d2-b799-4de0-81cf-86e4efabaca9\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '39fd83d2-b799-4de0-81cf-86e4efabaca9' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"39fd83d2-b799-4de0-81cf-86e4efabaca9\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"39fd83d2-b799-4de0-81cf-86e4efabaca9\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"39fd83d2-b799-4de0-81cf-86e4efabaca9\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '39fd83d2-b799-4de0-81cf-86e4efabaca9' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"39fd83d2-b799-4de0-81cf-86e4efabaca9\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import datetime\n",
    "import wordcloud\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bokeh.plotting as bp\n",
    "import bokeh.palettes\n",
    "import bokeh.models as bm\n",
    "import bokeh.io\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from math import sqrt\n",
    "from bokeh.io import push_notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "os.sys.path = os.sys.path if '..' in os.sys.path else os.sys.path + ['..']\n",
    "\n",
    "from common.file_utility import FileUtility\n",
    "from common.widgets_utility import BaseWidgetUtility\n",
    "\n",
    "import configuration_elements as config\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.ERROR)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "bp.output_notebook()\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "from common.treaty_state import load_treaty_state\n",
    "\n",
    "# Load and process treaties master index\n",
    "state = load_treaty_state('../data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Headnote word toplist and word-pair co-occurence toplist\n",
    "This report displays headnote toplists either single word occurrance or word-word co-occurrance toplists depending on whether or not the \"Co-occurrance\" is checked. The result is grouped by selected division's periods or by year.\n",
    "\n",
    "The word co-occurrance is defined as the number of times a pair of words co-occur in the same headnote. The length of headnotes is ignored in the computation (all pairs have equal weight). Multiple occurance of a word in a headnote is taken into account i.e \"cultural exchange cultural\" is counted as two co-occurances, and \"cultural exchange exchange cultural\" is four co-occurrances. Stopwords are removed if \"Remove stopwords\" are checked.\n",
    "\n",
    "Stopwords are always removed from the co-occurrance computation, whilst they are removed from single word occurrance toplist if the \"Remove stopwords\" flag is checked. The removal is based on NLTK's list of english stopwords (run ```nltk.corpus.stopwords.words('english')``` to display all stopwords).\n",
    "\n",
    "The toplist can be filtered so that only treaties involving any or one of the five parties of interest are included, and words can be excluded based on character length. Each resulting group can also be restricted by both a maximum number of pairs to display per group, as well as a min co-occurrance count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d917aef25340ce85630a14bc1c4107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Period:', index=1, layout=Layout(left='0', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "import os\n",
    "\n",
    "os.sys.path = os.sys.path if '..' in os.sys.path else os.sys.path + ['..']\n",
    "\n",
    "import nltk\n",
    "import qgrid\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from common.widgets_utility import BaseWidgetUtility\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import configuration_elements as config\n",
    "from common.treaty_state import load_treaty_state\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.ERROR)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Load and process treaties master index\n",
    "state = load_treaty_state('../data')\n",
    "class HeadnoteTokenServiceOLD():\n",
    "\n",
    "    def __init__(self, tokenizer, stopwords=None, lemmatizer=None, min_word_size=2):\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenizer,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_word_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)) \n",
    "        ]\n",
    "        \n",
    "        if stopwords is not None:\n",
    "            self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        if lemmatizer is not None:\n",
    "            self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "\n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def parse_headnotes(self, treaties):\n",
    "        \n",
    "        headnotes = treaties['headnote']\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        #tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compute_stacked(self, treaties):\n",
    "        \n",
    "        df = self.parse_headnotes(treaties)\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "    def compute_co_occurrence(self, treaties, pos_tags, only_cultural_treaties=False):\n",
    "\n",
    "        # Filter out tags based on treaties of interest\n",
    "        pos_tags = pos_tags.merge(treaties, how='inner', left_on='treaty_id', right_index=True)[[]]\n",
    "        \n",
    "        if only_cultural_treaties:\n",
    "            df_pos_tags = df_pos_tags[(df_pos_tags.is_cultural.str.contains('yes',na=False))]\n",
    "\n",
    "        # Self join of words within same treaty\n",
    "        df_co_occurrence = pd.merge(df_pos_tags, df_pos_tags, how='inner', left_on='treaty_id', right_on='treaty_id')\n",
    "        # Only consider a specific poir once\n",
    "        df_co_occurrence = df_co_occurrence[(df_co_occurrence.wid_x < df_co_occurrence.wid_y)]\n",
    "        # Reduce number of returned columns\n",
    "        df_co_occurrence = df_co_occurrence[['treaty_id', 'year_x', 'is_cultural_x', 'lemma_x', 'lemma_y' ]]\n",
    "        # Rename columns\n",
    "        df_co_occurrence.columns = ['treaty_id', 'year', 'is_cultural', 'lemma_x', 'lemma_y' ]\n",
    "\n",
    "        # Sort token pair so smallest always comes first\n",
    "        lemma_x = df_co_occurrence[['lemma_x', 'lemma_y']].min(axis=1)\n",
    "        lemma_y = df_co_occurrence[['lemma_x', 'lemma_y']].max(axis=1)\n",
    "        df_co_occurrence['lemma_x'] = lemma_x\n",
    "        df_co_occurrence['lemma_y'] = lemma_y\n",
    "\n",
    "        return df_co_occurrence\n",
    "\n",
    "class HeadnoteTokenCorpus():\n",
    "\n",
    "    def __init__(self, treaties, tokenize=None, stopwords=None, lemmatize=None, min_size=2):\n",
    "        \n",
    "        tokenize = tokenize or nltk.tokenize.word_tokenize\n",
    "        lemmatize = lemmatize or WordNetLemmatizer().lemmatize\n",
    "        stopwords = stopwords or nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenize,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)),\n",
    "            lambda ws: list(set(ws)) \n",
    "        ]\n",
    "        \n",
    "        #if stopwords is not None:\n",
    "        #    self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        #if lemmatizer is not None:\n",
    "        #    self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "        \n",
    "        treaty_tokens = self._compute_stacked(treaties)\n",
    "        vocabulary = treaty_tokens.token.unique()\n",
    "        lemmas = list(map(lemmatize, vocabulary))\n",
    "        lemma_map = { w: l for (w, l) in zip(*(vocabulary, lemmas)) if w != l }\n",
    "        stopwords_map = { s : True for s in stopwords }\n",
    "        treaty_tokens['lemma'] = treaty_tokens.token.apply(lambda x: lemma_map.get(x, x))\n",
    "        treaty_tokens['is_stopword'] = treaty_tokens.token.apply(lambda x: stopwords_map.get(x, False))\n",
    "\n",
    "        self.treaty_tokens = treaty_tokens.set_index(['treaty_id', 'sequence_id'])\n",
    "        \n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def _parse_headnotes(self, treaties):\n",
    "        \n",
    "        headnotes = treaties['headnote']\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _compute_stacked(self, treaties):\n",
    "        \n",
    "        df = self._parse_headnotes(treaties)\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "def compute_co_occurrance(treaties):\n",
    "    \n",
    "    treaty_tokens = state.treaty_headnote_corpus.treaty_tokens\n",
    "    i1 = treaties.index\n",
    "    # i2 = treaty_tokens.reset_index().set_index('treaty_id').index\n",
    "    i2 = treaty_tokens.index.get_level_values(0)\n",
    "    treaty_tokens = treaty_tokens[i2.isin(i1)]\n",
    "    \n",
    "    treaty_tokens = treaty_tokens.loc[treaty_tokens.is_stopword==False]\n",
    "    treaty_tokens = treaty_tokens.reset_index().drop(['is_stopword', 'sequence_id'], axis=1).set_index('treaty_id')\n",
    "\n",
    "    co_occurrance = treaty_tokens.merge(treaty_tokens, how='inner', left_index=True, right_index=True)\n",
    "    co_occurrance = co_occurrance.loc[(co_occurrance['token_x'] < co_occurrance['token_y'])]\n",
    "    #co_occurrance['token'] = co_occurrance.apply(lambda row: row[groupby_pair[0]] + ' - ' + row[groupby_pair[1]], axis=1)\n",
    "    co_occurrance['token'] = co_occurrance.apply(lambda row: ' - '.join([row['token_x'].upper(), row['token_y'].upper()]), axis=1)\n",
    "    co_occurrance['lemma'] = co_occurrance.apply(lambda row: ' - '.join([row['lemma_x'].upper(), row['lemma_y'].upper()]), axis=1)\n",
    "    co_occurrance = co_occurrance.assign(is_stopword=False, sequence_id=0)[['sequence_id', 'token', 'lemma', 'is_stopword']]\n",
    "    \n",
    "    return co_occurrance\n",
    "\n",
    "def create_bigram_transformer(documents):\n",
    "    import gensim.models.phrases\n",
    "    bigram = gensim.models.phrases.Phrases(map(nltk.tokenize.word_tokenize, documents))\n",
    "    return lambda ws: bigram[ws]\n",
    "\n",
    "def remove_snake_case(snake_str):\n",
    "    return ' '.join(x.title() for x in snake_str.split('_'))\n",
    "\n",
    "def get_top_partiesssss(data, period, party_name, n_top=5):\n",
    "    xd = data.groupby([period, party_name]).size().rename('TopCount').reset_index()\n",
    "    top_list = xd.groupby([period]).apply(lambda x: x.nlargest(n_top, 'TopCount'))\\\n",
    "        .reset_index(level=0, drop=True)\\\n",
    "        .set_index([period, party_name])\n",
    "    return top_list\n",
    "\n",
    "result=None\n",
    "def display_headnote_toplist(\n",
    "    period=None,\n",
    "    parties=None,\n",
    "    extra_groupbys=None,\n",
    "    only_is_cultural=True,\n",
    "    use_lemma=False,\n",
    "    compute_co_occurance=False,\n",
    "    remove_stopwords=True,\n",
    "    min_word_size=2,\n",
    "    n_min_count=1,\n",
    "    output_format='table',\n",
    "    n_top=50\n",
    "    # plot_style=tw.plot_style\n",
    "):\n",
    "    global ihnw, result\n",
    "    \n",
    "    try:\n",
    "        hnw.progress.value = 1    \n",
    "        treaties = state.treaties.loc[state.treaties.signed_period != 'other']\n",
    "\n",
    "        if state.treaty_headnote_corpus is None:\n",
    "            print('Preparing headnote corpus for first time use')\n",
    "            state.treaty_headnote_corpus = HeadnoteTokenCorpus(treaties=treaties)\n",
    "\n",
    "        if only_is_cultural:\n",
    "            treaties = treaties.loc[(state.treaties.is_cultural)]\n",
    "\n",
    "        if parties is not None:\n",
    "            ids = state.stacked_treaties.loc[(state.stacked_treaties.party.isin(parties))].index\n",
    "            treaties = treaties.loc[ids]\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        if compute_co_occurance:\n",
    "\n",
    "            treaty_tokens = compute_co_occurrance(treaties)\n",
    "\n",
    "        else:\n",
    "\n",
    "            treaty_tokens = state.treaty_headnote_corpus.treaty_tokens\n",
    "\n",
    "            if remove_stopwords is True:\n",
    "                treaty_tokens = treaty_tokens.loc[treaty_tokens.is_stopword==False]\n",
    "\n",
    "            treaty_tokens = treaty_tokens.reset_index().set_index('treaty_id')\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        treaty_tokens = treaty_tokens\\\n",
    "            .merge(treaties, how='inner', left_index=True, right_index=True)\\\n",
    "            .drop(['source', 'signed', 'headnote', 'is_cultural', # 'is_cultural_yesno', 'sequence',\n",
    "                   'topic1', 'topic2', 'title'], axis=1)\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        token_or_lemma = 'token' if not use_lemma else 'lemma'\n",
    "\n",
    "        groupbys  = []\n",
    "        groupbys += [ period ] if not period is None else []\n",
    "        groupbys += (extra_groupbys or [])\n",
    "        groupbys += [ token_or_lemma ]\n",
    "\n",
    "        result = treaty_tokens.groupby(groupbys).size().reset_index().rename(columns={0: 'Count'})\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        ''' Filter out the n_top most frequent words from each group '''\n",
    "        result = result.groupby(groupbys[-1]).apply(lambda x: x.nlargest(n_top, 'Count'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            # .set_index(groupbys)\n",
    "\n",
    "        if min_word_size > 0:\n",
    "            result = result.loc[result[token_or_lemma].str.len() >= min_word_size]\n",
    "\n",
    "        if n_min_count > 1:\n",
    "            result = result.loc[result.Count >= n_min_count]\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        result = result.sort_values(groupbys[:-1] + ['Count'], ascending=len(groupbys[:-1])*[True] + [False])\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        if output_format in ('table', 'qgrid'):\n",
    "            result.columns = [ remove_snake_case(x) for x in result.columns ]\n",
    "            if output_format == 'table':\n",
    "                display(HTML(result.to_html()))\n",
    "            else:\n",
    "                qgrid_widget = qgrid.show_grid(result, show_toolbar=True)\n",
    "                qgrid_widget\n",
    "        elif output_format == 'unstack':\n",
    "            result = result.set_index(groupbys).unstack(level=0).fillna(0).astype('int32')\n",
    "            result.columns = [ x[1] for x in result.columns ]\n",
    "            display(HTML(result.to_html()))\n",
    "        elif output_format == 'unstack_plot':\n",
    "            result = result.set_index(list(reversed(groupbys))).unstack(level=0).fillna(0).astype('int32')\n",
    "            result.columns = [ x[1] for x in result.columns ]\n",
    "            result.plot(kind='bar', figsize=(16,8))\n",
    "\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    hnw.progress.value += 1\n",
    "    hnw.progress.value = 0\n",
    "\n",
    "hnw = BaseWidgetUtility(\n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:',\n",
    "        icon='',\n",
    "        layout=widgets.Layout(width='200px', left='0')\n",
    "    ),\n",
    "    parties=widgets.Dropdown(\n",
    "        options=config.default_party_options,\n",
    "        value=None,\n",
    "        description='Parties:', icon='', layout=widgets.Layout(width='200px', left='0')\n",
    "    ),\n",
    "    use_lemma=widgets.ToggleButton(\n",
    "        description='Use lemma', value=False,\n",
    "        tooltip='Use WordNet lemma', icon='', layout=widgets.Layout(width='140px', left='0')\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords', icon='', layout=widgets.Layout(width='140px', left='0')\n",
    "    ),\n",
    "    extra_groupbys=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Topic': [ 'Topic' ],\n",
    "        },\n",
    "        value=None,\n",
    "        description='Groupbys:', icon='', layout=widgets.Layout(width='140px', left='0')\n",
    "    ),\n",
    "    min_word_size=widgets.BoundedIntText(\n",
    "        value=2, min=0, max=5, step=1,\n",
    "        description='Min word:', layout=widgets.Layout(width='140px')\n",
    "    ),\n",
    "    only_is_cultural=widgets.ToggleButton(\n",
    "        description='Only Cultural', value=True,\n",
    "        tooltip='Display only \"is_cultural\" treaties', icon='', layout=widgets.Layout(width='140px', left='0')\n",
    "    ),\n",
    "    compute_co_occurance=widgets.ToggleButton(\n",
    "        description='Cooccurrence', value=True,\n",
    "        tooltip='Compute Cooccurrence', icon='', layout=widgets.Layout(width='140px', left='0')\n",
    "    ),\n",
    "    output_format=widgets.Dropdown(\n",
    "        description='Output', value='table',\n",
    "        options={\n",
    "            'Table': 'table',\n",
    "            'Qgrid': 'qgrid',\n",
    "            'Unstack': 'unstack',\n",
    "            'Unstack plot': 'unstack_plot'\n",
    "        },\n",
    "        icon='',\n",
    "        layout=widgets.Layout(width='140px', left='0')\n",
    "    ),\n",
    "    plot_style=widgets.Dropdown(\n",
    "        options=config.matplotlib_plot_styles,\n",
    "        value='seaborn-pastel',\n",
    "        description='Style:', icon='', layout=widgets.Layout(width='140px', left='0')\n",
    "    ),\n",
    "    n_top=widgets.IntSlider(\n",
    "        value=25, min=2, max=100, step=10,\n",
    "        description='Top/grp #:', # continuous_update=False,\n",
    "    ),\n",
    "    n_min_count=widgets.IntSlider(\n",
    "        value=2, min=1, max=10, step=1,\n",
    "        tooltip='Filter out words with count less than specified value',\n",
    "        description='Min count:', # continuous_update=False,\n",
    "    ),\n",
    "    progress=widgets.IntProgress(\n",
    "        min=0, max=10, step=1, value=0, layout=widgets.Layout(width='99%')\n",
    "    )\n",
    ")\n",
    "\n",
    "ihnw = widgets.interactive(\n",
    "    display_headnote_toplist,\n",
    "    period=hnw.period,\n",
    "    parties=hnw.parties,\n",
    "    extra_groupbys=hnw.extra_groupbys,\n",
    "    only_is_cultural=hnw.only_is_cultural,\n",
    "    n_min_count=hnw.n_min_count,\n",
    "    n_top=hnw.n_top,\n",
    "    min_word_size=hnw.min_word_size,\n",
    "    use_lemma=hnw.use_lemma,\n",
    "    compute_co_occurance=hnw.compute_co_occurance,\n",
    "    remove_stopwords=hnw.remove_stopwords,\n",
    "    output_format=hnw.output_format,\n",
    "    # plot_style=tw.plot_style\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([ hnw.period, hnw.parties, hnw.min_word_size ]),\n",
    "        widgets.VBox([ hnw.extra_groupbys, hnw.n_top, hnw.n_min_count]),\n",
    "        widgets.VBox([ hnw.only_is_cultural, hnw.use_lemma, hnw.remove_stopwords, hnw.compute_co_occurance]),\n",
    "        widgets.VBox([ hnw.output_format, hnw.progress ])\n",
    "    ]\n",
    ")\n",
    "display(widgets.VBox([boxes, ihnw.children[-1]]))\n",
    "ihnw.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
